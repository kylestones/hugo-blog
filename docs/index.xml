<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Org Mode</title>
    <link>https://kylestones.github.io/hugo-blog/</link>
    <description>Recent content on Org Mode</description>
    <image>
      <url>https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Jun 2018 08:02:31 +0000</lastBuildDate><atom:link href="https://kylestones.github.io/hugo-blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于</title>
      <link>https://kylestones.github.io/hugo-blog/about/</link>
      <pubDate>Thu, 21 Jun 2018 08:02:31 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/about/</guid>
      <description>书籍是人类进步的阶梯 &amp;ndash; 高尔基
  多数人为了避免真正的思考，愿意做任何事情 &amp;ndash; 王兴
  努力做一个开心的人，开心到别人见了你也会觉得幸福！ &amp;ndash; 佚名
  未经审视的人生是不值得过的 &amp;ndash; 苏格拉底
  生命比盖房更需要蓝图 &amp;ndash; 卡内基
  抓住了这个主要矛盾，一切问题就迎刃而解了 &amp;ndash; 《毛泽东选集.第一卷》
  生活中 10% 由发生在你身上的事情组成，而另外的 90% 则由你对所发生事情如何反应决定 &amp;ndash; 费斯汀格法则 Festinger
  任何事物都需要创造两次，一次在大脑，一次在实践 &amp;ndash; 高效能人士的七个习惯
  Fear is your friend. More often than not it shows you exactly what you should do. &amp;ndash; TED
  The perceived complexity of a task will expand to fill the time you allow it.</description>
    </item>
    
    <item>
      <title>Learning GNU Emacs</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/emacs/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/emacs/</guid>
      <description>1. 第一章 1.1. 输入命令 如果不知道一个命令对应的按键或者需要输入命令的时候，首先输入&amp;rsquo;ESC x&#39;或者&amp;rsquo;META-x&amp;rsquo;，然后输入命令，接着回车即可。
1.2. 缓冲区 buffer 编辑器并不会对文件本身进行编辑，会首先将文件的内容放到一个临时性的缓冲区里（文件的一个副本），然后再对缓冲区中的东西进行编辑。缓冲区的名字通常就是正在编辑的文件的名字。 scratch是一个临时性的辅助性的缓冲区，类似草稿簿。 help是将帮助信息显示的地方
1.3. 编辑模式 Emacs有很多个主模式，且一个编辑缓冲区只能处于一个主模式，退出某个主模式的办法就是进入另一个主模式。 Emacs会依据文件名后缀或者文件的内容来判断文件的类型，从而尝试进入正确的模式。如果无法判断，会转入基本编辑模式。
   模式 功能     基本模式（fundamental mode） 默认模式，无特殊行为   文本模式（text mode） 书写文字材料 （2）   邮件模式（mail mode） 书写电子邮件消息（6）   RMAIL mode 阅读和组织电子邮件（6）   只读模式（view mode） 查看文件，但不进行编辑（5）   shell mode 在Emacs里运行一个UNIX shell（5）   FTP模式（ange-ftp mode） 下载或者查看远程系统上的文件（7）   telnet mode 登陆到远程系统（7）   大纲模式（outline mode） 书写大纲（8）   缩进文本模式（indented text mode） 自动缩进文本（8）   图形模式（pictures mode） 绘制简单的线条图形（8）   nroff mode 按nroff的要求对文件进行排版（9）   TEX mode 按TEX的要求对文件进行排版（9）   LATEX mode 按LATEX的要求对文件进行排版（9）   C mode 书写C语言程序（12）   C++ mode 书写C++程序（12）   FORTRAN mode 书写FORTRAN程序（12）   Emacs LISP mode 书写Emacs LISP程序（12）   LISP mode 书写LISP程序（12）   LISP 互动模式（LISP interaction mode） 书写和求值LISP表达式（12）    在主模式之外还有一些副模式（minor mode），用于定义Emacs的某些特定的行为，可以在主模式里打开或者关闭。比如自动换行模式（auto-fill mode）会使文件在一个适当的位置自动插入一个换行符。</description>
    </item>
    
    <item>
      <title>markdown 语法</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/markdown/</link>
      <pubDate>Thu, 02 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/markdown/</guid>
      <description>如何使用Markdown 作者：三石 上面的两个等同于一级和二级标题即：# 和##
   三个*, -, _均可以用于分割线
1. 基本用法 1.1 怎样使用斜体 命令： 在需要斜体的文字两边加上*或者_即可
操作系统：精髓与设计原理
深入理解操作系统
1.2 怎样使用粗体 命令：在需要斜体的文字两边加上两个*或者_即可
代码大全2
UML用户指南第二版
1.3 怎样使用粗斜体 命令：在需要斜体的文字两边加上三个*或者_即可
构建之法
编程之美
1.4 文字有背景 鸟哥的Linux私房菜
1.5 文字中间有删除线效果 Effective C++
1.6 需要阅读的书籍  设计模式 C语言程序设计  1.7 有序和无序列表  算法步骤   算法原理   实验结果   检测人脸 人脸对齐 超分重建 人脸识别 后面的即使没有按正确的顺序书写，或者使用无序的都可以（这里无序的好像不支持）  1.8  前面的符号和文字之间有5个以上的文字，字体效果会有变化   可以嵌套使用，如果想解除嵌套，需要分段     没有充分的空格的效果</description>
    </item>
    
    <item>
      <title>Vim summary</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/vim/</link>
      <pubDate>Sun, 02 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/vim/</guid>
      <description>1. vim的三种模式 模式可以理解为状态，不同的模式下适合处理不同的工作。vim有6种基本模式和5种派生模式，初学者只需要掌握后面三种模式即可，普通模式，编辑模式，命令行模式。 普通模式可以进行复制，删除，移动屏幕、光标，恢复、撤销等操作。有些人打开vim后敲代码，一阵忙碌之后发现似乎什么都没有打上去，或者根本跟自己想的不一样，因此变得畏惧这个编辑器，其实它只是并不处于编辑模式而已；编辑模式就是大部分编辑器的状态，可以书写程序。这个状态下vim的状态和通常的编辑器一样。命令行模式可以设置，替换，保存，退出等操作。 三种模式之间可以相互转化，打开vim后默认进入普通模式，普通模式和命令行模式可以认为是在相同的情况下状态，两者之间不需要进行转化，键入a、i、r、s、o（A、I、R、S、O）可以进入编辑模式，当然不同的字母有不同的效果(vim命令区分大小写)。编辑模式时按esc键转换到普通模式。
2. 插入文本    命令 作用     a 是append的缩写，变成编辑状态，在当前位置之后输入字符   A 在当前的行的末尾追加字符   i 是insert的缩写，从当前位置开始插入文字   I 从当前行的行首开始插入字符   s 删除当前字符并变成编辑状态   S 删除当前行并变成编辑状态   o 从当前行的下一行开始编辑   O 从当前行的上一行开始编辑    3. 移动光标 感觉学习vim最重要的是要先学会移动光标（普通模式），学会移动光标才会使这个编辑器听你的话。
3.1 最基本的当然是h、j、k、l，分别可以将光标向左、下、上、右移动一个位置。 3.2 在一行内移动光标    命令 作用     0 将光标移动到行首   ^ 移动光标到行首第一个非空字符上去   $ 移动光标到行尾   f[字符] 将光标直接定位到指定的字符，该命令只可以让光标在当前行移动。移动光标到当前位置右侧出现的第一个指定字符   F[字符] 将光标直接定位到指定的字符，该命令只可以让光标在当前行移动。移动光标到当前位置左侧出现的第一个指定字符   t[字符] 将光标直接定位到指定的字符的前面一个字符，该命令只可以让光标在当前行移动。移动光标到当前位置右侧出现的第一个指定字符前面的一个字符   T[字符] 将光标直接定位到指定的字符的前面一个字符，该命令只可以让光标在当前行移动。移动光标到当前位置左侧出现的第一个指定字符后面的一个字符   ; 配合f、F、t、T使用，重复上一个命令   , 配合f、F、t、T使用，反方向重复上一个命令   n+&amp;lt;sapce&amp;gt; 向后移动n个字符    3.</description>
    </item>
    
    <item>
      <title>Advanced Programming in the UNIX Environment</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/apue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/apue/</guid>
      <description>I/O  文件 I/O   通过文件描述符标识。对于内核而言，所有打开文件都通过文件描述符引用。 其也是有缓冲的，只是其缓冲区在内核空间，不再用户空间。体现在延迟写，只在适当的时候才调用写文件操作， 减少不必要的写操作，增加性能。 函数 open() 打开文件时，指定模式必须有且仅有 O _RDONLY / O _WRONLY / O _RDWR 三者中的一个。 选项 O _APPEND 指定为追加。原来的 UNIX 系统不支持追加，只能先使用 lseek() 先设置文件的偏移量到文件的 结尾，然后再写。但是这在多个进程同时写的时候会出问题，因为调用了两个函数来追加，所以不是一个原子操作。 而追加选项确保设置偏移和写操作为原子操作。 管道读写   写管道的时候不用使用追加选项，内核会自动按写的顺序写入管道。读取后自动将相应的内容从管道清除。 读取一个写端关闭的管道，在读取完全部数据后，read 函数返回 0 ； 写一个读端关闭的管道，产生 SIGPIPE 信号，设置该信号处理函数或者忽略该信号，write 返回 -1，且 errno 设置为 EPIPE。 读管道，如果管道为空，调用线程阻塞，同进程内的其他线程不受影响。 套接字描述符   虽然套接字描述符本质上是一个文件描述符，但不是所有参数为文件描述符的函数都可以接受套接字描述符。 套接字不支持文件偏移量概念，不能使用 lseek 函数，也不可以使用 mmap 函数，不可调用 fchdir 函数。  shutdown 函数可以直接关闭一个套接字的读端或者写端，不管该套接字描述符复制了多少分； close 函数只有在关闭最后一个套接字的时候才会释放该套接字。 改变文件偏移量   lseek 标准库 I/O   标准 I/O 库的操作围绕流(stream)进行。利用指向 FILE 对象维护，该结构体包含了标准 I/O 库为了维护该流所 需要的信息：文件描述符，指向缓冲区的指针，缓冲区的长度，缓冲区中当前的字符数，出错标志等。不需要关心 FILE 结构的具体形式。 相对于文件 I/O，标准库的 I/O 都是带缓冲的，标准库维护了一个缓冲区，在适当的时候才调用 read、write 函 数，从而减少系统调用的开销。 定位流   ftell、fseek、ftello、fseek、fgetpos、fsetpos IPC   IPC 传统上是 UNIX 中一个杂乱不堪的领域，虽然有了各种各样的解决办法，但没有一个是完美的。可分为四个主 要领域：    消息传递 : 管道、FIFO、消息队列    同步 : 互斥锁、条件变量、读写锁、信号量    共享内存区 : 匿名共享内存区、命令共享内存区    过程调用 : Solaris 门、Sun RPC    包括单个进程内多个线程的 IPC 和多个进程间的 IPC 。 消息边界  无边界   管道和 FIFO 是字节流，没有消息边界； TCP 没有记录边界的字节流； 有边界   POSIX 消息和 System V 消息有从发送者向接收者维护的边界； UDP 提供具有边界记录的消息； 窥探能力  可以窥探   socket recv、recvfrom 函数可以使用标志 MSG _PEEK 从接收队列读取数据，且系统不在读取之后丢弃这些数据； 不可窥探   管道、FIFO、POSIX 消息、System V 消息 只有一个副本递交到一个线程，且消息不能广播或多播到多个接收者（UDP 广播、多播） 读取顺序  先进先出   管道、FIFO 优先级最高的最早消息   POSIX 消息 指定优先级的消息   System V 消息 对象持续性  随进程持续 process-persistent   IPC 对象一直存在到打开着该对象的最后一个进程关闭该对象为止。没有 unlink 函数。 管道、FIFO、POSIX 互斥锁、POSIX 条件变量、POSIX 读写锁、POSIX 基于内存的信号量、fcntl 记录锁、TCP 套 接字、UDP 套接字、Unix 域套接字 随内核持续 kernel-persistent   一直存在到内核重新自举或显示删除该 IPC 对象。存在对应的 unlink 函数。 POSIX 消息队列、POSIX 有名信号量、POSIX 共享内存区、System V 消息队列、System V 信号量、System V 共 享内存 随文件系统持续 filesystem-persistent   一直存在到显示删除该 IPC 对象为止。 IPC 的一个基本设计目标是高性能，而具备随文件系统的持续性可能会使其性能降级，而且进程不可能跨越自举继 续存活。 名字空间 – name space   一种给定的 IPC 类型，其可能名字的集合称为名字空间。名字空间非常重要，因为名字是客户与服务器彼此连接 以交换消息的手段。    IPC 类型 打开或创建 IPC 的名字空间 IPC 打开后的标识     管道 NA 描述符   FIFO 路径名 描述符   POSIX 互斥锁 NA pthread _mutex _t 指针   POSIX 条件变量 NA pthread _cond _t 指针   POSIX 读写锁 NA pthread _rwlock _t 指针   fcntl 记录上锁 路径名 描述符   POSIX 消息队列 POSIX IPC 名字 mqd _t 值   POSIX 命名信号量 POSIX IPC 名字 sem _t 指针   POSIX 共享内存 POSIX IPC 名字 描述符   POSIX 基于内存的信号量 NA sem _t 指针   TCP 套接字 IP 地址与 TCP 端口 描述符   UDP 套接字 IP 地址与 UDP 端口 描述符   Unix 域套接字 路径名 描述符   Sun RPC 程序/版本 RPC 句柄   门 路径名 描述符   System V 消息队列 key _t 键 System V IPC 标识符   System V 信号量 key _t 键 System V IPC 标识符   System V 共享内存 key _t 键 System V IPC 标识符    打开 IPC 名字空间  无名 IPC   管道、POSIX 互斥锁、POSIX 条件变量、POSIX 读写锁、POSIX 基于内存的信号量； 无名 IPC 也可为进程所共享   互斥锁、条件变量、读写锁、POSIX 无名信号量都是无名的，也就是说他们是基于内存的。他们很容易为单个进程 内的不同线程所共享；当他们存放在不同进程间所共享的内存区中时，同时设置其进程共享属性，也可以为这些进 程所共享。 打开路径名 (path) 来打开 IPC   FIFO、fcntl、Unix 域套接字、门； 打开 POSIX IPC 名字来打开 IPC   POSIX 消息队列、POSIX 命令信号量、POSIX 共享内存； 基于 IP 地址和端口号来打开 IPC   TCP 套接字、UDP 套接字； 基于 key _t 键打开 IPC   System V 消息队列、System V 信号量、System V 共享内存； 打开 IPC 后的标识  描述符   管道、FIFO、fcntl、POSIX 共享内存、TCP 套接字、UDP 套接字、Unix 虞域套接字、门 相应形式的指针     POSIX 互斥锁    POSIX 条件变量    POSIX 读写锁    POSIX 命名信号量    POSIX 基于内存的信号量   值     POSIX 消息队列   System V IPC 标识符     System V 消息队列    System V 信号量    System V 共享内存   RPC 句柄   Sun RPC 需要定义结构体变量–指针   除了 Posix 信号量需要定义成指针，其他的都需要定义成变量。 结构体   互斥锁、条件变量、读写锁、Posix 无名信号量、Posix 消息队列、 描述符、Posix 共享内存、 指针   Posix 命名信号量、mmap、 fork、exec、exit 对 IPC 的影响     无名同步变量（互斥锁、条件变量、读写锁、基于内存的信号量），从一个具有多线程的进程中调用 fork 将变得 混乱不堪；如果这些变量驻留在共享内存区中，而且创建时设置了进程共享属性，那么对于能访问该共享内存区的 任意进程来说，其任意线程能继续访问这些变量。    System V IPC 的三种形式没有打开或关闭的说法，只需知道其标识符即可访问。      IPC 类型 fork exec _exit     管道、FIFO 子进程取得父进程的所有打开着的文件描述符的副本 所有打开着的文件描述符继续打开，除非设置了 FD _CLOEXEC 位 关闭所有打开着的描述符，最后一个关闭时删除管道或 FIFO 中残留的数据   POSIX 消息队列 子进程取得父进程的所有打开着的消息队列描述符的副本 关闭所有打开着消息队列描述符 关闭所有打开着的消息队列描述符   System V 消息队列 – – –   POSIX 互斥锁和条件变量 若驻留在共享内存区中而且具有进程间共享属性，则共享 除非在继续打开着的共享内存区中而且具有进程间共享属性，否则消失 除非在继续打开着的共享内存区中而且具有进程间共享属性，否则消失   POSIX 读写锁 若驻留在共享内存区中而且具有进程间共享属性，则共享 除非在继续打开着的共享内存区中而且具有进程间共享属性，否则消失 除非在继续打开着的共享内存区中而且具有进程间共享属性，否则消失   POSIX 基于内存的信号量 若驻留在共享内存区中而且具有进程间共享属性，则共享 除非在继续打开着的共享内存区中而且具有进程间共享属性，否则消失 除非在继续打开着的共享内存区中而且具有进程间共享属性，否则消失   POSIX 命名信号量 父进程中所有打开着的命名信号量在子进程中继续打开着 关闭所有打开着的命名信号量 关闭所有打开着的命名信号量   fcntl 记录锁 子进程不继承父进程持有的锁 只要描述符继续打开着，锁就不变 解开由进程持有的所有未处理的锁   System V 信号量 子进程中所有 semadj 值都置位 0 所有 semadj 值都携入新进程 所有 semadj 值都加到相应的信号量值上   mmap 内存映射 父进程的内存映射存留到子进程中 去除内存映射 去除内存映射   POSIX 共享内存区 父进程中内存映射存留到子进程中 去除内存映射 去除内存映射   System V 共享内存区 附接着的共享内存区在子进程中继续附接着 断开所有附接着的共享内存区 断开所有附接着的共享内存区   门 子进程取得父进程的所有打开着的描述符，但是客户在门描述符上激活其过程时，只有父进程是服务器 所有门描述符都应关闭，因为它们创建时设置了 FD _CLOEXEC 位 关闭所有打开着的描述符    锁释放  内核自动释放   持有某个锁的进程没有释放就终止，内核自动释放该锁； fcntl 记录锁、System V 信号量（可选项） 无法释放锁   互斥锁、条件变量、读写锁、POSIX 信号量 进程、线程与共享信息   没有任何东西限制任何 IPC 技术只适用于两个进程。    两个进程共享存留于文件系统中某个文件上的某些信息。为了访问这些信息，每个进程都得穿越内核（例如 read、write、lssk 等）；需要某种形式的同步，如记录锁。    两个进程共享驻留于内核的某些信息，访问共享信息的每次操作都涉及对内核的一次系统调用。管道、 System V 消息队列、System V 信号量均是；    两个进程有一个双方都能访问的共享存储区，需要某种形式的同步（信号量等）。每个进程一旦设置好该共享 内存区，就能根本不涉及内核而访问其中的数据。   进程 – 线程  设计线程的原因：     fork 的开销很大。内存映射要从父进程复制到子进程，所有描述符要在子进程复制一份。尽管存在写时复制 (copy-on-write) 的技术，fork 的开销仍然很大。    fork 子进程后，需要使用 IPC 在父子进程之间传递信息。   线程共享全局内存空间   一个进程内的所有线程共享同一个全局内存空间。使得线程间很容易共享信息，但这种易用性也带来了同步 (synchronization) 问题。 线程共享的资源     全局内存空间    进程指令    打开的文件    信号处理程序和信号处置    当前工作目录    用户 ID 和 组 ID   线程私有资源     线程 ID    寄存器集合，包括程序计数器和栈指针    栈，存放局部变量和返回地址    errno    信号掩码    优先级   系统开销     执行一般命令 1ns = 1/1,000,000,000s   从 L1 级缓存读 0.</description>
    </item>
    
    <item>
      <title>Algorithm fourth-edition</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/algotithm4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/algotithm4/</guid>
      <description>算法  算数表达式求值   E.W.Dijkstra&amp;#39;s Two-Stack Algorithm for Expression Evaluation – 1960s    Push operands onto the operand stack    Push operators onto the operator stack    Ignore left parenthesis    On encountering a right parenthesis, pop an operator, pop the requisite number of operands, and push onto the operand stack the result of applying that operator to those operands.    After the final right parenthesis has been processed, there is one value on the stack, which is the value of the expression.</description>
    </item>
    
    <item>
      <title>AutoML</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/automl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/automl/</guid>
      <description>就像使用 CNN 提取特征来代替人工设计的特征，使用 AutoML 代替人设计网络的架构，应该是可以达到更好的效果。 前提   CNN 通常需要大量的时间来设计网络的架构。当然我们可以使用迁移学习，但是 只有针对数据集设计自己的网络才能达到最好的性能 。然而设计网络架构需要专业的技能，且具有很大的挑战（对于商业应用来说代价太大）。 NAS   Neural Architecture Search 就是用于搜索最优网络架构的算法。提供了深度学习的一个新的研究方向。    定义候选 building blocks    使用一个 RNN 作为控制器，用于选择拼装 building blocks    在交叉验证集上，训练拼装好的网络，使其收敛    根据网络的准确率来更新 RNN 控制器（update with policy gradient），希望其能选择出更好的网络架构    In simple terms: have an algorithm grab diierent blocks and put those blocks together to make a network. Train and test out that network.</description>
    </item>
    
    <item>
      <title>C Traps and Pitfalls</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/c-traps-and-pitfalls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/c-traps-and-pitfalls/</guid>
      <description>导读   程序严格按照我们写明的程序来执行，但结果却并不是我们真正希望得到的。 程序设计错误实际上反应的是程序与程序员的“心智模式”两者的相异之处。（心智模式(mental model)解释为人 们深植心中，对于周遭世界如何运作的看法和行为；《列子》中记录有疑邻盗斧） 练习  0-1 返修率高   你是否愿意购买一个返修率很高的厂商所生产的汽车？如果厂家声明它已经做出了改进，你的态度是否会改变？用 户为你找出程序中的 Bug，你真正损失的是什么？  答：我们经常会依据厂商的信誉去购买其商品；会考虑其最近的高质量是真实的还是偶然的；会损失信誉。而信誉 一旦失去，就很难重新获得。 0-2 修建一个 100 英尺长的护栏，护栏的栏杆之间相距 10 英尺，共需要多少根栏杆？   答：11 根。 0-3 菜刀   在烹饪时你是否失手用菜刀切伤过自己的手？怎样改进菜刀使得使用更安全？你是否愿意使用这样一把经过改良的 菜刀？  答：我们很容易想到办法让一个工具更安全，代价是原来简单的工具现在要变得复杂一些。食品加工机一般有连锁 装置，保护使用者不让手指受伤。但是菜刀却不同，给这样一个简单、灵活的工具附加保护手指避免受伤的装置， 只能让其失去简单灵活的特点。实际上，这样做最后得到的也许更像一台食品加工机，而不是一把菜刀。 使其难于做“傻事”常常会使其难于做“聪明事”，正所谓“弄巧成拙”。 第 1 章 词法陷阱   从较低层面考虑，程序是由 符号(token) 序列组成的，将程序分解成符号的过程称为“词法分析” 术语符号指的是程序的一个基本组成单元，其作用相当于一个句子中的单词，在不同的句子中用于相同的意义；但 是组成符号的字符序列就不同，同一组字符序列在某个上下文环境中属于一个符号，而在另一个上下文环境中可能 属于完全不同的另一个符号。 编译器中负责将程序分解为一个一个符号的部分称为“词法分析器” 在 C 语言中，符号之间的空白（包括空格符、制表符或换行符）将被忽略，因此 C 语言书写的格式可以很随意， 但这并不是好习惯。 1.1 = is not ==   C 语言中赋值符号被作为一种操作符对待，因而重复进行赋值操作可以很容易的书写(a=b=c) 不要误用比较运算和赋值运算    比较运算时，如果有常量，将常量方在左侧    如果确实需要赋值运算，明确的表示出来，如下：   if(x = y) foo(); //应写成 if (0 !</description>
    </item>
    
    <item>
      <title>C&#43;&#43;</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/cpp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/cpp/</guid>
      <description>头文件  #include &amp;lt;cctype&amp;gt; // C++ 头文件，开头的 C 表明来自 C 语言，但更符合 C++ 规范#include &amp;lt;ctype.h&amp;gt; // C 语言头文件；不应该在 C++ 中使用// 为啥 C++ 的头文件名都没有 .h 后缀呀？  // IO 库 #include &amp;lt;iostream&amp;gt; // 容器 #include &amp;lt;vector&amp;gt;#include &amp;lt;list&amp;gt;#include &amp;lt;deque&amp;gt;#include &amp;lt;bitset&amp;gt;#include &amp;lt;stack&amp;gt;#include &amp;lt;queue&amp;gt;#include &amp;lt;priority_queue&amp;gt;  命名空间  namespace kyle { // 嵌套命名空间  namespace sanshi { ... } } // 命名空间重命名   作用域 scope     全局作用域：定义在所有函数外部    局部作用域：定义在函数内部    语句作用域：for 循环中，C 语言中不可以在 for 循环语句中定义变量    局部变量 hide 全局变量。 变量   变量都有特定的类型，在 C++ 中称变量为对象。  变量名的标识符区分大小写。  每个类定义一种类型，类型名与类名相同。class 的行为和内置数据类型一样，可以自然使用。  把变量定义到第一次被使用的地方，尽量延迟变量的定义；而不是像 C 语言那样在函数的开头定义变量。  变量都应该初始化。初始化并不是赋值，而是创建变量的同时赋值；  函数体外的变量都初始化为 0，函数体外的变量不自动初始化；类通过默认构造函数初始化。  左值 – 右值  C++ 是一门静态类型语言，编译时会做类型检查  volatile ：告诉编译器不要优化，每次使用时都会重新读取其值  链接指示来调用非 C++ 语言： // &amp;#34;Ada&amp;#34; &amp;#34;FORTARN&amp;#34; extern &amp;#34;c&amp;#34; { .</description>
    </item>
    
    <item>
      <title>Computer Systems - A Programmer&#39;s Perspective</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/csapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/csapp/</guid>
      <description>局部性   局部性良好的代码速度会大大提高  分为时间局部性和空间局部性；时间局部性表明一个变量在不远的将来会再次被访问；空间局部性表明一个变量周围的变量很快会被访问。  代码提升的关键在于充分利用高速缓存 cache (CPU 和主存读写速度之间的差距在不断增大） 程序的机器级表示   汇编代码以及机器码都不包含任何变量名字以及类型信息 。程序就仅仅是一些数字序列而已。  在代码中假如汇编的方法：    函数使用汇编代码实现，然后链接的回收使用    使用 GCC 提供的选项，直接在 C 函数中插入汇编代码   x86-64 16 个通用寄存器    Instruction pointer    %rip   Stack pointer    %rsp   Return value    %rax   Arguments passed in registers    %rdi, %rsi, %rdx, %rcx, %r8, %r9   Callee-saved    %rbx, %r12, %r13, %r14, %rbp, %rsp   Caller-saved    %rdi, %rsi, %rdx, %rcx, %r8, %r9, %rax, %r10, %r11    x86-64 惯例，操作 32 位寄存器，会将该寄存器的高 32 位设置为 0 。 栈   栈的地址向下增长，即每次压栈都会导致寄存器 %rsp 的值减小。大多数函数都需要栈帧（栈上的一段内存）来保存信息，栈帧的结构从 栈底到栈顶依次为    保存的 callee-saved 寄存器的值    局部变量    调用其他函数需要的 6 个参数寄存器以外的内存    返回地址    程序使用栈完成函数调用，此时主要由寄存器 %rip 以及 %rsp 完成    callq 指令完成将 callq 的下一条指令地址压栈，并 jump 到被调函数的第一条指令    ret 指令从栈中弹出之前保存在栈上的地址，并 jump 到该地址    以上是栈固定的时候  动态数组需要不定大小的栈，此时需要寄存器 %rbp 辅助实现。   ret 指令等效于    popd %rip; jump %rip   leave 指令等效于    movq %rbp, %rsp; popd %rbp   堆   由 glic 负责维护，使用 sbrk 系统调用来申请内存，然后 glic 提供 malloc 和 free 接口供应用程序使用。 实现自己的 malloc free 函数   要求：   Handling arbitrary request sequences    malloc 和 free 可能是任意的顺序   Making immediate responses to requests    不可以缓存请求或者对请求重新排序，必须立即响应   Using only the heap    函数使用的结构体 (nonscalar data) 必须存储在 heap 中   Aligning blocks    8 字节对齐   Not modifying allocated blocks    不可以对已经分配的 blocks 进行修改    目标：   Maximizing throughput    每秒钟响应请求的次数   Maximizing memory utilization    heap 峰值使用率（aggregate payload / total heap)    Implementation Issues   Free block origanization    怎样保存 free blocks ；隐士链表，显示链表   Placement    选择合适 free block 作为新申请的 block ； first fit, next fit, best fit   Splitting    是否以及怎样分割 block 中剩余的部分   Coalescing    怎样合并被释放的 block ； 考虑 previous 和 next 是否是 free 共分为四种情况    之前尝试写过几天，但是后来重装系统，换电脑等。 I lost my code.</description>
    </item>
    
    <item>
      <title>convolution</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/convolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/convolution/</guid>
      <description>cs231n   斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。 卷积   卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。  A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.  卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input.</description>
    </item>
    
    <item>
      <title>cuda 编程基础</title>
      <link>https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/</guid>
      <description>学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 -&amp;gt; Decode 译码 -&amp;gt; Execute 执行 -&amp;gt; Memory 访存 -&amp;gt; Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 –&amp;gt; 破坏了独立性、并行性    Granularity 粒度 –&amp;gt; 任务划分的粒度    Observed Speedup –&amp;gt; 加速比    Parallel Overhead 并行开销 –&amp;gt; 通信、同步    Scalability 可扩展性 –&amp;gt; GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model –&amp;gt; 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl&amp;#39;s Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { .</description>
    </item>
    
    <item>
      <title>Darknet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/darknet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/darknet/</guid>
      <description>   darknet 以 layer 为中心，通过构建不同的 layer 构成一个 net ；    所有的层保存在 net.layers 中 net-&amp;gt;layers = calloc(net-&amp;gt;n, sizeof(layer));    darknet 需要一个“.txt”文件，每行表示一张图像的信息： &amp;lt;object-class&amp;gt; &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;width&amp;gt; &amp;lt;height&amp;gt;    每个单元格会预测B个边界框（bounding box）以及边界框的置信度（confidence score）。    所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 Pr(object)，后者记为 预测框与实际框（ground truth）的 IOU 。很多人可能将 Yolo 的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的 乘积，预测框的准确度也反映在里面。    中心坐标的预测值 (x,y) 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的。而边界框的 w 和 h 预测值 是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 [0,1] 范围。通过 sigmoid 函数保证 (x,y) 在 0-1 之间， 这样，每个边界框的预测值实际上包含5个元素：(x,y,w,h,c)，其中前4个表征边界框的大小与位置，而最后一个值是置信度。    加速库： NNPACK </description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/deeplearning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/deeplearning/</guid>
      <description>神经网络和深度学习  神经网络概论     结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等    非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字    人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣 钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也 可以处理非结构化的数据。  每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会人为决定每个神经元 的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神 经网络在监督学习中效果很好很强大。  神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以 及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的 算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法]  scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神 经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好 的 SVM 算法结果要优于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在达到一定的性能之后效果变无 法继续提升，而神经网络此时的效果将明显领先于传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算法 都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速度， 因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，从 而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高（ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值）。速度的提升使得我们可以训练大型的网络或者在一定的时间内完成网络 的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断循环，迭代的更快使得验证自己的想法更加快速得到 验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。  1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任 意m 维到 n 维的映射）。虽然如此，但是若要模拟复杂的函数可能需要特别特别多的隐层神经元，因此现代网络总是加大网络的深度， 以让每一层的函数尽量简单，而整个网络完成复杂的映射。 神经网络基础   一张彩色图像像素点由 RGB 三个通道组成，作为神经网络的输入时，将三个矩阵都转换成向量并拼接起来组成一个列向量 \(x^{(i)} \in \mathbb{R}^{n_x}\)，列向量中先是红色通道的所有像素点，然后是绿色通道的所有像素点，最后是蓝色通道的所有像素点。m 个训 练样本 \(\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(i)},y^{(i)}) \}\) ；同时使用 \(X \in \mathbb{R}^{n_x \times m} \) 表示所有的训练样本\[X= \left[ \begin{array}{cccc} | &amp;amp; | &amp;amp; &amp;amp; | \\ x^{(1)} &amp;amp; x^{(2)} &amp;amp; \cdots &amp;amp; x^{(m)} \\ | &amp;amp; | &amp;amp; &amp;amp; | \end{array} \right] \] 相比于让每个样本按行向量堆叠，在神经网络中构建过程会简单很多。\(y^{(i)} \in \{0,1\}\)同 时将所有的标签组成一个行向量 \(Y \in \mathbb{R}^{1 \times m}\) \[ Y = [ y^{(1)}, y^{(2)}, \cdots, y^{(m)} ]\] 在Python 中使用 (n,m) = X.</description>
    </item>
    
    <item>
      <title>fast.ai</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/fast-ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/fast-ai/</guid>
      <description>调参  分阶段使用多个学习速率   越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last.</description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/r-cnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/r-cnn/</guid>
      <description>预处理     减去 RGB 像素的均值（无论训练还是测试统一使用训练样本集的均值）    Rescale 在图像的长边不超过阈值的情况下，将短边 resize 到指定值   def img_rescale(img, targetSize=600, maxSize=1000): w = img.width h = img.height minDim = min(w,h) maxDim = max(w,h) scale = targetSize / minDim if scale * maxDim &amp;gt; maxSize: scale = maxSize / maxDim img = rescale(img, scale) return img   为什么要这样 rescale 呢？这样仅仅只能让多数的图像的短边统一长度，长边的长度可能各不相同；另外一些图像长边都是最大值，但 是短边各不相同。这样做有什么意义呢？保持图像的横纵比？但是图像大小不一样，要怎样去训练呢？ RoI Pooling   RPN 生成的 RoI 由 (r,c,h,w) 表示，其中 (r,c) 是左上角的坐标，(h,w) 是高和宽。  RoI max pooling works by dividing the h*w RoI window into an H*W grid of sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window into the corresponding output grid cell.</description>
    </item>
    
    <item>
      <title>friends</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/friend/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/friend/</guid>
      <description>搞笑  3-3-05:25   Ross to Chandler: You never look. You just answer. It&amp;#39;s like a reflex. &amp;#34;Do I look fat?&amp;#34; &amp;#34;No.&amp;#34; &amp;#34;Is she prettier than I am?&amp;#34; &amp;#34;No.&amp;#34; &amp;#34;Does the size matter?&amp;#34; &amp;#34;No.&amp;#34; &amp;#34;And it works both ways.&amp;#34;  So you both just know this stuff?  You know, after about 30 or 40 fights, you kind of catch on. 3-12-10:43   It&amp;#39;s from Ross. It&amp;#39;s a love bug.</description>
    </item>
    
    <item>
      <title>Fuck you cancer</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/cancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/cancer/</guid>
      <description>我们从不去想死亡，总是认为那是很遥远的事情，殊不知它随时可能悄然而至。 书籍  你在死亡中探寻生命的意义， 你见证生前死亡呼吸化作死后的空气。 新人尚不可知，故旧早已逝去； 躯体有尽时，灵魂无绝期。 读者呀，趁生之欢愉，快与时间同行， 共赴永恒生命！ --福尔克.格莱维尔 《卡伊利卡》   《当呼吸化为空气》是美国一名外科医生保罗.卡拉尼什诊断出四期肺癌晚期后的作品。作者以医生和患者双重身份，反思医疗和人性， 记录自己的一生。书中有关于生命意义的深沉思索。如何生存，死亡是我们做好的老师。TED 上有其妻子的一个演讲《当死亡降临》  摘抄 遭遇氢弹     “医生马上就到” – 那么多年奋斗即将迎来的人生巅峰，都随着这句话消失了。    无论什么大病，都能完全改变一个病人以及全家人的生活。    重大疾病不是要改变人生，而是要将你的人生打的粉碎。感觉仿佛神技降临，强烈的光突然刺进眼睛，照射出真正重要的事情； 其 实更像有谁刚刚用炸弹炸毁了你一心一意前进的道路，现在必须绕道而行 。    不管我走到哪里，死亡的阴影都会模糊任何行动的意义。    我周围这些人，身上洋溢者自信与抱负的气息，他们的生命有着无线的可能性。他们的生活如同美妙的圣诞颂歌，而我却嵌入了“倒带”的 苦恼。   挣扎     我要逼迫自己，回归手术室。因为我必须学会以不同的方式活着。我会把死神看做威风凛凛、不是造访的贵客， 但心里要清楚，即 使我是一个将死之人，我仍然活着，直到真正死去的那一刻 。    作者认为达尔文和尼采有一个观点是一致的： 生物体最重要的特征就是奋斗求生 。没有奋斗的人生，就像一幅画里身上没有条纹 的老虎。最轻易的死亡有时候并非最好的结局。    我无法前行，我仍将前行。 I can&amp;#39;t go on, I&amp;#39;ll go on.</description>
    </item>
    
    <item>
      <title>Funny of C</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/fun-of-c/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/fun-of-c/</guid>
      <description>变量   所谓变量，其实是内存地址的一个抽像名字罢了;;  在静态编译的程序中，所有的变量名都会在编译时被转成内存地址。机器是不知道我们取的名字的，只知道地址。  不管结构体的实例是什么– 访问其成员其实就是加成员的偏移量。 宏定义   C 语言的宏实现对组成程序的字符进行变换的方式。宏既可以使一段看上去完全不合语法的代码成为一个有效的 C 程序，也能使一段看上去无害的代码成为一个可怕的怪物。比如可以阅读陈皓的 《6 个变态的 C 语言 Hello World 程序》 #define tString(x) #x // 传入参数两侧加上双引号&amp;#34; 。如果入参是变量，其定义必须在宏定义之前 #define toChar(x) #@x // 将参数两侧加上单引号&amp;#39; ， #define conn(x,y) x##y // 连接两个参数  字符串  数组     C 语言中只有一维数组，而且数组的大小必须在编译期就作为常数确定下来。但数组中的元素可以是任何类型 的对象，当然也就可以是另外一个数组    对一个数组只能做两件事：确定数组的大小；获得指向数组下标为 0 的元素的指针。    任何一个数组下标运算都等同于一个对应的指针运算。  对于数组 char s[10]来说，数组名 s 和 &amp;amp;s 都是一样的。 int a[10]; int i = 3; *a = 84; *(a+i) = 22; a[i] = 21; /* 表达式 *(a+i) 即数组 a 中下标为 i 的元素的引用；由于比较常用，所以被简记成 a[i] ； * 又由于 a+i 与 i+a 含义相同，因此 *(a+i) 与 *(i+a) 含义相同，所以 a[i] 与 i[a] 含义相同 */ i[a] = 12; //虽然如此，但强烈不建议这么写   数组的原地就是内容，长度为 0 的数组其并不占据内存。 指针  链表  #include &amp;lt;stdlib.</description>
    </item>
    
    <item>
      <title>Get Things Done</title>
      <link>https://kylestones.github.io/hugo-blog/blog/book/gtd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/book/gtd/</guid>
      <description> YOUR MIND IS FOR HAVING IDEAS, NOT HOLDING THEM. – David Allen  时间管理的本质，是管理我们的心智和行动！ </description>
    </item>
    
    <item>
      <title>Git</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/git/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/git/</guid>
      <description>简介   Git 是用 C 语言开发的分布式版本控制系统。  Git 保存的不是文件差异或者变化量，而只是一系列文件快照。而其他系统在每个版本中记录着各个文件的具体差 异。Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件 系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索 引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。  在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次 提交的作者等相关附属信息，包含零个或多个指向该提交对象的父对象指针。  Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。  保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。 这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致 文件数据缺失，Git 都能立即察觉。Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计 算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符组成。Git 的工作完全依赖于这类指纹 字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的， 而不是靠文件名。  对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged） 安装  $ apt-get install git  配置   命令 git config 专门用来配置或读取相应的工作环境变量，这些环境变量决定了 git 在各个环节的具体工作方 式和行为，存在在三个地方：    /etc/gitconfig 文件：系统中所有用户都普遍适用的配置；使用 git config 时用–system 选项，读写的就 是这个文件    ~/.</description>
    </item>
    
    <item>
      <title>GNU Compiler Collection</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/gcc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/gcc/</guid>
      <description>GCC 由最初的 GNU C Compiler 逐渐演变为 GNU Compiler Collection ，支持 C, C++, Objective-C, Fortran, Ada, Go 语言的编译，当然也包含这些语言的库。不同的语言会有不同的版本， gcc 是 c 语言的编译器。 编译   编译过程通常包含 4 个步骤，可以指定选项来只完成某一步操作：    预处理或预编译 (Preprocessing)，进行宏替换、注释消除、找到库文件 : gcc -E test.c -o test.i    编译 (Compilation) ，编译成汇编代码 : gcc -S test.i -o test.s    汇编 (Assembly) ，生成机器代码（目标代码）: gcc -c test.s -o test.o    连接 (Linking) ，将目标文件和库文件连接起来，生成可执行文件 : gcc test.</description>
    </item>
    
    <item>
      <title>GNU Project Debugger</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/gdb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/gdb/</guid>
      <description>GDB是GNU开源组织发布的一个强大的UNIX下的程序调试工具。  GDB主要帮忙你完成下面四个方面的功能：    启动你的程序，可以按照你的自定义的要求随心所欲的运行程序。    可让被调试的程序在你所指定的调置的断点处停住。    当程序被停住时，可以检查此时你的程序中所发生的事。    动态的改变你程序的执行环境。    预置条件：在编译连接的时候要指定 -g 选项，把调试信息加入到可执行文件。 启动  $ gdb program $ gdb --args program arg1 arg2 ... # 带着执行参数挂载 gdb $ gdb # 先进入 gdb 界面 (gdb) file program $ gdb program core # $ gdb program PID # 程序运行之后再挂载 $ gdb attach PID # attach  帮助  (gdb) help # 列出命令的所有种类 (gdb) help &amp;lt;class&amp;gt; # 查看该类的所有命令 (gdb) helo &amp;lt;command&amp;gt; # 查看该命令的帮助信息   在输入命令名称不完整的时候，gdb 可以自动配置命令，然后执行匹配到没有歧义的命令。若有歧义则提示用户输 入更多信息来批备命令。直接回车将执行上一次执行的命令。 查看源代码  (gdb) list # 查看程序代码，简写 l (gdb) l (gdb) list &amp;lt;first&amp;gt;, &amp;lt;last&amp;gt; # 显示从first行到last行之间的源代码 (gdb) show listsize # 查看 (gdb) set listsize 20 # 修改 (gdb) forward-search &amp;lt;regexp&amp;gt; # 向前搜索源代码；regexp 是正则表达式 (gdb) search &amp;lt;regexp&amp;gt; (gdb) reverse-search &amp;lt;regexp&amp;gt; # 向后搜索源代码 (gdb) info line # 查看源代码在内存中的地址 (gdb) disassemble func # 查看源程序的当前执行时的机器码，这个命令会把目前内存中的指令 dump 出来  程序运行上下文  (gdb) set args arg1 # 这里指可以设置运行参数，不能加程序名 (gdb) show args (gdb) path &amp;lt;dir&amp;gt; # 设定程序运行路径 (gdb) show path # 查看程序运行路径 (gdb) cd /root # (gdb) pwd (gdb) set environment USER=sanshi # 设置环境变量 (gdb) show environment (gdb) info terminal # 显示程序用到的终端模式 (gdb) run &amp;gt; outfile # 重定向程序输出 (gdb) tty /dev/tty1 # 设置输入输出使用的终端设备 (gdb) set logging on # 将命令的输出保存到默认的 gdb.</description>
    </item>
    
    <item>
      <title>How to read a book</title>
      <link>https://kylestones.github.io/hugo-blog/blog/book/how-to-read-a-book/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/book/how-to-read-a-book/</guid>
      <description>HOW TO READ A BOOK  第一篇 阅读的层次  第一章 阅读的活力与艺术   书是写给那些想要把读书的主要目的当作是增进理解能力的人而写。 “readers”：那些今天仍然习惯于从书写文字中汲取大量资讯，以增进对世界的了解的人，就和过去历史上每一个深有教养、智慧的人别无 二致。 许多资讯与知识是从口传或观察而得，或者收音机、电视、网络等，但这些是远远不够的，他们知道还得阅读，而他们也真的身体力行。 新时代的传播媒体是否真能增进我们对自己世界的了解？——我们为了理解一件事，并不需要知道和这件事相关的所有事情。太多的资 讯就如太少的资讯一样，都是一种对理解力的阻碍。换句话说，现代媒体正在以压倒性的泛滥资讯阻碍了我们的理解力。 现代媒体经过太精心的设计，直接将包装后的观点装进自己的脑海中，目的都在让人很容易整理出“自己”的思绪，使得思想形同没有需要 了；但事实并非如此。 主动的阅读   目标：    提醒读者，阅读可以是一件多么主动的事    阅读越主动，效果越好    听众或读者的“接收”，应该像棒球赛中的捕手一样，而不是类似被打了一拳，或者得到一项遗产。捕手的艺术就在能接住任何球的技巧： 快速球、曲线球、变化球、慢速球等等。阅读的艺术也在尽可能掌握住每一种讯息的技巧。 同样的书一个人比另一个人读的好：    这人读的更主动    他在阅读中的每一种活动都参与了更多的技巧    阅读是一个复杂的活动，就像写作一样，包含了大量不同的活动。要达成良好的阅读，这些活动都是不可或缺的。一个人越能运作这些活 动，阅读的效果就越好。 阅读的目标：为获得资讯而读以及为求得理解而读   真正的阅读：没有任何外力的帮助，你就是要读这本书。你什么都没有，只凭着内心的力量，玩味着眼前的字句，慢慢地提升自己，从只 有模糊的概念到更清楚地理解为止。 阅读的定义：这是一个凭借着头脑运作，除了玩味读物中的一些字句之外，不假任何外助，以一己之力来提升自我的过程。 凭着自己的心智活动努力阅读，从只有粗浅的了解推进到深入的体会，就像是自我的破茧而出。 这里的“学习”指的是理解更多的事情，而不是记住更多的资讯。 任何一个可以阅读的人，都有能力用这样的方式来阅读。只要我们努力运用这些技巧在有益的读物上，每个人都能读的更好，学的更多， 毫无例外。 阅读就是学习：指导型的学习，以及自我发现型的学习之间的差异   所谓吸收咨询，就是知道某件事发生了；想要被启发，就是要去理解，搞清楚这到底是怎么回事：为什么会发生，与其他的事实有什么关 联，有什么类似的情况，同类的差异在哪里等等。 要能被启发，除了知道作者所说的话之外，还要明白他的意思，懂得他为什么这么说。 蒙田说：初学者的物质在于未学，而学者的无知在于学后。——第二种无知是读错了许多书，总有一群书呆子读得太广，却读不通，被 称为“半瓶醋”（Sophomores） 自我发现型的学习是必要的—这是经由研究、调查或无人指导的状况下，自己深思熟虑的一种学习过程。 “辅导型的自我发现学习”：类似医生努力为病人做许多事，但最终的结论是这个病人必须自己好起来，变得健康起来；农夫为他的植物或 动物做了许多事，结果是这些植物必须长大，变得更好；老师用尽了方法来教学生，学生却必须自己能学习才行。当他学习到了，只是就 会在他的脑海中生根发芽。 无论那种学习方式，都不该没有活力，就像任何阅读都不该死气沉沉。 辅助型自我发现学习及非辅助型自我发现学习之间的差异—一个最基本的不同点就在学习者所使用的教材上。辅助型自我发现学习是阅 读自我或世界的学习；非辅助型学习是阅读一本书，包括倾听，从讲解中学习的一种艺术。 “思考”是指运用我们的头脑去增加知识或理解力。思考是在这两种学习中都会出现的东西，在阅读与倾听时我们必须思考，就像我们在研 究时一定要思考一样。 思考是主动阅读的一部分，还必须观察、记忆，在看不见的地方运用想象力。 阅读的艺术包括了所有非辅助型自我发现学习的技巧：敏锐的观察、灵敏可靠的记忆、想象的空间，再者就是训练有素的分析、省思能力。 因为阅读是一种经过帮助后的发现。 老师的出席与缺席   如果你问一本书一个问题，你就必须自己回答这个问题。只有在你自己作了思考与分析之后，才会在书本上找到答案。因此我们就要懂得 如何让书本来教导我们。 第二章 阅读的层次     读者要追求的目的决定会决定他阅读的技巧；    阅读的效果取决于他在阅读上花了多少努力与技巧。 基础阅读—elementary reading     这个阶段面对的主要问题：如何认出一页中的一个个字 不论我们身为有多精通这样的阅读技巧，我们在阅读的时候还是一直会碰上这个层次的阅读问题。 典型问题：这个句子在说什么？ 检视阅读—inspectional reading   特点在强调时间：必须在规定的时间内完成阅读 检视阅读是系统化略读（skimming systematically）的一门艺术 典型问题：这本书在谈什么？ 许多优秀的读者都忽略了检视阅读的价值。 分析阅读—analytical reading   一个分析阅读着一定会对自己所读的东西提出许多有系统的问题。 分析阅读就是特别在追寻理解的；分析阅读永远是一种专注的活动；读者会紧抓住一本书，一直读到这本书成为他自己的为止；分析阅读 就是要咀嚼消化一本书。 主题阅读—syntopical reading   需要读者阅读很多书，并列举出这些书之间的相关之处，且能架构出一个可能在哪一本书里都没有提过的主题分析。 主题阅读是最主动、也最花费力气的一种阅读；但确是所有阅读活动中左右收获的，所以绝对值得你努力学习如何做到这样的阅读。 第三章 阅读的第一个层次：基础阅读  基础阅读   在阅读每一位作者的作品时要相当顺手，用不着停下来检查许多生字的意思，也不会被文法或者文章的结构阻碍住。虽然不见得要每句每 字都读得透彻，但你已经能掌握主要句子与章节的意义了。 只有当他能自己阅读时，才能够自己开始学习，也只有这样他才能变成一个真正的阅读高手。 更高层次的阅读   一个人文素养优良的高中，就算什么也没做，也该培养出能够分析阅读的读者。一个优秀的大学，就算什么也没有贡献，也该培育出进行 主题阅读的读者。 第四章 阅读的第二个层次：检视阅读   阅读的层次是渐进积累的：基础阅读包含在检视阅读中，监视阅读又包含在分析阅读中，分析阅读则包含在主题阅读中。 检视阅读   检视阅读分两种：有系统的略读或粗读，粗浅的阅读。有经验的阅读者已经学会同时运用这两个步骤。 检视阅读的两个方式都需要快速的阅读。一个熟练的检视阅读这想要读一本书时，不论碰到多难读或者多长的书，都能够很快的运用这两 种方式读完。 检视阅读在主题阅读中占有非常重要的角色。 有系统的略读或粗读   你花在略读这本书上的时间绝没有浪费 目的：   帮助你决定一本书是否值得一读，以及从基本架构上先找到一些想法。 步骤     先看书名，然后如果有序就先看序    很快看过去。在脑海中将这本书归类为某个特定的类型，在那个类型中，已经包含了哪些书。    研究目录页    对这本书的基本架构做概括性的理解。就像是出发旅行之前，要先看一下地图一样。目录纲要还是很有价值的，在你开始阅读整本书之前， 你应该先仔细阅读目录才对。    如果书中附有索引，也要监视一下    用于快速评估一下这本书涵盖了哪些议题的范围。如果发现列举出来的哪一条词汇很重要，至少看一下引用该词汇的那几页。    读出版社的介绍    许多宣传文案都是作者在出版公司企宣部门的协助下亲自写就的，作者会尽力将书中的主旨正确的摘要出来。 至此，你已经有足够的咨询来判断该书是否值得一读。    挑几个看来跟主旨息息相关的篇章来看    如果这些篇章的开通或结尾有摘要说明，就要仔细阅读这些说明。    最后一步，把书打开，东翻翻西翻翻，念个一两段，有时候连续读几页，但不要太多    最终要的是，不要忽略书做后的两三页，一般作者都会在这里将认为即新又重要的观点重新整理一遍。 现在，在花了几分钟，最多不过一小时的时间内，你对这本书已经了解了很多了。 PS：这是一种非常主动的阅读。一个人如果不够灵活，不能够集中精神来阅读，就没法进行监视阅读。有多少次你在看一本书好书的时候， 翻了好几页，脑海却陷入了白日梦的状态中，等清醒过来，竟完全不明白自己刚才看的那几页在说些什么？如果遵循上述步骤就不会发生 这样的事，因为你始终有一个可以遵循作者思路的系统了。 你可以把自己想象成一名侦探，在找寻一本书的主题或思路的线索；随时保持敏捷，就很容易让一切状况清楚。 粗浅的阅读   头一次面对一本难读的书的时候，从头到尾先读一遍，碰到不懂的地方不要停下来查询或思索。 只注意你能理解的部分，不要为一些没法理解了解的东西而停顿。继续读下去，略过那些不懂的部分，很快你会读到你看的懂的地方。集 中精神在这个部分，继续读下去，将全书读完。这样就算你之理解了50%或者更少，当你读第二遍的时候，都能帮助你理解第一遍略读的 部分；就算不重读，对一本难度很高的书籍了解一半也是很不错的。如果一碰上困难就停住，最后就可能对这本书真的一无所知了。我们 经常被教导，遇到不懂的要去查字典、查资料，但是时候不到就做这些事，却只会妨碍我们的阅读，而非帮助。譬如，阅读莎士比亚的戏 剧会获得极大的快乐。但是一代代高中生被逼着要一幕幕的念，一个生字一个生字的查、一个注脚一个注脚的读，这种快乐就被破坏了， 等他们读到最后的时候，已经忘了开始是什么，也无法洞察全剧的意义了。 比如，去阅读亚当.</description>
    </item>
    
    <item>
      <title>LaTex</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/latex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/latex/</guid>
      <description>\(\LaTeX\) 是一个文档准备系统 (Document Preparing System)，它非常适用于生成高印刷质量的科技类和数学 类文档。它也能够生成所有其他种类的文档，小到简单的信件，大到完整的书籍。 \(\LaTeX\) 使用 \(\TeX\) 作 为它的排版引擎。  \(\TeX\) 是高德纳 (Donald E.Knuth)开发的、以排版文字和数学公式为目的的一个计算机软件。高德纳从 1977 年开始开发 \(\TeX\) ，以发掘当时开始用于出版工业的数字印刷设备的潜力。正在编写著作《计算机程序设计艺 术》的高德纳，意图扭转排版质量每况愈下的状况，以免影响他的出书。我们现在使用的 \(\TeX\) 排版引擎发布 于 1982 年，在 1989 年又稍加改进以更好地支持 8-bit 字符和多语言排版。\(\TeX\) 以其卓越的稳定性、跨平 台、几乎没有 Bug 而著称。1990 年推出 3.1 版, 并宣布不再更新 (只修正 bug)。\(\TeX\) 的版本号不断趋近 于 \(\pi\)，当前为 3.141592653。  \(\TeX\) 读作 &amp;#34;Tech&amp;#34; ，其中 &amp;#34;ch&amp;#34; 的发音类似于 &amp;#34;h&amp;#34; ，与汉字“泰赫”的发音类似。\(\TeX\) 的拼写来自希 腊词语 technique （技术） 的开头几个字母。在 ASCII 字符环境，\(\TeX\) 写作 TeX。\(\TeX\) 系统提供了 300 + 600 多条基本的排版命令。  \(\TeX\) 提供的命令都是一些很底层的命令, 普通用户使用起来不太方便；大牛们在 \(\TeX\) 基础上, 定义新的命令, 为普通用户排版提供方便    Plain TEX : 由 Knuth 开发, 新定义 600 多条命令, 是通常所说的 TEX    AMS-TEX : 由美国数学会开发, 适合排版各种复杂的数学公式    LATEX (1984) : 由 Lamport 开发, 适合论文书籍, 对 TeX 推广贡献巨大    pdfTEX (1997): 由 H.</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/machine-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/machine-learning/</guid>
      <description>ML   *机器学习* machine learning: Field of study that gives computers the ability to learn without being explicitly program.  机器学习并不是仅仅是若干算法的堆积，学会“十大算法”，熟练掌握具体算法算法的推导与编程实现，并不能让所有问题迎刃而解，因为 现实世界的问题千变万化。而应该像张无忌那样，忘记张三丰传授的太极剑法的具体招式，而只记住一些规则和套路，从而根据敌人的招 式去不断变化自己的招式，达到以不变应万变的效果。或者说用 Andrew Ng 的话，要成为一个 master carpenter （顶级木匠），可以 灵活使用工具来制造桌椅，只有手艺差的木匠才会抱怨工具不合适。因此必须把握算法背后的思想脉络，针对具体的任务特点，对现有套 路进行改造融通；要记住算法是 死 的，思想才是 活 的。  数据库提供数据管理技术，机器学习提供数据分析技术。 Supervised Learning   样本有标签的时候称为监督学习 Linear Regression   首先可以根据样本输入的维数来选择参数的个数（最终依据交叉验证的结果来选择模型和特征） \[ h_{\theta}(x) = \sum_{i=1}^{n} \theta_i x_i = \theta^{T}x \] 求取 \(\theta\) 的策略：在训练集上使得 \(h(x)\) 尽可能接近 y 。那么就涉及到距离的定义，距离通常定义为两者差的平方。因此 损失函数(cost function)定义为 \[ J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 \]  认为数据服从高斯分布 \(y|x;\theta \thicksim (\mu, \sigma^2)\) ，即其前置概率估计是高斯分布。 Last Mean Squares Algorithm   利用最小二乘法中的误差函数来描述损失函数；利用梯度下降法不断迭代来减小损失函数：先将参数初始化为某个值，然后不断迭代跟新 参数；跟新参数的规则：old value + 学习率 * 损失函数对该参数求偏导。  \begin{align} \theta_j &amp;amp; := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\ &amp;amp; := \theta_j + \alpha \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}, \quad for \ every \ j \\ &amp;amp; := \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}, \quad for \ every \ j; \ outer \ for \ every \ i \\ \end{align}  最小二乘法(ordinary least squares)仅仅是描述损失函数的模型，本身不是一个最优解工具，可以求解线性以及非线性问题。最优化方 法有：梯度下降法；矩阵解法；牛顿法；坐标上升法；  判断收敛：    两次迭代，发现 \(\theta\) 变化不大    \(J(\theta)\) 基本不变 【常用】   最优化方法  梯度下降法   梯度下降法：结果与初值有关，且最终一定会结束在某个局部最小值。若维度非常高时，可能不存在局部最小值，可能只是鞍点（深度学 习）。数学上，某一函数在该点的方向导数沿着梯度方向取得最大值。  梯度下降法具体包括：    batch gradient descent，批量梯度下降法：每次都需要计算所有样本的误差来更新参数    stochastic gradient descent / incremental gradient descent，随机梯度下降法：每次只需要计算一个样本的误差来更新参数。 可以较快的收敛，但参数结果可能始终在最优解周围徘徊，但在实际应用中可以满足    mini-batch：每次使用同样个数的一小批的样本来更新权重参数。   Newton&amp;#39;s method   牛顿法用于查找一个函数的零点。假如希望找到使得 \(f(\theta) = 0\) 的 \(\theta\) 值。在 \(\theta\) 处，用函数 f 的切线来近 似 f ，找到切线的零值点；然后在切线的零值点开始继续用该点处的切线来近似表示函数，不断迭代直至参数满足要求。计算零值点时 可以利用直角三角形一个角的对边的长度除以临边的长度来得到该角的正切值，而正切值就是函数在该点的导数。  \begin{equation} \theta := \theta - \frac{f(\theta)}{f&amp;#39;(\theta)} \end{equation}  而求解似然函数的极值点，就是找到似然函数导数的零点 \begin{equation} \theta := \theta - \frac{\ell &amp;#39; (\theta)}{\ell &amp;#39;&amp;#39;(\theta)} \\ \theta := \theta - H^{-1} _{\theta} \nabla \ell(\theta), \quad Newton-Raphson \end{equation}  牛顿法：    优点：初值不影响结果，一般选择 0 为初值；算法一般都会收敛且收敛速度比梯度下降法快很多，是二次收敛(quadratic conversions)[在解距离最优解足够近时，下一次误差为上一次误差的平方]    缺点：每次迭代都需要计算 Hessian 矩阵（n*n维）。如果特征维数较多，代价会比较大。   Maximum Likelihood Estimation - MLE     似然：参数的函数    概率：数据的概率    极大似然估计：选择参数 \(\theta\) 使得数据出现的可能性尽可能大。  让所有样本出现的概率最大，需要将每个样本的概率都乘起来，而连乘操作易造成数据下溢，因此通常采用对数似然(log-likelihood)， 将连乘转换成相加操作。  贝叶斯学派认为参数是未被观测的随机变量，其本身也可有分布。频率学派认为虽然参数未知，但确实客观存在的固定值。极大似然估计 源自频率学派。 Locally Weighted Linear Regression   只利用待预测样本周围的样本来预测结果。  \begin{align} w^{(i)} = exp \left( - \frac{(x^{(i)}-x)^2}{2\tau^2} \right) \\ \sum_i w^{(i)} (y^{(i)} - \theta^T x^{(i)})^2 \end{align}    x 是待预测的值，样本距离 x 的距离 \( |x^{(i)} - x| \) 越近，权重越大；反之则越小。    \(\tau\) 称为波长参数，是一个超参，控制着权重随距离下降的速度。\(\tau\) 越小，权重衰减的钟形越窄；\(\tau\) 越大，权重 衰减的钟形越宽。而且权值函数的积分可能是正无穷而不是像高斯密度函数那样积分值为 1    局部加权线性回归并没有依据训练样本学习得到一些参数，而是在每次预测的时候都需要使用训练样本来决策。是一个非参数学习算法 (non-parametric)，非正式的可以理解为其参数随着训练样本的增加而增加。  Andrew Moore 的 KD tree 讲述了在训练集较多时，高效计算的方法。 Logistic Regression   逻辑斯蒂回归用于处理二分类问题。由于标记只有两个：0 和 1。所以当计算的结果超过 1 或者小于 0 将变得没有意义。因此选取一个 函数值从 0 平滑过度到 1 的函数用于将计算求得的结果重映射到区间[0-1]。这样的函数存在很多，但是逻辑斯蒂回归选择了 sigmoid function，也称为 logistic function。利用广义线性模型也会推导出此处选择 sigmoid 函数。  \begin{align} g(z) = \frac{1}{1+e^{-z}} \\ h_{\theta}(x) = g({\theta}^T x) = \frac{1}{1 + e^{-{\theta}^T x}} \end{align}  假设  \begin{align*} P(y=1|x;θ) &amp;amp; = hθ (x) P(y=0|x;θ) &amp;amp; = 1 - hθ (x) P(y|x;θ) &amp;amp; = (hθ (x))^y (1 - hθ (x))1-y \end{align*}  似然函数：损失函数没有选择成算法输出与标记误差的平方，是因为这样的损失函数是非凸的，会有很多局部最优解而无法找到全局最优 解。  \begin{align*} ℓ(θ) &amp;amp; = ln L(θ) &amp;amp; = ln ∏i=1^m p(y|x;θ) &amp;amp; = ln ∏i=1^m (hθ (x(i)))^{y(i)} (1 - hθ (x(i)))^{1-y(i)} &amp;amp; = ∑i=1^m y(i) ln h(x(i)) + (1-y(i))ln(1-h(x(i))) \end{align*}  目标函数对 \(\theta\) 的每个分量求偏导，且求解过程直接使用 sigmoid 函数的导数表达式可简化计算。最终得到参数的迭代表达式  \begin{align} \frac{\partial}{\partial \theta_j} \ell(\theta) &amp;amp; = (y-h_{\theta}(x))x_j \\ \theta_j &amp;amp; := \theta_j + \alpha \sum_{i=1}^m(y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \\ \theta_j &amp;amp; := \theta_j + \alpha(y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \end{align}  sigmoid 函数的导数： \begin{equation} g&amp;#39;(z) = g(z) (1-g(z)) \end{equation}  认为数据服从伯努利分布 \(y|x;\theta \thicksim Bernoulli(\phi)\) ，即数据的前置概率估计是伯努利分布。    TP – 将正类预测为正类的个数；true positive    FN – 将正类预测为负类的个数；false negative    TN – 将负类预测为负类的个数；true negative    FP – 将负类预测为正类的个数；false positive    准确率、召回率、以及两者的调和均值（准确率和召回率都高的时候也会高）： \begin{gather} P = \frac{ TP }{ TP + FP } \\ R = \frac{ TP }{ TP + FN } \\ \frac{2}{F_1} = \frac{1}{P} + \frac{1}{R} \\ F_1 = \frac{2TP}{2TP + FP + FN} \end{gather} Perception Learning Algorithm   sigmoid 函数并没有直接输出样本的类别标记，而是根据输出再与 0.</description>
    </item>
    
    <item>
      <title>make</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/make/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/make/</guid>
      <description>make 可以自行决定一个大型程序中的哪些源文件需要重新编译，并根据指定的命令对其进行重新编译。make 不仅 可以用于 C 语言的编译，其可以用于任何可以使用 shell 命令进行编译的语言。而且 make 并不仅仅限于编译某 种语言，也可以使用 make 工具来做一些其它的事。例如，有这样的需求：当我们修改了某个或者某些文件后，需 要能够根据修改的文件来自动对相关文件进行重建或者更新。  需要编写 makefile 文件来使用 make 这个工具。makefile 用于告诉 make 需要编译哪些文件，以及如何编译。 makefile 文件中描述了文件之间的关系，并制定了一些命令用于编译更新。一般来说，一个可执行程序文件依赖 一些目标文件，而这些目标文件有某些源文件编译得到。一旦完成了一个有效的 makefile 文件，每次只需要执行 make 命令便可以完成增量编译。make 根据文件的最后修改时间来决定哪些文件需要被更新。 edit : main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \  insert.o search.o files.o utils.o main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.</description>
    </item>
    
    <item>
      <title>mAP</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/map/</guid>
      <description>词语解释   mAP 是 Mean Average Precision 的缩写，是检测算法的评价指标。这个名词中包含两个平均，其中 Mean 取的是不同类别的平均值， Average 取的是不同的召回率的平均值，当然计算的是正确率 Precision 的平均值。另外 coco 数据集还取了不同 IoU 阈值的平均。  另外想说一下，recall 这个单词被国内广泛翻译称召回率，大多数人根本无法从字面理解这个召回率到底是个什么鬼。而周志华老师在 其西瓜书中，将 recall 翻译成查全率，将 precision 翻译成查准率。一下子直观了很多。查准率表示模型查找到结果的准确率，就是 你说这些都是好瓜，但其中真正是好瓜的比例；查全率表示模型找到所有正样本的比例，就是说在所有好瓜中，你判别出来了多少。 计算方法   不同的数据集有不同的 mAP 计算方法。主要包括 PASCAL 07 、PASCAL 10、COCO 数据集的计算方法较常用。  PASCAL 数据集使用的都是固定的 IoU 阈值（默认为 0.5），就是只要预测的 box 和真实的 box 的 IoU 大于等于 0.5 ，就认为检测正 确（当然类别也必须正确）。所不同的是 PASCAL 07 只计算 11 个查准率 precision 的平均值，而 PASCAL 10 则要求所有的检测结果 都用于计算 AP 。  PASCAL 07 只计算查全率 recall 在 0.</description>
    </item>
    
    <item>
      <title>Mask R-CNN</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/</guid>
      <description>训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0.</description>
    </item>
    
    <item>
      <title>mpayer</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/mplayer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/mplayer/</guid>
      <description>拷贝路径 mplayer 参数说明  格式有点凌乱，待整理。 TODO  MPlayer 名称 概要 说明 一般注记 播放选项 ( 仅用于 MPLAYER) 分路器 / 媒体流选项 OSD/ 字幕选项 音频输出选项 ( 仅用于 MPLAYER) 视频输出选项 ( 仅用于 MPLAYER) 解码 / 滤镜选项 编码选项 ( 仅用于 MENCODER) 键盘控制 SLAVE 模式协议 文件 示例 BUGS 作者 标准声明 名称 mplayer − Linux下的电影播放器 mencoder − Linux下的电影编码器  概要 mplayer [选项] [ 文件 | URL | 播放列表 | - ] mplayer [全局选项] 文件1 [特定选项] [文件2] [特定选项] mplayer [全局选项] {一组文件和选项} [针对该组的特定选项] mplayer [dvd|vcd|cdda|cddb|tv]://title [选项] mplayer [mms[t]|http|http_proxy|rt[s]p]:// [用户名:密码@]URL[:端口] [选 项] mencoder [选项] [ 文件 | URL | - ] [−o 输出文件] gmplayer [选项] [−skin skin]  说明 mplayer 是一个LINUX下的电影播放器, (也能运行在许多其它的Unices 和 非x86 的CPU 上, 参看文档).</description>
    </item>
    
    <item>
      <title>mxnet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/gluon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/gluon/</guid>
      <description>mxnet 常用包  from mxnet import gluon # 提供简单易用的 mxnet 接口 from mxnet import nd from mxnet import init # 用于权重参数初始化 from mxnet import autograd # 自动求导 from mxnet.gluon import nn # 用于构建网络结构 from mxnet.gluon import data as gdata from mxnet.gluon.data.vision import transforms # 用于变换数据 import sys import time  常用函数    transfroms.Compose    实现数据格式的转换，转换成 (height, width, channel) 格式，以及变成浮点数；这是 mxnet 要求的格式； 同时可以实现 argument   gluon.</description>
    </item>
    
    <item>
      <title>Org-mode笔记</title>
      <link>https://kylestones.github.io/hugo-blog/blog/tools/org-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/tools/org-mode/</guid>
      <description>概要     以 # 号后加一空格开始的行表示注释，文件导出时这些内容不被导出，上面第一行就是。    以 #+ 符号开始的行用于设置文档参数或内容属性，比如文档的标题、作者，org-mode打开时文档的呈现状态等。    用  括起来的内容表示外部链接    用 &amp;lt;&amp;lt;&amp;gt;&amp;gt; 括起来的内容表示文档的内部链接    以 * 符号开始的行，表示该行为标题。    标题内容前的 TODO 标记是待办任务的标记符号。    标题行后面两个 : 符号间的内容表示标签（ TAG ）。    标题行如果标题文本前有 COMMENT 标记表示该标题下的所有内容为注释。    位于 #+BEGIN_XXX 和 #+END_XXX 之间的内容为特殊文档块，如代码块、例子、引用等。    C-c C-x C-h查看按键的帮助信息   元数据  文档元数据  具体包括： #+TITLE: the title to be shown (default is the buffer name) #+AUTHOR: the author (default taken from user-full-name) #+DATE: a date, an Org timestamp1, or a format string for format-time-string #+EMAIL: his/her email address (default from user-mail-address) #+DESCRIPTION: the page description, e.</description>
    </item>
    
    <item>
      <title>Positive Psychology</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/positive-psychology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/positive-psychology/</guid>
      <description>为什么需要积极心理学     需要集中研究，带来更大的收货    并不是治愈了抑郁，就会快乐    积极心理学可以有效预防压抑等   Question very often create reality   测试实验：30 s 观察一幅画，问这幅画中有多少个几何图形？那么在这 30 S 内，测试者会进全力去数这幅画的几何图形的个数，但是 由于这幅画里有太多太多的几何图形，被测试者在有限的 30s 内根本没有办法数过来。而再给这些测试者提问。图片上的大巴里面有几 个孩子？左下角的钟表的时间是几点？右侧的颜色？几乎没有几个人知道。  这种注意力可以帮助我们投入到一件事情上，但是如果只是给自己提问这一个问题，那么我们忽略其他。  生活中，大多数人会问自己什么问题。我的缺点是什么，有什么不足？这没有什么不对，但是，如果我们只是给自己提问这一个问题，我 们会忽略掉我们的优点，我们将只能看到自己的缺点。again，question create reality.  Stavros and Torres 一本关于婚姻的书中说：We see what we look for and we miss much of what we are not looking for even though it is there. Our experience of the world is heavily influenced by where we place our attention.</description>
    </item>
    
    <item>
      <title>python</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/python/</guid>
      <description>Python 常识 调试  logging   logging.debug ，不要再胡乱使用 print 了 use pdb     在Python 文件内 import pdb ，然后在需要调试的开始行添加 pdb.set_trace()    shell 中 python -m pdb test.py    pdb.run()    使用 args 查看入参，没有查看局部变量的方法 axis   axis = 0 表示第一维数据，axis = -1 表示最后一维数据；就是 np.array(.).shape 输出的顺序 数组索引     Python 可以使用数组作为数组的索引   # 生成数组 # 切片可以指定 start:stop:step , a[::-1] 可以将数组逆序 &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; np.</description>
    </item>
    
    <item>
      <title>shell</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/shell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/shell/</guid>
      <description>ssh  隧道  top  vmstat  iostat  ps  tar  grep  sort  # -u 重复关键字只保留第一个 # -k 知道排序字段（开始,结束）和类型，可通过 info sort 具体查看 # 2n,2 表示安装第二个字段，以数字顺序排序。默认安装字段顺序 # 1rn,1 表示按照数字逆序排序 sort -u -k 2n,2 file | sort -k 1rn,1 -k 3,3  sed   sed 全名叫 stream editor，流编辑器（非交互）。sed 基本上就是玩正则模式 匹配。sed 比 awk 大 2-3 岁。 基本语法  $ sed options file # 并不修改文件，只将处理的结果打印出来 $ sed -e &amp;#39;s/hello/Hello/g&amp;#39; filename # 使用 -i 将直接修改文件 $ sed -i &amp;#39;s/^/# /g&amp;#39; filename # 同时处理多条语句 $ sed -e &amp;#39;2,5s/hello/Hello/; s/world/(&amp;amp;)/g&amp;#39; filename $ sed -e &amp;#39;1,3s/hello/Hello/3&amp;#39; -e &amp;#39;3,$s/world/World/3g&amp;#39; filename # 读取 sed 脚本处理文件 $ more rep.</description>
    </item>
    
    <item>
      <title>something</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/summarize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/summarize/</guid>
      <description>https://pan.baidu.com/s/1hs3Z2ao https://www.aliyun.com/zixun/wenji/1283158.html https://www.cnblogs.com/objectDetect/p/5947169.html https://www.zhihu.com/question/43609045/answer/132235276 https://blog.csdn.net/shentanyue/article/details/82109580?utm_source=blogxgwz1 https://blog.csdn.net/qq_33783896/article/details/80675398 网络架构     解释resnet、优缺点以及适用范围    解释inception net、优缺点以及适用范围    densenet结构优缺点以及应用场景    dilated conv优缺点以及应用场景    moblenet、shufflenet的结构   卷积核参数   一个标准卷积层参数的个数是 input*ksize*ksize*output ，不知道为什么自己会两次犯了相同错却仍然不自知。  如果卷积分组 group ，那么每个卷积核的大小将变小，变为 imput/group*ksize*ksize ，一个 group 的参数为 input/group*ksize*ksize*output/group ；所有 group 组成整个卷积层的参数，input/group*ksize*ksize*output/group*group = input/group*ksize*ksize*output 。即参数会变为原来的 1/group 倍。  deepwith conv 对输入的每一个 channel 分别进行独立的卷积操作，此时输入的 channel 个数必然等于输出 channel 的个数，此卷积 层参数的个数为 ksize*ksize*input。 感受野   vgg 论文上说明两次 3x3 卷积可以达到 5x5 卷积核的效果、三层 3x3 卷积可以达到 7x7 卷积核的效果； 现在终于理解作者的意思， 使用两次 3x3 卷积，第二层卷积核的感受野就是 5x5 ，而三层 3x3 卷积，第三层卷积的感受野为 7x7 。  感受野计算公式： Dilated conv vs Deconvolution     Dilated convolution 在卷积核的每两个值中间插入 d-1 个空洞    Deconvolution 在 feature map 上插入像素值为 0 的点    Unlike dilated convolutions, which have same output size as input size (if input borders are properly padded) &amp;#34;deconvolution&amp;#34; layers actually produce upsampling (larger input then output)    数学公式：  \begin{align*} Z &amp;amp;= W ⋅ A dW &amp;amp;= dZ ⋅ A dA &amp;amp;= W^T ⋅ dZ \end{align*} Dilated convolution   The goal of this layer is to increase the size of the receptive field (input activations that are used to compute a given output) without using downsampling (in order to preserve local information).</description>
    </item>
    
    <item>
      <title>TensorFlow</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/tensorflow/</guid>
      <description>架构   阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。  Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。  Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。  Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：    普通边：用于承载 Tensor，常用实线表示；    控制依赖：控制节点的执行顺序，常用虚线表示。    TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。  计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。 架构设计   TensorFlow 遵循良好的分层架构：    front end ： 用户接口，负责构造计算图    runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现    计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速    通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA    设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统     Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。  此时，TensorFlow 并未执行任何的图计算，直至与后台计算引擎建立 Session，并以 Session 为桥梁，建立 Client 与 Master 之间的 通道，并将 Protobuf 格式的 GraphDef 序列化后传递给 Master，启动计算图的执行过程。 runtime  master   在分布式的运行时环境中，Client 执行 Session.</description>
    </item>
    
    <item>
      <title>The New King of Comedy</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/the-new-king-of-comedy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/the-new-king-of-comedy/</guid>
      <description>  星爷的新喜剧之王，影评很差，不过我看苦了。 自己的认真   自己始终在努力演戏，    关键是想象力，你必须用心体会当时的情境    我知道机会难得，我愿意等。永远是多远呀？那宇宙毁灭之后呢？   众人的嘲笑     她是啥子    被直接扔出来    你有病呀，有病就去看病，别在这里传染我    你还到我面前找骂，你是不是贱呀？做个反应要什么前因后果，回回都是你，是吧    你再说一遍，我怕我听错了    被王宝强踩脸    你不照镜子吗？你觉得你有机会吗？你没有。你不用等，永远都没有。永远就是从现在开始到宇宙毁灭。人类最大的灾难就是这种人， 又不行，又没有自知之明，还不死心，整天出来搞事情    这是命；这就是命，别跟命运做斗争了好吗？ – 致命打击   入围选角   王宝强告知被周星驰选角之后，依然发现好多人，好多人在形体动作、情绪掌控、才艺展示上面有很厉害的表现，远超自己。自己的才艺 表演却只是，我可以用手发声。 家人的不理解   由于做了十几年的临时演员，始终没有很好的发展。在父亲的生日上被赶出来。 男友的背叛     蠢人好，蠢人多快乐。    这不是骗，我是收费的。    你会成功的，你是最棒的，你再给我钱，我继续说，说多少次都可以。   自我放弃   没有，我不是演员，我醒了。 someone 的认可     你做的那个寒冰掌好有层次，我觉得很到位    我不知道她的名字，但我知道她是一个演员   自己错误   对于不幸被车撞的大爷的真身表情，在以自己的演员修养来分析，认为大爷多处“表演”欠缺。 观后感   我想做演员，每天都超级努力，十几年的坚持，然而每天得到的都是讽刺嘲笑。不过仍然每天告诉自己要坚持。整容失败后，被成功选角， 而且看到墙上的标语“奇迹在这里发生”，突然感觉自己好像熬出了头，但是随后得到的确是一顿暴打。然后继续忍受着别人的大骂。终于 面对直到宇宙毁灭也没有机会，这就是命，再加上男友的背叛，选择放弃。然后王宝强的鼓励，重燃希望之火，到北京参加选角，但是仍 然看到太多太多远比自己优秀的人。不过最后被成功选上，且一年后选为最佳女主角（感觉有点快呀）  星爷还是成功讲了一个励志故事。  最后挺喜欢一句台词：我当你是朋友，你居然想睡我！还有花絮中表演生气的女演员，真的看了好多遍。 </description>
    </item>
    
    <item>
      <title>TODO</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/todo/</guid>
      <description> paper 阅读   基础论文：    Visualizing and Understanding Convolutional Networks    Spatial Transformer Networks    目标检测：    SSD    RetinaNet    Deformable Convolutional Networks    cornernet    AutoML:    Exploring Randomly Wired Neural Networks for Image Recognition   </description>
    </item>
    
    <item>
      <title>ubuntu18.04 install gpu mxnet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/gpu-mxnet-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/gpu-mxnet-install/</guid>
      <description>安装 Ubuntu18.04     从 中科大源 下载系统镜像，并制作 u 盘启动盘   # 使用 wget 下载 Ubuntu18.04 $ wget http://mirrors.aliyun.com/ubuntu-releases/bionic/ubuntu-18.04.1-desktop-amd64.iso # fdisk 查看 u 盘分区 $ sudo fdisk -l # 使用 dd 命令制作 u 盘启动镜像 $ dd if=ubuntu18.04.1-desktop-amd64.iso of=/dev/sdb     启动电脑，修改为 U 盘启动    关闭 uefi 安全检查，有秘钥的通通删掉    使用 U 盘启动后，选择安装 Ubuntu    手动分区，建议只有两个分区，一个根分区，一个 home 分区；这样在需要重装系统的时候可以格式化根分区，同时保留 home 分区 的内容    安装完成后，修改 root 的密码，从而可以使用 root 登录系统   安装 NVIDIA 驱动   硬件配置    GPU GTX2070       由于 GPU 较新，需要添加 PPA；如非必要可以直接使用 Ubuntu repository    查看驱动列表    安装驱动    重启使驱动生效   # Ubuntu repository 的驱动虽然比较旧，但是更加稳定，bug 较少； # 如果想要安装最先的版本，可以添加由 Ubuntu 团队维护的 PPA # PPA 仍然在测试，可能存在某些依赖问题 # 不需要手动更新，Ubuntu18.</description>
    </item>
    
    <item>
      <title>Unix Network Programming</title>
      <link>https://kylestones.github.io/hugo-blog/blog/apue/unp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/apue/unp/</guid>
      <description>问题  程序异常终止   此时所有打开的文件描述符将被关闭，TCP 连接发送一个 FIN 。然后呢？对端的确认报文是否收到无所谓？ 对端发送 FIN 报文，此时已经终止？无法恢复确认报文？对端会重复发送？ IP 分片(fragment) – TCP 分段  MTU : 最大传输单元   链路层对网络数据帧的一个限制：以太网限制 MTU 为 1500 字节。 尽管 IP 报文头中有 16 位表示数据报的长度（最大长度 2^16-1=65535） IP 报文的长度超过 MTU 就需要分片，IP 数据报的分片与重组都是在网络层完成的。 IP 头有 3 个标志位，一个标志位保留；一个标志位 DF 表示是否允许分片，为 0 表示允许分片，为 1 表示禁止 分片，当报文长度大于 MTU 则丢弃该报文，并向源主机发送 ICMP 报文；一个标志位 MF 表示之后是否还有分片 片偏移用于确定该片偏移原始数据报开始处的位置。 16 位标识用于确定相应的片是否属于同一个 IP 报文。 每个以太帧长度在 64 bytes ~ 1518 bytes，减去以太网帧头（DMAC 48bits=6Bytes + SMAC 48bits=6Bytes + Type 2Bytes + CRC 4Bytes）最大只能有 1500 bytes。链路层数据部分长度的要求是 46bytes ~ 1500bytes MSS : 最大报文段长度   一般 MSS 为 MTU 减去 IP 首部长度 20 字节和 TCP 首部长度 20 字节，1500-40-40=1460 。但是许多 BSD 实现 版本需要 MSS 为 512 的倍数，所以一般 MSS 都是 1024。当建立一个 TCP 连接时，每一方都通告其期望的 MSS 选项（MSS 选项只能出现在 SYN 报文段中），最终选择两者中的较小者作为 MSS，是一个协商选项。 TCP 报文段的分段与重组是在运输层完成的。 不希望 IP 分片的原因   对于 IP 分片的数据报，即使丢失一片数据，也要重传整个数据报。因为 IP 层没有重传机制，没有办法只重传某 一片。所以即使 IP 分片过程看起来是透明的，但人们仍然不想用它。所以 TCP 总是避免分片，通过设置 MSS 使 得以太网帧长度不超过 MTU TCP、UDP 与 IP 分片   UDP 很容易导致分片； TCP 试图避免分片，应用程序几乎不可能强迫 TCP 发送一个需要分片的报文。 CSMA/CD   CSMA/CD是Carrier Sense Multiple Access with Collision Detection 的缩写， 可译为“载波侦听多路访问/冲突检测”，或“带有冲突检测的载波侦听多路访问”。 所谓载波侦听（carrier sense），意思是网络上各个工作站在发送数据前都要侦听总线上有没有数据传输。 若有数据传输 （称总线为忙），则不发送数据；若无数据传输（称总线为空），立即发送准备好的数据。 所谓多路访问（multiple access)意思是网络上所有工作站收发数据共同使用同一条总线，且发送数据是广播式的。 所谓冲突（collision），意思是，若网上有两个或两个以上工作站同时发送数据，在总线上就会产生信号的混合， 两个工作站都同时发送数据，在总线上就会产生信号的混合，两个工作站都辨别不出真正的数据是什么。 这种情况称数据冲突又称碰撞。为了减少冲突发生后的影响。工作站在发送数据过程中还要不停地检测自己发送的 数据，有没有在传输过程中与其它工作站的数据发生冲突，这就是冲突检测（collision detected）。    IPG(Inter-Packet Gap) Preamble Start Frame Delimiter DMAC SMAC Type Data Frame check Sequence   12Bytes 7Bytes 1Bytes 6Bytes 6Bytes 1Byte 48~1500Bytes 4Bytes    AA (01010101) AB (01010011)        帧间隙 前导码 前导码-帧界定符          CSMA/CD媒体访问控制方法的工作原理，可以概括如下： 先听后说，边听边说； 一旦冲突，立即停说； 等待时机，然后再说； 注：“听”，即监听、检测之意；“说”，即发送数据之意。 CSMA/CD网络上进行传输时，必须按下列五个步骤来进行 （1）传输前监听 （2）如果忙则等待 （3）如果空闲则传输并检测冲突 （4）如果冲突发生，重传前等待 （5）重传或夭折 主要参数： 时间片 51 2比特时间 帧间间隔 9．6 微秒 尝试极限 16 退避极限 10 人为干扰长 32 比特 最大帧长 1518 字节 最小帧长 64 字节 地址字段长 48 比特 二进制指数退避算法 1）确定基本退避时间（基数），一般定为2τ，也就是一个争用期时间，对于以太网就是51.</description>
    </item>
    
    <item>
      <title>VGG GoogLeNet ResNet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/vgg-googlenet-resnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/vgg-googlenet-resnet/</guid>
      <description>VGG   VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION    论文表明增加网络的深度可能提高网络的性能，论文成功将网络的深度推到 16-19 层    使用 small size (3x3) filter ：两层 3x3 的卷积与 5x5 的卷积等效，三层 3x3 的卷积与 7x7 的卷积等效；选用小尺寸的 filter 可以减小参数    第一层卷积的滤波器的个数是 64 ，之后每经过一个 max-pooling 层都将滤波器的个数乘 2，直到最大为 512 之后不再继续翻倍    Not all the conv layer are followed by max-pooling.    论文为了训练 19 层深层网络，先构建了一些浅层的网络，用于预训练，然后逐渐使用预训练好的浅层网络参数对深层的网络进行初始化。 不过文章中作者也指出，写完 paper 后发现，使用随机初始化是不需要预训练的。看来大神们发 paper 也是历经坎坷呀。 GoogLeNet   强调算法的重要性，比硬件、大的数据量更加重要。Most of the progress is not just the result of more powerful hardware, larger dataset and bigger models, but mainly a consequence of new ideas, algorithms and improved network architechtures.</description>
    </item>
    
    <item>
      <title>YOLO 实现细节</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/yolo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/yolo/</guid>
      <description>之前写的乱七八糟，现在开始重构，主要针对 YOLOv3。真的是得亲自动手实现，才能真正了解一个算法。The best way to go about learning object detection is to implement the algorithms by yourself, from scratch. – Ayoosh Kathuria 主网络   主网络采用 Darknet53 ，网络为全卷积网络 FCN(Fully Convolutional Networks)，之前一直以为全卷积网络就是不包含全连接的网络， 现在才知道，全卷积网络连 Pooling 层都没有，Pooling 层使用步长为 2 的卷积代替，防止由于 Pooling 导致 low-level features 的丢失；分类之前使用 Global Average Pooling 。[Darknet53 中有全连接层呀，上面写错了] 卷积层   Darknet 的每个卷积层都是由 Conv-BN-LReLU 组成，是网络的最小重复单元。 def cbl_gen(channels, kernel_size, strides, padding): &amp;#39;&amp;#39;&amp;#39;conv-BN-LeakyReLU cell&amp;#39;&amp;#39;&amp;#39; cbl_unit = nn.HybridSequential() # 所有卷积后面都有 BN ，所以 bias 始终为 False cbl_unit.</description>
    </item>
    
    <item>
      <title>乱想</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/guess/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/guess/</guid>
      <description> 目标检测     loss 改进。类似人脸识别，通过改进 loss 来提高 map ； FocalLoss 也算是一种改进    微软的 Deformable-Convolutional 应该大有用处，现在功力没有很好的发挥出来    神经网络对细节很敏感，通过在图片上粘贴一些胶带就可以成功的误导网络。是不是说明网络没有很好的区分背景？或者说背景没有 达到随机的效果？将目标完全分割出来（没有背景）用于训练会怎样？感觉人类在观察事物的时候，第一步就是先对目标进行了分割 呀？   网络参数剪枝     先训练一个大型的网络，然后裁剪成一个小一点的网络，其性能比直接训练的同等大小的网络效果要好很多。说明现在的网络训练方 法还是有很大的问题。 人类做梦是在进行无监督学习吗？ 神经网络的训练能否设计成监督训练和无监督训练相结合的方法？   AutoML   人工设计的特征远远没有网络自己提取的特征好，人工设计的网络结构也很有可能没有自己学习的结构效果好。每个人的大脑内的神经元 也应该是各不相同，我们只是被告知什么是猫、什么是鸟，而我们的神经元会自己学习怎样链接。 婚礼现场生成     一个人的一系列帧，可以插入视频某起始帧的某个地方    可以换脸，让自己喜欢的明星来参加自己的婚礼   边缘计算   软件硬化：高通 CMDA 、思科路由器、地平线    专业话，继续推动摩尔定律。由计算变化成 AI    低功耗    软件和硬件联合优化    特殊场景的特殊问题，一定要有场景。Google X 成立了 6-7 年，但只是一地鸡毛。  做东西一定要解决实际问题 。  对话系统：对话一定是要有目的，而不是仅仅为了对话。对话是要解决实际问题的。  亚马逊老总左贝斯：将理想和现实分开，  把握住十年中不变的东西。  最终由 big data 转变到 big computer 的 发展历程   余凯：历史都是先是 toC ，然后再是 toB 。  英伟达股票大幅增长，英伟达是 toC 。 创业   创业就是赌，如果是实实在在的放在桌子上东西，那根本不是创业，那是上班。  看长线，踏踏实实做下去，更可能钓大鱼。  看短期（最近 2-3 个月）和远期（目标），不用看半年或者 1-2 年，因为到时候，你的这段时间的思考都会废掉。 </description>
    </item>
    
    <item>
      <title>人脸识别</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/facerecognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/facerecognition/</guid>
      <description>FaceNet   A Unified Embedding for Face Recognition and Clustering  原来使用卷积神经网络来提取人脸的特征通常都是使用 softmax-loss 来训练网络，以期望网络的到的 embedding 足够好。本文作者直 接使用 embedding 的误差来训练网络，然后通过计算 embedding 的欧式距离来实现人脸验证。 triplet loss   每次使用三张图像，一个是 anchor ，另外两张中一张图像与 anchor 是同一个人，另一张是不同的人。  \begin{align*} L = ∑_i^N ≤ft [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + α \right ]_+ \end{align*} Triplet Selection   为了较好的训练效果，挑选hard-positive 和 hard-negative 的人脸对，就是同一个人时选择两张差别最大的图像，不同人脸的时候， 挑选差别最小的两张图像。其中 \(\alpha\) 是 margin 。当然所有这些选择都是在一个 mini-batch 中，而不是整个训练样本中。  另外为了防止网络进入局部最优解或者训练崩溃（如 f(x) = 0），选择 semi-hard negative 样本，即满足 \[ ||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 \]  论文中有 online 和 offline 两种方式来选择。选择方法较麻烦 center loss   A Discriminative Feature Learning Approach for Deep Face Recognition  作者认为 Softmax 仅仅增加了类间离散度，并没有很好的减小类内离散度，所以作者对 softmaax-loss 进行了改造，增加了每一个样本 到中心点距离的惩罚项，强制让学习到的同一个人的 embedding 都簇拥到一起。Penalizes the distances between the deep features and their corresponding class centers.</description>
    </item>
    
    <item>
      <title>他山之石</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/experience/</guid>
      <description>胜败   有时候，暂时的失利比暂时的胜利要好很多。 – 《毛.骗》  体育中总是胜者不变，败者变。失败者会努力思考现状，改变修正不足，查找出奇制胜奇招，最终打败对手。当你失败的时候，可能是为 了让你拥有更大的成功。人生不如意事常八九，可我们却没有很好的接受失败教育。 – 《白说》    First they ignore you. Then they laugh at you. Then they fight you. Then you win.   聪明   木秀于林，风必催之；堆出于岸，流比湍之；行高于人，人必非之。杨修和曹冲都是被自己的聪明害死的。  同时不叫的鹅被杀。应介乎才与不才之间  看破不说破；不痴不聋不做阿家翁；难得糊涂；等等都是古人的智慧，他们并非不知道，只是装糊涂，但绝不踩雷。这才是真聪明。  谁都不傻，但谁都有傻的时候。 比赛   比赛，骑自行车，总是许多人一起比赛时结果 优于 单独一个人去测试。  感觉这个可能和鲶鱼理论有某种程度上的关联。 量子纠缠   两个粒子相互纠缠，即使相距遥远，一个粒子的行为将会影响另一个粒子的状态。 策略     《三侠五义》义渠人偷东西。《毛骗》第二季第六集44:22。第二季第八集19：11-23：58,27：55-29：59    先让他失望，在给他希望，那样他就深信不疑。《毛骗》第一季，第二集,28:30    当一个卧底都不知道自己是卧底的时候，自然毫无破绽。   思考     ego cogito ergo sum – 我思故我在    A man is but the product of his thoughts what he thinks, he becomes.</description>
    </item>
    
    <item>
      <title>卷积神经网络进化</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/</guid>
      <description>总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \begin{align*} μ = \frac{1}{m} ∑_i Z[l](i) σ^2 = \frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \frac{Z[l](i) - μ}{\sqrt{σ^2+ε}} {\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \(d\gamma, \ d\beta\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \begin{align*} {μmean}[l] &amp;amp; = β {μmean}[l] + (1-β) {μ}^{\{i\}[l]} {σmean}2[l] &amp;amp; = β {σmean}2[l] + (1-β) {σ}^{2\{i\}[l]} \end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0.</description>
    </item>
    
    <item>
      <title>发现</title>
      <link>https://kylestones.github.io/hugo-blog/blog/idea/something-find/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/idea/something-find/</guid>
      <description>   https://36kr.com/p/5042735    Conway&amp;#39;s law   </description>
    </item>
    
    <item>
      <title>哈弗幸福公开课</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/happiness/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/happiness/</guid>
      <description>内容比较凌乱，待系统学习，梳理脉络，整理提纲 TODO  并不是所有的心理学观点或者研究都适合你，选择合适自己的去实践很重要。。。 为什么要研究积极心理学？     需要集中研究，这样会带来更大的收获    并不是治疗了抑郁等负面情绪，就会变得幸福快乐    积极心理学可以有效预防压抑、    Questions always create reality. 我关注什么就只会看到什么。游戏：限定时间内观察一幅画，问题画中有多少几何图形？时间到后， 大多数人只会数几何图像的个数。如果再问，图中钟表的时间？大巴车中有几个孩子？图像左侧主要的颜色？等这些问题时，几乎没有人 知道。  We see what we look for and we miss much of what we are not looking for even though it is there… Our experience of the world is heavily influenced by where we place our attention. — Stavros and Torres  resilience :: 即使面对糟糕的环境、恶劣的处境，仍旧保持乐观、对未来充满希望。不是那种盲目乐观，而是这次没有成功、总结经验， 相信下次定会成功。    认清长处    设立目标    榜样 role model    不单干，社会支持    欣赏优点、美好的东西也很重要。  appreciate：欣赏长处、成功。不应把其视为理所当然；成长，欣赏长处-采取行动-长处增长；  责任：为了让小组变得更加美好，我能做什么？而不是去抱怨那个人哪里做的不够好等等  No one is coming!</description>
    </item>
    
    <item>
      <title>基础心理学</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/basicpsychlolgy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/basicpsychlolgy/</guid>
      <description>心理暗示   一种主观意识上被肯定的假设，不一定有根据，但是由于主观上已经肯定了他的存在，所以在心理上便竭力倾向于这项内容。 心理战术   先上她失望，然后再给她希望，那样她就会深信不疑。 偷东西   绿林人抛砖引玉的方法，《三侠五义》 胜败   有时候暂时的失利比暂时的胜利要好很多。  谁都不傻，但谁都有傻的时候。  木秀于林，风比摧之；堆出于岸，流必湍之；行高于人，人必非之。 自信 self-confidence   The ability or the belief to believe in yourself, to accomplish any task, no matter the odds, no matter the difficulty, no matter the adversity. The belief that you can accomplish it. Thoughts influence actions. Self-confidence people interpret feedback the way they choose to. No one will believe in you unless you do.</description>
    </item>
    
    <item>
      <title>常见心理学效应</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/effect/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/effect/</guid>
      <description> 羊群效应   “羊群效应”也叫“从众效应”：是个人的观念或行为由于真实的或想象的群体的影响或压力，而向与多数人相一致的方向变化的现象。 无论意识到与否，群体观点的影响足以动摇任何抱怀疑态度的人。群体力量很明显使理性判断失去作用。  羊群是一种很散乱的组织，平时在一起也是盲目地左冲右撞，但一旦有一只头羊动起来，其他的羊也会不假思索地一哄而上，全然不顾前 面可能有狼或者不远处有更好的草。因此，“羊群效应”就是比喻人都有一种从众心理，从众心理很容易导致盲从，而盲从往往会陷入骗局 或遭到失败。  羊群效应的出现一般在一个竞争非常激烈的行业上，而且这个行业上有一个领先者（领头羊）占据了主要的注意力，那么整个羊群就会不 断模仿这个领头羊的一举一动，领头羊到哪里去“吃草”，其它的羊也去哪里“淘金”。  一则幽默 一位石油大亨到天堂去参加会议，一进会议室发现已经座无虚席，没有地方落座，于是他灵机一动，喊了一声：“地狱里发现石油了！”这 一喊不要紧，天堂里的石油大亨们纷纷向地狱跑去，很快，天堂里就只剩下那位后来的了。这时，这位大亨心想，大家都跑了过去，莫非 地狱里真的发现石油了？于是，他也急匆匆地向地狱跑去。 松毛虫实验 法国科学家让约翰·法伯曾经做过一个松毛虫实验。他把若干松毛虫放在一只花盆的边缘，使其首尾相接成一圈，在花盆的不远处，又撒 了一些松毛虫喜欢吃的松叶，松毛虫开始一个跟一个绕着花盆一圈又一圈地走。这一走就是七天七夜，饥饿劳累的松毛虫尽数死去。而可 悲的是，只要其中任何一只稍微改变路线就能吃到嘴边的松叶。 镜子效应   镜子效应来源与现实的生活，主要用于人际交往。当你照镜子时，镜子里的你会随着你的喜怒哀乐而变化。这是最狭义的镜子效应！同样 的，在人际交往中，你对别人好，别人也会对你好。相反你对别人不好，别人也对你不好！这就是镜子效应的真谛！ 在现实中的应用。 当你和你不喜欢的人相处时，或许他也不怎么喜欢你，但是只要你试着慢慢的喜欢他，逐渐的他也会开始喜欢和你相处！人都是相互的！ 多看效应   对越熟悉的东西越喜欢的现象，心理学上称为“多看效应”。多看效应不仅仅是在心理学实验中才出现，在生活中，我们也常常能发现这种 现象。 有些人善于制造双方接触的机会，从而提高彼此间的熟悉度，互相产生更强的吸引力。在我们新认识的人中，有时会有相貌不佳的人，最 初，我们可能会觉得这个人难看，可是在多次见到此人之后，逐渐就不觉得他难看了，有时甚至会觉得他在某些方面很有魅力。 实验 20世纪60年代，心理学家查荣茨做过这样一个实验：他向参加实验的人出示一些人的照片，让他们观看。有些照片出现了二十几次，有的 出现十几次，而有的则只出现了一两次。之后，请看照片的人评价他们对照片的喜爱程度。结果发现，参加实验的人看到某张照片的次数 越多，就越喜欢这张照片。他们更喜欢那些看过二十几次的熟悉照片，而不是只看过几次的新鲜照片。也就是说，看的次数增加了喜欢的 程度。 另一个实验：在一所大学的女生宿舍楼里，心理学家随机找了几个寝室，发给她们不同口味的饮料，然后要求这几个寝室的女生，可以以 品尝饮料为理由，在这些寝室间互相走动，但见面时不得交谈。一段时间后，心理学家评估她们之间的熟悉和喜欢的程度，结果发现：见 面的次数越多，相互喜欢的程度越大；而见面的次数越少或根本没有，相互喜欢的程度就较低。 作用 可见，若想增强人际吸引，就要留心提高自己在别人面前的熟悉度，这样可以增加别人喜欢你的程度。因此，一个自我封闭的人，或是一 个面对他人就逃避和退缩的人，由于不易让人亲近而令人费解，也就是不太让人喜欢了。想想看，你周围有没有常在你面前“露脸”的人。 如果想给别人留下不错的印象，常出现在他面前就是一个简单有效的好方法。 所以，作为大学生，如果你想改善自己的人缘，不妨多在寝室间走动一下，即使只是露个脸，借瓶开水，换本书。在这些细节的来来往往 中，就无形中提高了自己的人际吸引力，也获得了你所期待的群众基础。作为职场人士，埋头苦干也并非明智之举。自我封闭，不与人交 往，遇事逃避退缩，只会让你的职业发展之路止步不前。 当然，“多看效应”并非万能钥匙。它发挥作用的前提是，首因效应要好，即给人的第一印象不是很差，如果第一印象就很差，那么见面越 多就越惹人讨厌，“多看”反而让人“多厌”，那就得不偿失了。所以，一切的前提，还是认清自己，理顺了，再拿给别人看。 标签效应   当一个人被一种词语名称贴上标签时，他就会作出自我印象管理，使自己的行为与所贴的标签内容相一致。这种现象是由于贴上标签后面 引起的，故称为“标签效应”。 心理学认为，之所以会出现“标签效应”，主要是因为“标签”具有定性导向的作用，无论是“好”是“坏”，它对一个人的“个性意识的自我认 同”都有强烈的影响作用。给一个人“贴标签”的结果，往往是使其向“标签”所喻示的方向发展。 鸟笼效应   鸟笼效应：假如一个人买了一只空鸟笼放在家里，那么一段时间后，他一般会为了用这只笼子再买一只鸟回来养而不会把笼子丢掉，也就 是这个人反而被笼子给异化掉了，成为笼子的俘虏。 鸟笼效应是一个著名的心理现象，又称“鸟笼逻辑”，是人类难以摆脱的十大心理之一 ，其发现者是近代杰出的心理学家詹姆斯。“鸟 笼效应”是一个很有意思的规律，人们会在偶然获得一件原本不需要的物品的基础上，自觉不自觉的继续添加更多自己不需要的东西。 禁果效应   禁果效应也叫做“亚当与夏娃效应”，越是禁止的东西，人们越要得到手。越希望掩盖某个信息不让别人知道，却越勾起别人的好奇心和探 求欲，反而促使别人试图利用一切渠道来获取被掩盖的信息 。这种由于单方面的禁止和掩饰而造成的逆反现象，即心理学上的“禁果 效应” 。这与人们的好奇心与逆反心理有关。有一句谚语：“禁果格外甜”，就是这个道理。 刺猬效应   刺猬效应（Hedgehog Effect），是指刺猬在天冷时彼此靠拢取暖，但保持一定距离，以免互相刺伤的现象。这个比喻来自叔本华的哲学 著作，它强调的是人际交往中的“心理距离效应”。刺猬效应的理论可应用于多种领域。在管理实践中，就是领导者如要搞好工作，应该与 下属保持“亲密有间”的关系，即为一种不远不近的恰当合作关系。在教育学中，教育者与受教育者日常相处只有保持适当的距离，才能取 得良好的教育效果。 淬火效应   淬火效应，原意指金属工件加热到一定温度后，浸入冷却剂（油、水等）中，经过冷却处理，工件的性能更好、更稳定。心理学把这定义 为“淬火效应”。教育上也会有类似的现象，被称之为“冷处理”。 在心理学与教育学中衍生出的含义为长期受表扬头脑有些发热的学生，不妨设置一点小小的障碍，施以“挫折教育”，几经锻炼，其心理会 更趋成熟，心理承受能力会更强；对于麻烦事或者已经激化的矛盾，不妨采用“冷处理”，放一段时间，思考会更周全，办法会更稳妥。 负重效应   负重效应起源于一个航海故事。 有一位经验丰富的老船长，当他的货轮卸货后在浩淼的大海上返航时，突然遭遇到了狂风巨浪。水手们惊慌失措，急得团团转。老船长果 断地命令水手们立刻打开所有的货舱，往里面灌水。“怎么能够往船舱里灌水呢？这样只会增加船的重量，使船迅速往下沉，这是自寻死 路啊！”不少年轻的水手不解地问道。但看着船长严厉的脸色和毫无商量的神情，水手们只好半信半疑地照做了。随着货舱里的水位越升 越高，随着船一寸一寸地下沉，依旧猛烈的狂风巨浪对船的威胁却一点一点地减少，货轮渐渐平稳了。船长望着松了一口气的水手们说： “一只空木桶，是很容易被风打翻的，如果装满水负重了，风是吹不倒的。同样道理，船在负重时是最安全的，空船时则是最危险的。”于 是，人们将这种现象称之为“负重效应”。 科学家计算过，倘若地球上的鱼一年之内产的鱼仔都能成活，都能顺利长成成鱼，可以把地球上江湖河海都塞满，然而事实却不是这样， 从鱼仔到成鱼，要经历千百次的自然和自身的淘汰。最优秀的和最适应的才能留了下来。自然界要维持其自身的平衡功能，都是在各种压 力下完成的，否则生物就会逐步退化。孩子的成长也应该如此，当有的爱对孩子来讲是不愿承受的负担时，就可以要求在孩子不同的年龄 里承担不同的责任，适当、适时的压力也是孩子成长的强大动力。 出丑效应   出丑效应又叫仰巴脚效应，犯错误效应(PRATFALLEFFECT)：是指才能平庸者固然不会受人倾慕，而全然无缺点的人，也未必讨人喜欢。最 讨人喜欢的人物是精明而带有小缺点的人，此种现象亦称为仰巴脚效应。 出丑效应：意指精明的人无意中犯点小错误，不仅是瑕不掩瑜，反而更使人觉得他具有和别人一样会犯错的缺点，反而成为其优点，让人 更加喜爱他。仰巴脚就是不小心跌了一跤，有时可能要跌个脊背着地，四脚朝天，所以，又叫“出丑效应”。生活中有不少比较完美精明的 人。其实，这种完美往往是外在的表演，这样就未必讨人喜欢了。因为一般人与完美无缺的人交往时，总难免因己不如人而感到惴惴不安。 最讨人喜欢的是那些精明而小有缺点的人，比如，学生眼中的老师，老师眼中的领导，老百姓眼中的大官等。这些貌似完美无缺的人在不 经意中犯个小错误，不仅是瑕不掩瑜，反而让人觉得他和大家一样有缺点，就因为他显露出平凡的一面而使周围的人都感到了安全。 注：pratfall一字是英文中的俚语，意同北京土话“仰巴脚儿”。指不小心摔个四脚朝天的姿势，准此而论，此效应亦可称为“仰巴脚效应”。  就是对于那些取得过突出成就的人来说，一些微小的失误比如打翻咖啡杯这样的细节，不仅不会影响人们对他的好感，相反，还会让人们 从心理感觉到他很真诚，值得信任。而如果一个人表现得完美无缺，我们从外面看不到他的任何缺点，反而会让人觉得不够真实，恰恰会 降低他在别人心目中的信任度，因为一个人不可能是没有任何缺点的，尽管别人不知道，他心里对自己的缺点也可能是心知肚明的。 踢猫效应   踢猫效应是指对弱于自己或者等级低于自己的对象发泄不满情绪，而产生的连锁反应。 “踢猫效应”，描绘的是一种典型的坏情绪的传染。人的不满情绪和糟糕心情，一般会沿着等级和强弱组成的社会关系链条依次传递。由金 字塔尖一直扩散到最底层，无处发泄的最弱小的那一个元素，则成为最终的受害者。其实，这是一种心理疾病的传染。 一般而言，人的情绪会受到环境以及一些偶然因素的影响，当一个人的情绪变坏时，潜意识会驱使他选择下属或无法还击的弱者发泄。受 到上司或者强者情绪攻击的人又会去寻找自己的出气筒。这样就会形成一条清晰的愤怒传递链条，最终的承受者，即“猫”，是最弱小的群 体，也是受气最多的群体，因为也许会有多个渠道的怒气传递到他这里来。 现代社会中，工作与生活的压力越来越大，竞争越来越激烈。这种紧张很容易导致人们情绪的不稳定，一点不如意就会使自己烦恼、愤怒 起来，如果不能及时调整这种消极因素带给自己的负面影响，就会身不由己地加入到“踢猫”的队伍当中——被别人“踢”和去“踢”别人。 鲶鱼效应   鲶鱼效应：鲶鱼在搅动小鱼生存环境的同时，也激活了小鱼的求生能力。鲶鱼效应是采取一种手段或措施，刺激一些企业活跃起来 投入到市场中积极参与竞争，从而激活市场中的同行业企业。其实质是一种负激励，是激活员工队伍之奥秘。 挪威人喜欢吃沙丁鱼，尤其是活鱼。市场上活鱼的价格要比死鱼高许多。所以渔民总是想方设法的让沙丁鱼活着回到渔港。可是虽然经过 种种努力，绝大部分沙丁鱼还是在中途因窒息而死亡。但却有一条渔船总能让大部分沙丁鱼活着回到渔港。船长严格保守着秘密。直到船 长去世，谜底才揭开。原来是船长在装满沙丁鱼的鱼槽里放进了一条以鱼为主要食物的鲶鱼。鲶鱼进入鱼槽后，由于环境陌生，便四处游 动。沙丁鱼见了鲶鱼十分紧张，左冲右突，四处躲避，加速游动。这样沙丁鱼缺氧的问题就迎刃而解了，沙丁鱼也就不会死了。这样一来， 一条条沙丁鱼活蹦乱跳地回到了渔港。这就是著名的“鲶鱼效应”。 （有学者文章指出，“鲶鱼效应”的主体内容存在致命瑕疵：鲶鱼属于生活在江河湖等淡水水体中的鱼类，而沙丁鱼则属于海洋咸水鱼类， 运输沙丁鱼需要使用海水，将鲶鱼放入海水将导致鲶鱼死亡，根本不可能出现两种物种共生并相互影响的情况。目前关于鲶鱼效应的描述 都是中文，并没有确切的外文文献引用可以说明其出处是国外，且没有数据表明其在实践中的有效性，请读者斟酌采纳。） 启示 鲶鱼效应对于“渔夫”来说，在于激励手段的应用。渔夫采用鲶鱼来作为激励手段，促使沙丁鱼不断游动，以保证沙丁鱼活着，以此来获得 最大利益。在企业管理中，管理者要实现管理的目标，同样需要引入鲶鱼型人才，以此来改变企业相对一潭死水的状况。 费斯汀格法则   生活中的 10% 是由发生在你身上的事情组成，而另外的 90% 则是由你对所发生的事情如何反应所决定。 </description>
    </item>
    
    <item>
      <title>广告</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/advertisement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/advertisement/</guid>
      <description> LiveRe 广告   许久许久没有整博客，都不知道这两年是怎么虚度的。打开自己再 github 上的博客，突然发现有广告，在页面底部，来必力评论的下面。虽然来比力是可以免费使用的，对应公司需要钱来支持运营。但是看到广告还是感觉非常的不爽， 那就删掉评论的配置了，况且也不会有人来我这垃圾博客来评论。。。 </description>
    </item>
    
    <item>
      <title>情商</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/emotion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/emotion/</guid>
      <description> 真正的情商高手，少不了对情绪的洞察   心里学家Kang和Shaver的研究发现，一个情绪管理能力越强，TA的情绪复杂度也就越高，情绪回路也越多。而情绪复杂度高的关键就在于， 他们习惯分析他人的情绪，这使得他们有了更多的机会，从中学习情绪的多种回路表达。 很多研究表明，人识别面部情绪表达是由特定的神经通路实现的，当这些结构损伤时，人识别情绪的能力就会发生障碍，这样的人往往在 人际交往中四处碰壁。因为如果一个人失去了，从别人身上学习情绪的机会，那他就只能依赖自己的情绪经验，然而每个人的体验都是有 限的。甚至大多时候，由于当局者迷，我们自身也缺少识别情绪的能力。这会让我们在情商提高这件事上，陷入瓶颈。通俗地说，读千卷 书，行万里路，都不如阅人无数。 情绪复杂度高的人，对他人的情绪产生的多样化原因，能有更加深度的认识，这使得他们能有更好的方法，更多的语言来解决别人的情感 冲突，引得他人的理解。情绪的识别面小了，我们太容易钻牛角尖，容易把人活活逼死。所以太史公司马迁说过：世间本无事，一切在人 心。 例子  朋友  案例   当我因为命运的一些戏剧变化，突然间生活状态远高于过去时，即便我最好的朋友，也难以对我发自内心的祝贺。哪怕我有下乡支教这种 比较硬派的高尚美德，嫉妒情绪也会妨碍她们由衷替我感到高兴。老朋友们之所以不高兴，是因为她们突然发现，似乎我的地位可能高过 了她们时，她们认为这是一种对她们尊严的冒犯。 回答   当别人嘲讽时，我回答：“别说，我恐怕混不到约炮的那天，就已经被人骂死了吧。我这种满身晦气的网红，哪有女生看得上。要想真正 赚到钱，怕是要我们一起才行哦！” 回答要点，在急着炫耀自己成就前，先用一些事实让对方觉得‘我并没有比你强’，从而消除焦虑。之后再提醒对方，对于我的改变，他们 可以有其他的角度去思考。所以，没必要急着批评朋友们出现的嫉妒情绪，这不过是一种‘我是否安全’的危险觉知。你需要充分察觉到对 方的情绪，引导他们走出负面的揣测。 亚当斯密把这种处境比喻为‘好运者的悲剧’，他认为这样的人，会走上放弃人际间感情链接，拼命努力的道路，然后有一天，会因为事业 的受挫，身边缺乏社会支持，精神备受打击，从而一蹶不振。 总结   我们之所以在这样的情景里，所有的解释都感到苍白无力，就是因为对方的情绪系统其实被堵住了，你需要先引导他把情绪管道里的某些 东西释放出来，他才可能接收到你的某些积极能量。你应该顺从他们的意思帮助他们缓解焦虑，因为焦虑往往会让人，只注意到眼前的威 胁，即我似乎比他们地位高，他们很难从更大的角度去思考一些问题，比如是否可以学习一点我的经验，从其他途径为自己积累资本。 所谓孤独不是身边没人，就是你面对的那个人，他的情绪和你的情绪不再同一个频率。当解决了焦虑这个情绪时，对方的情绪同类才会畅 通，阳光才能照进别人的心里。 女老师  案例   有位大学女老师吐槽：“我最近压力太大了，学校为了评文明单位，我每天晚上都工作到十一点，我有个朋友因为忙，孩子都流产了，真 是不想干下去了。但评上后，今年会多发一个月工资。” 这个时候要学会识别她的情绪，她此刻的情绪，比起‘焦虑’更像‘愤怒’，一种对领导安排工作不合理的愤怒。但内心确实又没法完全否定 这件事的意义，这使她矛盾。接着，你要学会和她的情绪对话，疏导它的情绪。 回答   “可能你们领导觉得，一个单位材料越多就越文明，就算他们对待员工的方式，不是很文明” 总结   这里，疏导的关键就在于，你要用一种高度总结，逻辑严密的句子，把对方想说又不知道说的东西表达出来。他的内心仿佛有一种堵着的 东西，被你一说，瞬间就通畅了。网络上很多意见领袖，就是靠这样的方式吸粉，可以说，这是一种很高级的提供情感价值的方式，叫做 帮你说出，你想说的话。 小乌龟  案例   有姑娘和我倾诉：“我曾经养过一只小乌龟，后来我去旅游，就把它托付给我爹照顾，谁知道回来的时候，我的小乌龟就被我爹养死了！” 这时候，安慰她或者说一个自己的悲伤故事都不好。其实，这位姑娘的情绪要更加综合而复杂，她的真是情绪是，即为小乌龟的逝世感到 难过，又不想指责她的父亲。 回答   “爸爸接下这个任务，说明他有男人的担当，但比起你来，缺乏一点女性的细致。看你这么伤心，天国的小乌龟不会责怪你的。” 这段话不仅给了这位姑娘情绪价值，也为我们内心构建了更为复杂的情绪回路。 总结   人的记忆，从来都不是一栋完整的房子，到处是回忆的瓦 。情绪就像雨滴，当它淋在记忆的废墟上时，你会看到一个海市蜃楼的家，在 那个家里，有画满涂鸦的床头柜，有你曾经遗弃的小熊。进家拿起小熊，那种幻觉一般的触感，会让你找回过去的幸福，这份感觉会让你 不再沉默寡言，更不会歇斯底里。 情绪识别   你的情绪别人一定会察觉到，且受到你的情绪的影响。 放松的状态，无欲无求的状态是最自在的状态，能够最大的发挥自己的优点。 体会自己的感受，思想，从而去体会别人的思想。 一定要从别人身上体会其情感，观察其面部变化，才能提升自己的情感回路。  女人喜欢在她们面前很自在的男人，而且有魅力的男人都是这种在女人面前很自在的男人。 我内心的平静让女人在我身边时，感觉到安全。 情绪没有味道，但确实可以闻的到，你闻到某个人情绪的时候，你的内心也会自然升起那个情绪。 你所匮乏的会变成你所恐惧的，你一旦开始恐惧自己所匮乏的事物不能满足自己，你就会开始充满欲望。 每一个欲望背后都藏着一个深深的恐惧。 当注意力高度集中的时候，情绪，焦虑，和欲望都会消退，有人说男人在高潮的时候最接近神，其实是因为性爱的过程会高度集中人的注 意力。 这个世界上有很多事情需要等，但爱情这件事无需再等。  聊天时：不要说教，不要比较，不要评论，尽量少提问，尽量让语境变得轻松愉快一点。女生倾向于分享感受，很多话他们说出来没有目 的，就是想说而已，想分享一下此时此刻心里想到的而已。所以能和女生共鸣的对谈，必要的成分是分享感受。 感恩、知足   一定要懂得感恩和知足，关注那些美好的东西，让自己开心快乐起来。 自信   自信非常非常重要，beyand的歌曲中总会提到：打不死的自信心态活到老。 面试或者其他的重要场合，穿一身新衣服，你会不由自主的抬头挺胸，会更加有自信。 化一切尴尬于无形   黄勃：移花接木，反客为主；假痴不癫，以城动人。 看破不说破   看破且说破，情商大忌。看破不说破，从来都不会让别人难堪。还能不着痕迹地化解尴尬，让人如木春风。 在自己能力范围内尽量做到极致和卓越。 </description>
    </item>
    
    <item>
      <title>情话</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/love/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/love/</guid>
      <description>摘抄的关于爱情的美句</description>
    </item>
    
    <item>
      <title>损失函数</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/loss-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/loss-function/</guid>
      <description>为了度量算法关于某个数据集的性能，我们需要损失函数。当算法希望生成比真实值一个较小的数字，那么损失函数中应该体现出现，较 大的输出比较小的输出有更大的惩罚。    Loss: Used to evaluate and diagnose model optimization only.    Metric: Used to evaluate and choose models in the context of the project. Mean Squared Error     MSE 是经常被使用的损失函数，易于理解，且表现很好。    take the difference between your predictions and the ground truth    square it    average it out across the whole dataset   def MSE(y_predicted, y): squared_error = (y_predicted, y) ** 2 sum_squared_error = np.</description>
    </item>
    
    <item>
      <title>文学待修养</title>
      <link>https://kylestones.github.io/hugo-blog/blog/book/literature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/book/literature/</guid>
      <description>文化水不平  大家     俄国大师: 普希金、屠格涅夫、果戈里《死魂灵》、契诃夫（三大伟大短片小说家之一）、莱蒙托夫、托尔斯泰、陀思妥耶夫斯基    米兰.昆德拉 《不能承受的生命之轻》 捷克作家    肖斯塔科维奇 《肖七》 （请在我们脏的时候爱我们）    马克.罗斯柯 《红色中的赭色和红色》 《绿色和粟色》、马克.夏加尔 《我与村庄》《生日》《七个手指头的自画像》、亨利.马斯 蒂、    《莫斯科不相信眼泪》、《两个人的车站》    美国文学奠基者： 马克吐温、杰克.伦敦 《野性的呼唤》 《热爱生命》 《黄祸》 《史无前例的入侵》 、海明威    中国伤痕作家： 苏童、余华、莫言、王朔、张贤亮    崖山之后再无中国 ； 昭武九姓    80 年代，大师的年代    亚里士多德、孔子、释迦牟尼几乎处于同一个年代    李约瑟难题    西汉： 绿林起义，绿林军    贝尼托.</description>
    </item>
    
    <item>
      <title>深度学习新闻跟踪</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/ai-news/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/ai-news/</guid>
      <description> 摄像头是天生的神经网络   还记得之前说冯诺依曼结构的计算机，将计算和存储分离，需要移动大量的数据，严重限制了计算的性能，忆阻器可以实现类神经元，同时完成计算和存储。今天看到一个有意思的研究，说摄像头直接完成神经网络的计算，摄像头 CMOS 有望成为新一代的 GPU 。维也纳大写的研究发表在 20200304 期 nature 上，运行速度非常快，运行速度受限于电路中电子的速度，真是不能再快了。  https://www.nature.com/articles/s41586-020-2038-x TOADD  </description>
    </item>
    
    <item>
      <title>点滴生活</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/life-diary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/life-diary/</guid>
      <description> 针眼   在 &amp;lt;2020-02-28 Fri&amp;gt; 感觉左眼疼，发现上眼皮内有个小疙瘩，非常不舒服。于是用芦荟胶涂抹，慢慢发现好像小了点，感觉已经受控。但是眼皮内总是有一些粘稠的东西，以为是芦荟胶，没有在意。无奈发现下眼皮又长了个疙瘩，而且比上眼皮的还严重。疼痛无法忍受，于是网上查了一下，感觉是针眼，也叫麦粒肿。于是 35 元买了瓶左氧氟沙星，说明书上写着一天 3 滴，可适量增减，我感觉比较严重，一天似乎滴了十几次。后来发现小疙瘩上面总是会流出一些浑浊的液体，有点担心会通过眼睛传入大脑，受到感染。另外发现左脸肿了好大一块。于是请假远程办公，连续滴眼液两天，在家还用热毛巾热敷了两次，发现肿胀小了很多。前后一个星期多呢。  一个星期内基本啥也没做，csapp 也没有坚持看。。。 </description>
    </item>
    
    <item>
      <title>目标检测</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/objectdetection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/objectdetection/</guid>
      <description>YOLO   非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子    Darknet is public domain.    Do whatever you want with it.    Stop emailing me about it!    YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。  区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。  而且此时是 R-CNN two-stage 方法盛行的时候，先找到一些候选区域（代替窗口扫描），然后只在这些候选区域上进行检测。多么好的 主意。一般人肯定都会在思考怎样更加快速有效的提取这些候选区域（Faster R-CNN），或者加快提取特征的速度（Fast R-CNN）。作者 居然可以完全另辟蹊径，直接进行检测输出 one-stage 完成，根本无需候选区域，的确大牛。由此也可以看到有些时候一些非常好的思 想同样会束缚住我们。  先放三张 YOLOv3 的检测结果：    算法   首先将图像分成 s*s grid cells ，在每个 grid cell 上都输出 B 个 bouding box ，每个 bouding box 都包含 5 个值 (tx,ty,tw,th,to) ，其中 (tx,ty) 表示目标的中心点，(tw,th) 表示宽度和高度，(to) 表示 bouding box 中有 object 的概率。另外 每个 grid cell 会输出 C 个不同类别的使用 Softmax 求解到的概率。这样每个 grid cell 输出向量的维数是 (B*5 + C) ，网络总的 输出向量维数是 (s*s * (B*5 +c)) 。目标的中心点落在哪个 grid cell 中，则由这个 grid cell 进行检测。  其中 (tx,ty) 是相对于 grid cell 左上角 (cx,cy) 的偏移 offset ，通过 sigmoid 函数使得 offset 在 0~1 之间（认为每个 grid cell 的宽度和高度都是 1 ）。而 (tw,th) 利用与 Anchor Boxes 的宽和高的相对关系来求取。当然 YOLOv1 中并没有使用 Anchor box， 这是在 YOLOv2 中的改进。相比于直接使用坐标，使用 offset 或者相对关系可以是模型更加稳定。因为通过随机初始化，网络需要很长 的训练时间才能找到目标的位置，而找到相对偏移量则会容易很多。  由于 sum-squared error 比较容易优化，作者使用均方误差计算检测误差。即计算 grid cell 的四个坐标值与 ground truth 值差值的 平方和。  另外由于大多数的 grid cell 中没有 object ，只有少数的 grid cell 中会有目标，所以作者为这两种不同的误差赋予了不同的权重， 以阻止预测值趋向于 0 。具体 \(\lambda_{coord}=5, \ \lambda_{noobj}=0.</description>
    </item>
    
    <item>
      <title>秘密--吸引力法则</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/secret/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/secret/</guid>
      <description>  内在的声音和影像比外在的观点更深刻更清晰和明确时，这个时候你就掌握了自己的命运。  吸引力法则： 目前的生活状况是过去想法和行为造成的结果，如果你用目前的生活状况来定义自己，你很容易相信，未来的生活和现在没有任何区别。 你今天的想法和感觉，正在创造你的未来 你把注意力放在什么想法和感受上，你就把他们吸引到你的生活中来，不管是不是你期待的。 你跟家人一起担心这些事跟朋友一起讨论那些不好的事，你的态度会令你进一步处于却这少那的状态。 你无论认为自己行还是不行都是对的。 你的人生是你自己创造出来的。 如果你想改变自己的状况，那你需要先改变自己的想法。 不要把过多的经历放在不想要的事情上，把焦点放在自己想要的事物上。学会静下心来，把注意力从自己不要的事物上转移开，让所有情 绪沉淀下来，把注意力放在你想要的体验的事情上。精力会随着注意力流转。 如果你心情好，那么你正在创造理想的未来。 只要简单转换一下心情，这一天甚至这辈子都会变得不一样；只要你不让任何事情影响你的好心情。 感恩 如果你对已经拥有的事物感到满足和感恩，你也许会很快得到其他你更想要的东西。 你必须要有渴望，要专心的想拥有她们。你要有企图心，一旦有了企图心，热切盼望你想要的东西，宇宙就会把你一直等着，想拥有的一 切，送到你面前。好好注意身旁那些美好的事物，祝福她们，赞赏她们；在另一方面，对于那些你目前不如意的事，不要浪费精神去挑剔 或者抱怨，去拥抱一切你想要的事物，你就会得到更多。 你的愿望、想法和你内心的感觉都非常重要，因为这些都会在你的生命里实现。 如果你想要富足、成功，那么你就要把注意力放在富足和成功上。 人能得到心里想要得到的一切。 每个人都有能力改变自己的人际关系和自己的财务状况等一切东西。 你能拥有你想要拥有的一切，你能成为你想成为的人，能做到任何想做的事。 你是你自己命运的主宰 所有的力量都来自内心，也就是说被我们自己掌控着。 你内心的力量比整个世界的力量还大。 经常有人这么跟我说：我希望明年薪水可以增加，可是接下来我从他们的所有的行为里看不出来他们打算要让他们的目标实现。你知道他 们会转头说：我付不起。 愿景版：把自己想要的东西都贴上去。 你可以设计自己的命运，你是个作者，你在写自己的故事，而笔就我在你的手里。最后的结局其实是你自己选的。 灵感出现后，你要相信她，还要采取行动。 宇宙喜欢快速行动，不要拖延、不要猜测、不要怀疑，当机会出现的时候，当冲动来临的时候，当内在的直觉推动你的时候，你必须马上 行动，这就是你的任务，也是你唯一要做的事情。 世界永远处于供过于求，美好的事物与创意是取之不尽的，能量、爱和快乐也是取之不尽的；只要你在心里感觉到自己有着无限的力量， 那么这些就会出现在你的现实生活里。永远不用担心供不应求。  观想：视动行为复演法：你所观想的一切都会变成现实。 奥林匹克运动员，你的心智辨别不出你是在跑步还是在做心里练习。 你在生活中得到是你所感受的集合，而不是你思考的东西。 吸引力不仅靠我们心里图像或想法来产生，真正创造她的是我们的感觉。 没有产生那种热爱或快乐的感觉，那么你就无法从心里产生吸引的力量。 感觉那种喜爱和快乐。 制造一种自己已经坐那辆车里，而不是你将来一定要拥有那辆车。 让你的渴望显现的捷径是—把你想要的，看成是即成的事实。 观想的力量很大很大 想象力就是一切，她是未来生命的预览。 决定你要什么，相信自己可以得到，也能够得到她，并且相信你有可能实现她，接下来每天闭上眼睛几分钟，想象自己已经得到了她，感 觉一下得到她是什么滋味，然后脱离出来，想象自己已经拥有了她并心怀感激，你要真的心满意足。  爱自己： 在两性关系里，先了解在这关系里面的人是非常重要的事。指的是你自己。 如果你独处的时候都不快乐，那别人跟你在一起的时候，又怎么会快乐呢？ 你对待你自己的方式和期待别人对你的方式一样吗？ 直到我真正的爱上我自己，周围的人才开始爱上我。 你拥有很棒的特质，我研究自己27年了，我真的想亲我自己！ 你一定要学者爱自己。 你天生完美 不是要自以为是，至少要尊重自己。 你只有爱自己，才会懂得怎样去爱别人。 你该对自己好一点，多留点时间给自己，给自己充电。然后你就可以给予。 在人际关系方面，大家都习惯抱怨别人不好。如果你真想建立良好的关系，你就因该多多关注和欣赏别人的优点，而不是注意他们的缺点。 因为我们越是抱怨，那些事情就会不断的发生。 如果和某人关系很恶劣，也是可以改善的。在未来30天里，拿出一张纸来，每天坐下来，把这个人的优点一一写下来。想想你为什么会爱 那个人，因为他很幽默还是因为他给你不断的支持，最后你会发现，只要你专心去想，感激和确定这个人的优点，他就会更为你展现这一 面，你们之间的问题就会消失。 我们无法控制别人，无论我们再怎么努力都没有用。你常常会把那些让自己快乐的机会拱手让给了别人，他们又常常没有办法如你所愿。 为什么呢？因为事实上，唯一能够控制你的快乐还有幸福的人是你自己。就算是你的父母、你的孩子、你的配偶，那些你最亲近的人，他 们也没有办法控制你的幸福。快乐在你自己的心中。 内在的快乐是成功必须的燃料。 初期目标是感觉和体验快乐，接着开始去做所有能给我带来快乐的事。有一句格言叫做“不好玩的事不做”。如果你坐着冥想一个小时能让 你快乐，那就去冥想，如果你吃香肠三明治的时候感到快乐，那就去买，当我摸的猫咪就会很开心，走进大自然就会很舒服，所以我想让 自己经常的处于这种状态。我所要做的就是专心把注意力放在我想要的，我希望在生活中显现的事。 生命太不平凡了，他是一段精彩的旅程。 你只要尽力做好你自己。 你经历的每一件事，你所经历的每一刻，其实都是为了现在这一刻所做的准备。 你得先追求内心的快乐与平和，内心现有这样的景象，然后外在事物才会一一呈现。  健康的秘密： 思想和情绪决定了我们的身体物质结构与功能。 我们都知道安慰剂的效用有多大。 原来人的心态才是治疗最重要的元素，疗效甚至超过药物。 这个世界上有着数不清的疾病，他们全都是由一个原因导致的，那就是压力。你在一点上施加压力，就是对整个系统施压，某个环节就会 断开。 一个心情健康愉快的身体里是不会出现什么疾病的。 快乐的想法会产生快乐的生物化学反映，产生更快乐、更健康的身体。负面的想法、压力会让身体更加恶化，也会降低脑部的功能。因为 我们的想法跟情绪会持续不断地重新调整、重新排列、重新创造我们的身体。把身体的压力给除掉，让身体发挥自己该有的功能，这样就 能自我疗逾。 想象这自己生活在一个非常健康的身体里，让自身的免疫系统去处理一切。 几年内，我们就会有一个全新的身体。 我们的生理系统制造出疾病来给我们反馈，他让我们知道，我们有不平衡的想法，我们缺乏爱和感恩。  生命应该在各个方面都很富足。 这个宇宙是富饶、健康和富足的最佳杰作。你只要敞开心胸去感受这个宇宙的富足，你就能感受到美好、喜悦和幸福。你可以体验到宇宙 提供的所有美好的事物。比如说健康、富有、良好本质。假如你关上心门，满脑子负面想法，你就会感觉不舒服，一身病痛，会觉得很伤 心，你会觉得好像每天都过的很难过。  你是需要吸收咨询，但不需要到泛滥的程度。 </description>
    </item>
    
    <item>
      <title>笑话集锦</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/joke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/joke/</guid>
      <description>   孩子学习不好，被妈妈痛骂，挨骂后，儿子用哀怨的眼神看着爸爸说：你为什么要娶她？爸爸也用哀怨的眼神说：还不是因为你！    葵花点赞手!    脑子是个好东西，希望你能有。     </description>
    </item>
    
    <item>
      <title>美句</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/goodsentence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/goodsentence/</guid>
      <description>杂记   人们宁愿去关心一个蹩脚的电影演员的吃喝拉撒和鸡毛蒜皮，而不愿了解一个普通人波涛汹涌的内心…… —路瑶《平凡的世界》  世界上有两样东西不能直视，一是太阳，而是人心 —东野奎吾《白夜行》  人的恶，连佛都度不了 —《西游记》  一个人可以被毁灭，但不能被打败。 —海明威《老人与海》  生活就像一座围城，城里的人想出去，城外的人想进来。 —《围城》  其实所有纠结做选择的人心里早就有了答案，咨询只是想得到心里内心所倾向的选择，最终的所谓命运，还是自己一步步走出来的。 —东野圭吾《解忧的杂货铺》  让你难过的事，总有一天你会笑着说出来。 —《肖申克的救赎》  孩子，我要求你读书用功，不是因为我要你跟别人比成绩，而是，我希望你将来拥有选择的权利，选择有意义，有时间的工作，而不是被 迫谋生。当你的工作在你心中有意义，你就有成就感。当你的工作给你时间，不剥夺你的生活，你就有尊严。成就感和尊严，给你快乐。 —《亲爱的安德鲁》  真正有气质的淑女，从不炫耀她所拥有的一切，她不告诉人她读过什么书，去过什么地方，有多少件衣服，买过什么珠宝，因为她没有自 卑感。 —《圆舞》  十年饮冰，难凉热血。 —梁启超《饮冰室合集》  每逢你想要批评任何人的时候，你就记住，这个世界上所有人，并不是个个都有过你拥有的那些优越条件。 —《了不起的盖茨比》  我什么都没忘，只是有些事适合收藏。 —史铁生《我与地坛》  这是一个最好的时代，这是一个最坏的时代；这是一个智慧的年代，这是一个愚蠢的年代；这是一个光明的季节，这是一个黑暗的季节 —狄更斯《双城记》  被真相伤害总比被谎言欺骗的好，得到了再失去，总是比从来就没有得到更伤人。 —《追风筝的人》  也许每一个男子都有过这样两个女人，至少两个。娶了红玫瑰，久而久之，红的变成了墙上的一抹蚊子血，白的还是床前明月光。娶了白 玫瑰，白的便是衣服上沾的一粒饭粘子，红的却是心口上一粒朱砂痣。 —张爱玲《红玫瑰与白玫瑰》  庭有枇杷树，吾妻死之年所手植也，今已亭亭如盖矣。 —《项脊轩志》  哪有人喜欢孤独，不过是不喜欢失望罢了。 —村上春树《挪威的森林》  钱钟书先生对杨降，从今往后，咱们只有死别，再无生离。 —《我们仨》  对世界而言，你是一个人；但对于某个人，你是他的整个世界。 —《飘》  我知道我长的丑，被仍石头无所谓，但让你害怕让我觉得很难过。 —《巴黎圣母院》  海底月是天上月，眼前人是心上人。 —《我不喜欢这世界，我只喜欢你》  这世上真话本就不多，一位女子的脸红胜过一大段对白。 —《骆驼祥子》  I love three things in this world, Sun, moon and you, Sun for morning, moon for night, and you forever.</description>
    </item>
    
    <item>
      <title>自我完善</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/self-improvement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/self-improvement/</guid>
      <description>自信、乐观开朗、有活力、笑     自信打不死的心态活到老。    凡事都要有自信。展现在生活的方方面面。    不要低声下气。    对未来乐观、有热情。    It is not the strongest of the species that survives nor the most intelligent. It is the one that is the most adaptable to change. – Charles Darwin    要多笑，这么多年的牙可别白刷了    You got a dream, you gotta protect it. People can&amp;#39;t do something themeselves, they wanna tell you you can&amp;#39;t do it.</description>
    </item>
    
    <item>
      <title>见招拆招</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/seethemove/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/seethemove/</guid>
      <description>机器学习并不是若干算法的堆积，熟练掌握了“十大算法”或“二十大算法”并不能让一切问题迎刃而解。所以不能将目光仅聚焦在具体算法 的推导和编程实现上。基本算法仅能呈现典型“套路”，而现实世界任务千变万化，以有限的套路应对无限的变化，焉有不败！所以务必掌 握算法背后的思想脉络，面对现实问题时，根据任务特点对现有套路进行改造融通。无论科研创新还是应用实践，皆以此为登堂入室之始。 — 周志华 &amp;lt;机器学习&amp;gt;  还有张三丰传授太极剑法给张无忌时，张无忌忘记了剑法具体的招式，仅记住了太极剑法的指导思想，从而根据敌人的招式使用相应的制 敌之策，达到见招拆招的目的。  Easy to learn. Hard to master. YOLO   下面举 YOLO 算法中的几个的例子    作者通过均方无法来计算预测的 binding boxes 和 ground truth 之间的误差，由于 loss function 中还包含分类错误的误差。而 由于大多数的 grid cell 都没有目标，所以不应该让有目标和没有目标的 grid cell 产生相同权重的误差，所以作者让目标位置误 差的权重 \(\lambda_{coord} = 5\) ，让没有目标的分类误差权重 \(\lambda_{noobj} = 0.5\) ，从而来平衡由于数量悬殊导致的 问题。    使用均方误差计算，size 比较大的目标相比于 size 比较小的目标产生更大的误差。所以作者使用开方之后的宽度和高度值相减然后 求平方。    作者使用 224x224 的图像在 ImageNet 上使用分类网络对检测网络进行预训练，同时作者想让检测网络输入的分辨率为 448x448 。 由于需要同时改变输入的尺寸以及网络的任务，作者先使用 448x448 的ImageNet 对网络进行 fine-tune，然后在使用检测误差进行 调优，以达到更好的效果。    作者想要使用 Anchor Boxes ，但是 R-CNN 一般都是人为设定其大小，这个是目标的先验，如果能有更好的先验，那么应该会有更好 的检测结果，所以作者使用 k-means 聚类方法来求取 Anchor Boxes 先验的大小。    k-means 算法一般使用欧式距离度量误差，而此处，作者真实关心的是两者的 IOU ，所以作者用 1 - IOU 作为损失函数。    可以发现作者始终在依据自己的实际需求，对算法进行了一些改进，而不是直接生搬硬套 。 Debug - Diagnostic   吴恩达老师也非常强调运用自己的聪明智慧去设计诊断方法。自己思考，自己想测试方法，去发现问题到底出现在哪里，而不要盲目的去 修改测试，白白浪费更多的时间。 比如一定要找到是算法的收敛性问题还是目标函数选择有问题？很多人在 \(J(\theta)\) 有问题的 时候，却始终在不断增加迭代次数。  花费在诊断上的时间通常在 1/3 - 1/2 之间，但这些时间是值得的。Error analysis and diagnostic also give insight into the problem.</description>
    </item>
    
    <item>
      <title>话题</title>
      <link>https://kylestones.github.io/hugo-blog/blog/emotion/topic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/emotion/topic/</guid>
      <description> 社交网络     人们总是倾向于在网络上展现理想的自我，这与真实的形象一定存在落差。很多人把这归结为欺骗和背叛。    社交网络给人太多幻想，让人们以为自己在任何情况下都会被关注。这种被关注的压力是双向的，一方面人们会为了在他人眼中表现 的更完美，而强迫自己去做很多不必要的事。在被动消费的情况下，看到更多其他人精心展现的形象之后，下意识地以更高的标准来 评估和改造自己；另一方面又会占据情感高点，看不上那些在社交媒体上不如自己有吸引力的朋友。    社交网络让每个人都更加完美了，但也更加孤独了。   友谊     相比于美丽闪耀的时刻，脆弱和尴尬才是催化友谊的重要契机，因为暴露弱点这一行为体现了信任，而彼此信任是友谊的基础。   读书     傲慢的人，最容易被收买—只要吹捧几句。傲慢是因为受尊重不足。    大仲马在《基督山伯爵》一文的最后告诉人们：人类的全部智慧都包含在两个词里面—等待与希望。   自信   从小缺乏自信的孩子有什么表现：    走路很慢，低头，遇人就缩在路里边    说话声音很小    不敢看人的脸    别人分东西时每次拿少的那份    害怕别人看到自己优秀    害怕别人看到自己出丑    害怕被人反问    在天黑没有人没有灯光没有星星和月亮的晚上特别的开心和激动，有时会偷偷地笑或者哭    试卷做完了不交，等很多人都交了才交    排队从来不敢站最前面    人多的时候尽量不动，不说话，不笑    但心里一直在渴望成为那个大家都喜欢的人   情商     真正的情商高手少不了对情绪的洞察   </description>
    </item>
    
  </channel>
</rss>
