<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blogs | Org Mode</title><meta name=keywords content><meta name=description content="Blogs - Org Mode"><meta name=author content="Me"><link rel=canonical href=https://kylestones.github.io/hugo-blog/blog/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/hugo-blog/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><link rel=icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://kylestones.github.io/hugo-blog/blog/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="Blogs"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://kylestones.github.io/hugo-blog/blog/"><meta property="og:image" content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blogs"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://kylestones.github.io/hugo-blog/blog/"}]}</script></head><body class=list id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io/hugo-blog accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/hugo-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://kylestones.github.io/hugo-blog>Home</a></div><h1>Blogs
<a href=index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9" /><path d="M4 4a16 16 0 0116 16" /><circle cx="5" cy="19" r="1" /></svg></a></h1></header><article class=post-entry><header class=entry-header><h2>cuda 编程基础</h2></header><div class=entry-content><p>学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构 CPU 可以分成 数据通道 和 控制逻辑 两个部分。 Fetch 取址 -> Decode 译码 -> Execute 执行 -> Memory 访存 -> Writeback 写回 Pipeline 流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing 不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches 分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90% 分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测 IPC instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度 Read-After-Write - RAW Write-After-Read - WAR Write-After-Write - WAW 寄存器重命名来改善 Out-of-Order OoO 乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构 寄存器 - L1 - L2 - L3 - 主存 - 硬盘 硬件管理 L1 , L2, L3 软件管理 主存 , 硬盘 缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。 分区 Banking ，避免多端口 一致性 coherency 控制器 Memory Controller ，多个通道，增加带宽 编址方式 Shared Memory Distributed Memory Hybrid Distributed-shared Memory CPU 内部并行性 指令级并行 Instruction-Level extraction 超标量、乱序执行 数据并行 Data-Level Parallelism 矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽 线程级并行 Thread-Level Parallelism 同步多线程 Simultaneous Multithreading SMT ，多核 Multicore Multicore 真多核 除最后一级缓存外，不共享其他资源； 假多核 可能只是多个 ALU 锁存 多个线程读写同一款数据：加锁； 谁的数据是正确的 ： 缓存一致性协议 Cohernce 什么样的数据是正确的 Consistency ： 存储器同一性模型 Powerwall 由于能量墙的限制，导致摩尔定律无法保持。 新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升； 处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵 SISD SIMD MISD NIND 名词不解释 Task 任务 Parallel Task 并行任务 Serial Execution 串行执行 Parallel Execution Shared Memory 共享存储 Distributed Memory 分布式存储 Communication 通信 Synchronization 同步 –> 破坏了独立性、并行性 Granularity 粒度 –> 任务划分的粒度 Observed Speedup –> 加速比 Parallel Overhead 并行开销 –> 通信、同步 Scalability 可扩展性 –> GPU 从 4 核到 400 核时，性能上的提升 并行编程模型 共享存储模型 shared memory model 线程模型 threads model 消息传递模型 message passing model 数据并行模型 Data Parallel Model –> 对数据切分 OpenMP , MPI , SPMD , MPMD Amdahl's Law 程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p) 设计并行处理程序和系统 GPU 设计思路 去掉复杂的分支预测、乱序执行、内存管理等单元 设计加入多个核（多个 SM） 在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流） 提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程 最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。 具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。 一个 Grid 内，每个 Block 的线程数是一样的 Block 内每个线程可以 synchronize 同步； Block 内每个线程都可以访问 shared memory ； 每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）； 一个 Block 内的所有线程必须位于同一个 SM 中 ； Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展 GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。 GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O I/O PCIE SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务） GPU 架构决定，编写 GPU 代码的时候需要注意 尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归 不要使用静态变量 少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测） 小心通过指针实现函数调用（注意区分设备侧和主机侧地址） GPU 内存模型 寄存器 片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器 Local Memory 在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache Shared Memory 片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict Global Memory 片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写 Constant Memory 在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写 存储器 编程声明 作用域 生命期 register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel local 编译器管理，自动变量数组 thread kernel shared _ shared_ int sharedVar block kernel global _ device_ int globalVar grid application constant _ constant_ int constantVar grid application 线程调度 cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。 warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。 特征： 在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中） 同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO） warp 内所有线程始终执行相同的指令 divergent warp 由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。 问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？ 答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。 架构 SM 中 SP 的数量 开普勒 192 mashival 128 Fermi 32 问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？ 答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥% 线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { ....</p></div><footer class=entry-footer>5 min&nbsp;·&nbsp;1030 words&nbsp;·&nbsp;Kyle Three Stones</footer><a class=entry-link aria-label="post link to cuda 编程基础" href=https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/></a></article><article class=post-entry><header class=entry-header><h2>Darknet</h2></header><div class=entry-content><p>darknet 以 layer 为中心，通过构建不同的 layer 构成一个 net ； 所有的层保存在 net.layers 中 net->layers = calloc(net->n, sizeof(layer)); darknet 需要一个“.txt”文件，每行表示一张图像的信息： &lt;object-class> &lt;x> &lt;y> &lt;width> &lt;height> 每个单元格会预测B个边界框（bounding box）以及边界框的置信度（confidence score）。 所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 Pr(object)，后者记为 预测框与实际框（ground truth）的 IOU 。很多人可能将 Yolo 的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的 乘积，预测框的准确度也反映在里面。 中心坐标的预测值 (x,y) 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的。而边界框的 w 和 h 预测值 是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 [0,1] 范围。通过 sigmoid 函数保证 (x,y) 在 0-1 之间， 这样，每个边界框的预测值实际上包含5个元素：(x,y,w,h,c)，其中前4个表征边界框的大小与位置，而最后一个值是置信度。 加速库： NNPACK</p></div><footer class=entry-footer>1 min&nbsp;·&nbsp;53 words&nbsp;·&nbsp;Kyle Three Stones</footer><a class=entry-link aria-label="post link to Darknet" href=https://kylestones.github.io/hugo-blog/blog/machinelearning/darknet/></a></article><article class=post-entry><header class=entry-header><h2>Deep Learning</h2></header><div class=entry-content><p>神经网络和深度学习 神经网络概论 结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等 非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字 人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣 钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也 可以处理非结构化的数据。 每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会人为决定每个神经元 的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神 经网络在监督学习中效果很好很强大。 神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以 及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的 算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法] scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神 经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好 的 SVM 算法结果要优于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在达到一定的性能之后效果变无 法继续提升，而神经网络此时的效果将明显领先于传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算法 都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速度， 因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，从 而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高（ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值）。速度的提升使得我们可以训练大型的网络或者在一定的时间内完成网络 的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断循环，迭代的更快使得验证自己的想法更加快速得到 验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。 1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任 意m 维到 n 维的映射）。虽然如此，但是若要模拟复杂的函数可能需要特别特别多的隐层神经元，因此现代网络总是加大网络的深度， 以让每一层的函数尽量简单，而整个网络完成复杂的映射。 神经网络基础 一张彩色图像像素点由 RGB 三个通道组成，作为神经网络的输入时，将三个矩阵都转换成向量并拼接起来组成一个列向量 \(x^{(i)} \in \mathbb{R}^{n_x}\)，列向量中先是红色通道的所有像素点，然后是绿色通道的所有像素点，最后是蓝色通道的所有像素点。m 个训 练样本 \(\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(i)},y^{(i)}) \}\) ；同时使用 \(X \in \mathbb{R}^{n_x \times m} \) 表示所有的训练样本\[X= \left[ \begin{array}{cccc} | & | & & | \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\ | & | & & | \end{array} \right] \] 相比于让每个样本按行向量堆叠，在神经网络中构建过程会简单很多。\(y^{(i)} \in \{0,1\}\)同 时将所有的标签组成一个行向量 \(Y \in \mathbb{R}^{1 \times m}\) \[ Y = [ y^{(1)}, y^{(2)}, \cdots, y^{(m)} ]\] 在Python 中使用 (n,m) = X....</p></div><footer class=entry-footer>22 min&nbsp;·&nbsp;4670 words&nbsp;·&nbsp;Kyle Three Stones</footer><a class=entry-link aria-label="post link to Deep Learning" href=https://kylestones.github.io/hugo-blog/blog/machinelearning/deeplearning/></a></article><article class=post-entry><header class=entry-header><h2>fast.ai</h2></header><div class=entry-content><p>调参 分阶段使用多个学习速率 越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last....</p></div><footer class=entry-footer>4 min&nbsp;·&nbsp;670 words&nbsp;·&nbsp;Kyle Three Stones</footer><a class=entry-link aria-label="post link to fast.ai" href=https://kylestones.github.io/hugo-blog/blog/machinelearning/fast-ai/></a></article><article class=post-entry><header class=entry-header><h2>Faster R-CNN</h2></header><div class=entry-content><p>预处理 减去 RGB 像素的均值（无论训练还是测试统一使用训练样本集的均值） Rescale 在图像的长边不超过阈值的情况下，将短边 resize 到指定值 def img_rescale(img, targetSize=600, maxSize=1000): w = img.width h = img.height minDim = min(w,h) maxDim = max(w,h) scale = targetSize / minDim if scale * maxDim > maxSize: scale = maxSize / maxDim img = rescale(img, scale) return img 为什么要这样 rescale 呢？这样仅仅只能让多数的图像的短边统一长度，长边的长度可能各不相同；另外一些图像长边都是最大值，但 是短边各不相同。这样做有什么意义呢？保持图像的横纵比？但是图像大小不一样，要怎样去训练呢？ RoI Pooling RPN 生成的 RoI 由 (r,c,h,w) 表示，其中 (r,c) 是左上角的坐标，(h,w) 是高和宽。 RoI max pooling works by dividing the h*w RoI window into an H*W grid of sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window into the corresponding output grid cell....</p></div><footer class=entry-footer>5 min&nbsp;·&nbsp;989 words&nbsp;·&nbsp;Kyle Three Stones</footer><a class=entry-link aria-label="post link to Faster R-CNN" href=https://kylestones.github.io/hugo-blog/blog/machinelearning/r-cnn/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://kylestones.github.io/hugo-blog/blog/page/2/>« Prev</a>
<a class=next href=https://kylestones.github.io/hugo-blog/blog/page/4/>Next »</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io/hugo-blog>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>