<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mask R-CNN | Org Mode</title><meta name=keywords content="目标分割,,深度学习"><meta name=description content="训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0."><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/hugo-blog/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/hugo-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="Mask R-CNN"><meta property="og:description" content="训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0."><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/"><meta property="og:image" content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Mask R-CNN"><meta name=twitter:description content="训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"Mask R-CNN","item":"https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mask R-CNN","name":"Mask R-CNN","description":"训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0.","keywords":["目标分割,","深度学习"],"articleBody":" 训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0.5, 1, 2]    在 RPN 网络最后一层的 feature map 上每个点生成一组 anchor （按照指定的 scale 和 ratio）    判断为 positive anchor 的标准：    anchor 和 gtbb IoU  0.7 ；    anchor 和 gtbb 有最大的 IoU ；    判断为 negative anchor 的标准：    anchor 和任意 gtbb IoU   Mask RCNN 需要 P N anchor 来训练 RPN ，每张图片上采集 256 个 anchor ，正负 anchor 的比例为 1:1 。    anchor num 256/per image   P N anchor ratio 1:1     注意：    判断 PA ，一般第一种情况即可满足，少数情况下需要用到第二种判别方法    非 P 非 N anchor 不参与训练    positive anchor 不足时，使用 negative anchor 补足    Faster R-CNN 中训练时会忽略跨越图像边界的 anchor，否则训练不易收敛；但是 FPN 中训练时保留了与图像边界相交的 anchor ； 测试时保留，仅裁剪到图像的边界    在 FPN 网络中，将不同大小的 anchor 分配到了网络的不同 level 上，靠近输入的网络层分配到较小的 anchor ，靠近输出的网络层分 配到较大 anchor 。将 [32^2, 64^2, 128^2, 256^2, 512^2] 分别分配到了 [P2, P3, P4, P5, P6] 。 region proposal   anchor 经过 RPN 网络得到 region proposal ，会有该 anchor 中包含目标的概率 cls ，以及对 anchor 坐标的一个修正，以更好的框 住目标。  Faster RCNN    在 1000*600 的图片上，变换到 60*40 的 feature map ，每个点 9 个 anchor ，共有 60*40*9 大约 20000 个 anchor    去掉与图片边界相交的 anchor ，剩余大约 6000 个    依据 RPN 网络得到 anchor 的 cls ，使用 NMS ，阈值采用 0.7 ，最终得到大约 2000 个 region proposal   RoIs for detection and segment   RPN 生成的 region proposal 用于训练检测和分割网络    RoIs num 512/per image for FPN   P N RoIs ratio 1:3     从每张图片生成的 region proposal 中 sample N 个作为 RoIs(region of interest)，用于训练检测和分割网络。在 FPN 做 backbone 的网络中，让 N=512 。且 positive 和 negative 的比例为 1:3 。为什么采用这个比例呢（这个是 RCNN paper 中提到的 25%-75% TODO）？可能 positive RoI 比较少  detection positive RoIs 标准：    RoI 与 gtbb IoU = 0.5 即认为是 positive anchor    否则就认为是 negative anchor    上面是 Mask RCNN paper 中指出的，但是 RCNN 中还特意写了 IoU   然后将采样的 RoIs 经过 RoI Align 后用于检测和分割的网络。  Mask R-CNN Implementation Details  Training:  采用 image-centric 训练方法。  RPN 和 Mask R-CNN 使用相同的 backbone ，且共享参数。  mask branch 一个 box 会生成 k 个 masks ，但计算误差时，只使用 k-th mask ，这里的 k 表示这个 box 预测的类别。使用浮点数表 示大小的 m*m mask resize 为 box 的 size ，且将其中的值以 0.5 为阈值进行二值化。mask loss 为类别 k 的的 box mask 和 gt mask 的 IoU 。计算时可以将两个 mask 都 resize 到整张图片的大小，然后计算两个 mask的内积就是 IoU 。  训练时是将所有用于检测的 box 都用于训练 mask 了吗？  Inference:  测试时，FPN 网络使用 region proposal 的个数为 1000 ，在这 1000 个 RoIs 上进行目标检测，然后使用检测分数最高的 100 个 box 进行 mask 分割 annotation  输出  RPN   Region Proposal Networks 是一个全卷积网络 a fully convolutional network 。  在 RPN 和检测共享网络最后一层上使用 3*3 的滑动窗口扫描 feature maps ，为每个位置生成一个固定长度的特征，然后使用该特征利 用全连接来提取 anchor 的分类和回归参数 a box-regression layer (reg) and a box-classification layer (cls)。具体可实现为先 使用 3*3*512 卷积得到 channel 为 512 的特征，然后使用 1*1*2k 卷积得到分类概率，使用 1*1*4k 卷积得到 region proposal 的回 归参数。由于这里是用不同的 channel 来表示不同的 anchor ，所以 k anchor 的 cls 和 reg 并不共享参数。    每个 feature map 上的一个点包含 2k 个 softmax 类别概率 scores ，其中 k 是 anchor 的个数，且使用二分类的 softmax 来表 示是否包含目标    每个 RoIs 包含 4k 个相对于 anchor 的坐标回归参数    根据生成的 region 的区域大小，分给不同的 FPN 的不同 level \\(P_k\\) 。FPN 网络的不同 level 使用不同大小的 anchor 生成了一 系列的 region proposal ，这些 region proposal 会对原始的 anchor box 进行修正，修正后的 region 会和原始的 anchor 大小不一 致，此时需要根据修正后的 region proposal 的大小，将其重新分配给 FPN 的不同 level 。公式如下：  \\begin{equation} k = k_0 + \\lfloor \\log_2 (\\sqrt{wh} / 224) \\rfloor \\end{equation}  其中 224 是 imageset 中图像的尺寸； \\(k_0\\) 表示 \\(w*h=224^2\\) 的 RoI 应该映射的 level ，论文中设置 \\(k_0=4\\) ；  RoI 尺寸比较小的会被分到较小的 level 上，否则会被分配到较大的 level 上。即在靠近输入的 feature map 上检测尺寸较小的目标， 在远离输入的 feature map 上检测尺寸较大目标。  当然不管分配到哪个 level 上，后续的检测分割 head 的参数都是共享。  将采样的 RoIs 经过 RoI Align 后用于检测和分割的网络。 object detection     C+1 softmax    4C reg    RoI Align - 1024fc - 1024fc - class / box  Mask R-CNN TF 中 fpn_classifier_graph() instance segmentation     28*28*C mask    RoI Align - 14*14*256 X4 - 14*14*256 - 28*28*256 - 28*28*80  这里前四个 14*14*256 的卷积和随后的 14*14*256 的卷积有什么区别吗？ TODO 测试 inference  输入  region proposal     在 1000*600 的图片上，变换到 60*40 的 feature map ，每个点 9 个 anchor ，共有 60*40*9 大约 20000 个 anchor    将与图片边界相交的 anchor 裁剪到图像的边界    依据 RPN 网络得到 anchor 的 cls ，使用 NMS ，阈值采用 0.7    使用 Top N region 用于检测；Mask RCNN Tensorflow 代码中 N = 100    Faster RCNN 文中验证，去掉提取 region proposal 的 RPN 网络，直接使用 anchor 来检测， mAP 会下降。准确的 region proposal 对与检测结果很重要。  On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52.1%. This suggests that the high-quality proposals are mainly due to the regressed box bounds. The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection. 输出  region proposal  RoI Pooling   \\(x_i\\) 是 RoI Pooling 的第 i 个输入，\\(y_{r_j}\\) 是第 r 个 RoI 的第 j 个 sub-window 的输出。  A single \\(x_i\\) may be assigned to several different output \\(y_{r_j}\\)  TODO  \\begin{equation} \\frac{\\partial L}{\\partial x_i} = \\sum_r \\sum_j [i = i^*(r,j)] \\frac{\\partial L}{ \\partial y_{r_j} } \\end{equation} 起因   去北京周同科技面试的时候，两轮技术面，面试官都让我讲解一个算法，要求是讲清楚算法的输入、输出是什么？突然发现自己并没有好 好去分析某个算法的具体输入输出，而仅仅看了一下算法改进的关键点。其实是对算法的整体流程并不清楚，故在此分析总结。  YOLO 最终输出？ 是怎样和训练样本的标注数据比较得到误差的？  去迈外迪面试的时候，面试官问我， VGG、GoogLeNet、ResNet、DenseNet 的作者是谁，我除了何凯明大神，其他的都不知道。面试官说， 当然这些都不重要，但是可以反应出你对这个圈子是否熟悉。本来信心就不足，还答错了一个非常非常低级的错误卷积层参数的个数  所有标有 TODO 的位置都表示未理解 待参考     https://blog.csdn.net/zziahgf/article/details/78730859   ","wordCount":"788","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io/hugo-blog accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/hugo-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io/hugo-blog>Home</a></div><h1 class=post-title>Mask R-CNN</h1><div class=post-meta>4 min&nbsp;·&nbsp;788 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/machinelearning/MaskRCNN.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h3 id=headline-1>训练 train</h3><h4 id=headline-2>输入</h4><h5 id=headline-3>mini-batch</h5><p>论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。</p><h5 id=headline-4>image spatial</h5><p>图片大小</p><p>到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO</p><h5 id=headline-5>anchor</h5><p>anchor 用于生产目标的最初候选区域</p><p>生成 anchor 的方法</p><ol><li><p>anchor scale [32, 64, 128, 256, 512]</p></li><li><p>anchor ratio [0.5, 1, 2]</p></li><li><p>在 RPN 网络最后一层的 feature map 上每个点生成一组 anchor （按照指定的 scale 和 ratio）</p></li></ol><p>判断为 positive anchor 的标准：</p><ol><li><p>anchor 和 gtbb IoU > 0.7 ；</p></li><li><p>anchor 和 gtbb 有最大的 IoU ；</p></li></ol><p>判断为 negative anchor 的标准：</p><ol><li><p>anchor 和任意 gtbb IoU &lt; 0.3</p></li></ol><p>Mask RCNN 需要 P N anchor 来训练 RPN ，每张图片上采集 256 个 anchor ，正负 anchor 的比例为 1:1 。</p><table><tbody><tr><td>anchor num</td><td>256/per image</td></tr><tr><td>P N anchor ratio</td><td>1:1</td></tr></tbody></table><p>注意：</p><ol><li><p>判断 PA ，一般第一种情况即可满足，少数情况下需要用到第二种判别方法</p></li><li><p>非 P 非 N anchor 不参与训练</p></li><li><p>positive anchor 不足时，使用 negative anchor 补足</p></li><li><p>Faster R-CNN 中训练时会忽略跨越图像边界的 anchor，否则训练不易收敛；但是 FPN 中训练时保留了与图像边界相交的 anchor ；
测试时保留，仅裁剪到图像的边界</p></li></ol><p>在 FPN 网络中，将不同大小的 anchor 分配到了网络的不同 level 上，靠近输入的网络层分配到较小的 anchor ，靠近输出的网络层分
配到较大 anchor 。将 [32^2, 64^2, 128^2, 256^2, 512^2] 分别分配到了 [P2, P3, P4, P5, P6] 。</p><h5 id=headline-6>region proposal</h5><p>anchor 经过 RPN 网络得到 region proposal ，会有该 anchor 中包含目标的概率 cls ，以及对 anchor 坐标的一个修正，以更好的框
住目标。</p><p>Faster RCNN</p><ol><li><p>在 1000*600 的图片上，变换到 60*40 的 feature map ，每个点 9 个 anchor ，共有 60*40*9 大约 20000 个 anchor</p></li><li><p>去掉与图片边界相交的 anchor ，剩余大约 6000 个</p></li><li><p>依据 RPN 网络得到 anchor 的 cls ，使用 NMS ，阈值采用 0.7 ，最终得到大约 2000 个 region proposal</p></li></ol><h5 id=headline-7>RoIs for detection and segment</h5><p>RPN 生成的 region proposal 用于训练检测和分割网络</p><table><tbody><tr><td>RoIs num</td><td>512/per image for FPN</td></tr><tr><td>P N RoIs ratio</td><td>1:3</td></tr></tbody></table><p>从每张图片生成的 region proposal 中 sample N 个作为 RoIs(region of interest)，用于训练检测和分割网络。在 FPN 做 backbone
的网络中，让 N=512 。且 positive 和 negative 的比例为 1:3 。为什么采用这个比例呢（这个是 RCNN paper 中提到的 25%-75%
TODO）？可能 positive RoI 比较少</p><p>detection positive RoIs 标准：</p><ol><li><p>RoI 与 gtbb IoU >= 0.5 即认为是 positive anchor</p></li><li><p>否则就认为是 negative anchor</p></li><li><p>上面是 Mask RCNN paper 中指出的，但是 RCNN 中还特意写了 IoU &lt; 0.1 的情况 TODO</p></li></ol><p>然后将采样的 RoIs 经过 RoI Align 后用于检测和分割的网络。</p><p>Mask R-CNN Implementation Details</p><p>Training:</p><p>采用 image-centric 训练方法。</p><p>RPN 和 Mask R-CNN 使用相同的 backbone ，且共享参数。</p><p>mask branch 一个 box 会生成 k 个 masks ，但计算误差时，只使用 k-th mask ，这里的 k 表示这个 box 预测的类别。使用浮点数表
示大小的 m*m mask resize 为 box 的 size ，且将其中的值以 0.5 为阈值进行二值化。mask loss 为类别 k 的的 box mask 和 gt
mask 的 IoU 。计算时可以将两个 mask 都 resize 到整张图片的大小，然后计算两个 mask的内积就是 IoU 。</p><p>训练时是将所有用于检测的 box 都用于训练 mask 了吗？</p><p>Inference:</p><p>测试时，FPN 网络使用 region proposal 的个数为 1000 ，在这 1000 个 RoIs 上进行目标检测，然后使用检测分数最高的 100 个 box
进行 mask 分割</p><h5 id=headline-8>annotation</h5><h4 id=headline-9>输出</h4><h5 id=headline-10>RPN</h5><p>Region Proposal Networks 是一个全卷积网络 a fully convolutional network 。</p><p>在 RPN 和检测共享网络最后一层上使用 3*3 的滑动窗口扫描 feature maps ，为每个位置生成一个固定长度的特征，然后使用该特征利
用全连接来提取 anchor 的分类和回归参数 a box-regression layer (reg) and a box-classification layer (cls)。具体可实现为先
使用 3*3*512 卷积得到 channel 为 512 的特征，然后使用 1*1*2k 卷积得到分类概率，使用 1*1*4k 卷积得到 region proposal 的回
归参数。由于这里是用不同的 channel 来表示不同的 anchor ，所以 k anchor 的 cls 和 reg 并不共享参数。</p><ol><li><p>每个 feature map 上的一个点包含 2k 个 softmax 类别概率 scores ，其中 k 是 anchor 的个数，且使用二分类的 softmax 来表
示是否包含目标</p></li><li><p>每个 RoIs 包含 4k 个相对于 anchor 的坐标回归参数</p></li></ol><p>根据生成的 region 的区域大小，分给不同的 FPN 的不同 level \(P_k\) 。FPN 网络的不同 level 使用不同大小的 anchor 生成了一
系列的 region proposal ，这些 region proposal 会对原始的 anchor box 进行修正，修正后的 region 会和原始的 anchor 大小不一
致，此时需要根据修正后的 region proposal 的大小，将其重新分配给 FPN 的不同 level 。公式如下：</p><p>\begin{equation}
k = k_0 + \lfloor \log_2 (\sqrt{wh} / 224) \rfloor
\end{equation}</p><p>其中 224 是 imageset 中图像的尺寸； \(k_0\) 表示 \(w*h=224^2\) 的 RoI 应该映射的 level ，论文中设置 \(k_0=4\) ；</p><p>RoI 尺寸比较小的会被分到较小的 level 上，否则会被分配到较大的 level 上。即在靠近输入的 feature map 上检测尺寸较小的目标，
在远离输入的 feature map 上检测尺寸较大目标。</p><p>当然不管分配到哪个 level 上，后续的检测分割 head 的参数都是共享。</p><p>将采样的 RoIs 经过 RoI Align 后用于检测和分割的网络。</p><h5 id=headline-11>object detection</h5><ol><li><p>C+1 softmax</p></li><li><p>4C reg</p></li></ol><p>RoI Align -> 1024fc -> 1024fc -> class / box</p><p>Mask R-CNN TF 中 fpn_classifier_graph()</p><h5 id=headline-12>instance segmentation</h5><ol><li><p>28*28*C mask</p></li></ol><p>RoI Align -> 14*14*256 X4 -> 14*14*256 -> 28*28*256 -> 28*28*80</p><p>这里前四个 14*14*256 的卷积和随后的 14*14*256 的卷积有什么区别吗？ TODO</p><h3 id=headline-13>测试 inference</h3><h4 id=headline-14>输入</h4><h5 id=headline-15>region proposal</h5><ol><li><p>在 1000*600 的图片上，变换到 60*40 的 feature map ，每个点 9 个 anchor ，共有 60*40*9 大约 20000 个 anchor</p></li><li><p>将与图片边界相交的 anchor 裁剪到图像的边界</p></li><li><p>依据 RPN 网络得到 anchor 的 cls ，使用 NMS ，阈值采用 0.7</p></li><li><p>使用 Top N region 用于检测；Mask RCNN Tensorflow 代码中 N = 100</p></li></ol><p>Faster RCNN 文中验证，去掉提取 region proposal 的 RPN 网络，直接使用 anchor 来检测， mAP 会下降。准确的 region proposal
对与检测结果很重要。</p><p>On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to
52.1%. This suggests that the high-quality proposals are mainly due to the regressed box bounds. The anchor boxes,
though having multiple scales and aspect ratios, are not sufficient for accurate detection.</p><h4 id=headline-16>输出</h4><h3 id=headline-17>region proposal</h3><h3 id=headline-18>RoI Pooling</h3><p>\(x_i\) 是 RoI Pooling 的第 i 个输入，\(y_{r_j}\) 是第 r 个 RoI 的第 j 个 sub-window 的输出。</p><p>A single \(x_i\) may be assigned to several different output \(y_{r_j}\)</p><p>TODO</p><p>\begin{equation}
\frac{\partial L}{\partial x_i} = \sum_r \sum_j [i = i^*(r,j)] \frac{\partial L}{ \partial y_{r_j} }
\end{equation}</p><h3 id=headline-19>起因</h3><p>去北京周同科技面试的时候，两轮技术面，面试官都让我讲解一个算法，要求是讲清楚算法的输入、输出是什么？突然发现自己并没有好
好去分析某个算法的具体输入输出，而仅仅看了一下算法改进的关键点。其实是对算法的整体流程并不清楚，故在此分析总结。</p><p>YOLO 最终输出？ 是怎样和训练样本的标注数据比较得到误差的？</p><p>去迈外迪面试的时候，面试官问我， VGG、GoogLeNet、ResNet、DenseNet 的作者是谁，我除了何凯明大神，其他的都不知道。面试官说，
当然这些都不重要，但是可以反应出你对这个圈子是否熟悉。本来信心就不足，还答错了一个非常非常低级的错误卷积层参数的个数</p><p>所有标有 TODO 的位置都表示未理解</p><h3 id=headline-20>待参考</h3><ol><li><p><a href=https://blog.csdn.net/zziahgf/article/details/78730859>https://blog.csdn.net/zziahgf/article/details/78730859</a></p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/hugo-blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/>目标分割,</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/hugo-blog/blog/machinelearning/map/><span class=title>« Prev</span><br><span>mAP</span></a>
<a class=next href=https://kylestones.github.io/hugo-blog/blog/tools/mplayer/><span class=title>Next »</span><br><span>mpayer</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Mask R-CNN on twitter" href="https://twitter.com/intent/tweet/?text=Mask%20R-CNN&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f&hashtags=%e7%9b%ae%e6%a0%87%e5%88%86%e5%89%b2%2c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Mask R-CNN on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f&title=Mask%20R-CNN&summary=Mask%20R-CNN&source=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Mask R-CNN on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f&title=Mask%20R-CNN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Mask R-CNN on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Mask R-CNN on whatsapp" href="https://api.whatsapp.com/send?text=Mask%20R-CNN%20-%20https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Mask R-CNN on telegram" href="https://telegram.me/share/url?text=Mask%20R-CNN&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fmachinelearning%2fmaskrcnn%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z" /></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io/hugo-blog>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>