<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>cuda 编程基础 | Org Mode</title><meta name=keywords content="cuda,,深度学习"><meta name=description content="学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 -> Decode 译码 -> Execute 执行 -> Memory 访存 -> Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 –> 破坏了独立性、并行性    Granularity 粒度 –> 任务划分的粒度    Observed Speedup –> 加速比    Parallel Overhead 并行开销 –> 通信、同步    Scalability 可扩展性 –> GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model –> 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl's Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { ."><meta name=author content="Kyle Three Stones"><link rel=canonical href=https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/hugo-blog/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/hugo-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="cuda 编程基础"><meta property="og:description" content="学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 -> Decode 译码 -> Execute 执行 -> Memory 访存 -> Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 –> 破坏了独立性、并行性    Granularity 粒度 –> 任务划分的粒度    Observed Speedup –> 加速比    Parallel Overhead 并行开销 –> 通信、同步    Scalability 可扩展性 –> GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model –> 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl's Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { ."><meta property="og:type" content="article"><meta property="og:url" content="https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/"><meta property="og:image" content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="cuda 编程基础"><meta name=twitter:description content="学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 -> Decode 译码 -> Execute 执行 -> Memory 访存 -> Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 –> 破坏了独立性、并行性    Granularity 粒度 –> 任务划分的粒度    Observed Speedup –> 加速比    Parallel Overhead 并行开销 –> 通信、同步    Scalability 可扩展性 –> GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model –> 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl's Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { ."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"cuda 编程基础","item":"https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"cuda 编程基础","name":"cuda 编程基础","description":"学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 -\u0026gt; Decode 译码 -\u0026gt; Execute 执行 -\u0026gt; Memory 访存 -\u0026gt; Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 –\u0026gt; 破坏了独立性、并行性    Granularity 粒度 –\u0026gt; 任务划分的粒度    Observed Speedup –\u0026gt; 加速比    Parallel Overhead 并行开销 –\u0026gt; 通信、同步    Scalability 可扩展性 –\u0026gt; GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model –\u0026gt; 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl\u0026#39;s Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { .","keywords":["cuda,","深度学习"],"articleBody":"  学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 - Decode 译码 - Execute 执行 - Memory 访存 - Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 – 破坏了独立性、并行性    Granularity 粒度 – 任务划分的粒度    Observed Speedup – 加速比    Parallel Overhead 并行开销 – 通信、同步    Scalability 可扩展性 – GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model – 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl's Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { .... __syncthread(); } else { .... __syncthread(); }  CUDA 开发工具     nvcc 编译器    cuda-gdb 调试器，和 gdb 一样，额外增加了 cuda 相关命令，可通过 help cuda 查看    nvprof 性能分析工具，注意后面接的程序要加相对路径，否则会找不到进程    Developer Zone 查看资料；两个主要的手册 和  cuda 和 OpenCL 是同一级别的。 cuda 算法框架     在设备侧分配空间和初始化    并行计算，kernel 函数    I/O 回主机，释放内存    cuda 算法最主要的性能问题是访存 ，GPU 计算能力很强，应该努力 exploit exploied 其计算性能。  矩阵乘法，使用 CPU 代码需要三层循环，外面两次遍历结果矩阵的行和列，第三个循环用于一行和一列的乘和累加；而 cuda 代码已经 不需要外面的双层循环了，因为通过不同的线程索引，由 GPU 的并行线程处理了。同时不需要锁或者同步，因为各个计算单元相互独立， 互不依赖。  只有 block 内的同步，没有全局同步。block 内同步要求线程执行时间尽量接近，否则会浪费计算资源；而全局的同步需要很复杂的硬 件结构，会有很大的开销，所以并没有设计。 GPU 向量数据类型 - 函数库     char[1-4] uchar[1-4]   int[1-4] uint[1-4]   long[1-4]    float[1-4]    double1 double2    int4 i4 = make_int4(1,2,3,4); int x = i4.x; int y = i4.y; int z = i4.z; int w = i4.w;   cuda 提供常见的函数库，如正余弦、指数等，同时提供低精度的版本，函数名前加两个下划线，这些函数执行更快，但精度较低 cuda 优化   有效的数据并行算法 + 针对 GPU 架构特性的优化 = 最优性能 存储优化  较少 CPU 和 GPU 侧数据的传输     Host - device 数据数据传输 PCIe 带宽远低于设备访问 global memory    减少传输：中间数据直接在 GPU 分配、操作、释放；如果没有减少数据传输，将 CPU 代码移植到 GPU 可能无法提升性能；GPU 有足 够的计算单元，不怕计算，怕数据传输    组团传输：大块传输好于小块传输；    内存传输与计算时间重叠：双缓存   访存合并   coalescing 合并是最重要的性能影响因子 。Global memory 延迟通常在数百个 cycles 。  给定一个矩阵以行优先的方式存储于 global memory ，一个 warp 内相邻的 thread 分别读取相邻地址的数据（第 k 个线程读取第 k 段数据，如果有一些交叉或者错位也没有太大关系），就是合适的访存模式。忌讳随机凌乱的访问和一个线程访问很长的地址空间，会造 成多次数据读取，有很大带宽浪费。  设计程序时，优先考虑怎么样设计一个规则的访存访问模式。如果不规则，可以先读入 shared memory 中，然后在 shared memory 中不 规则访问，或者重排； shared memory   shared memory 的设计很大程度上就是由于应对不规则的存储器访问 。shared memory 比 global memory 快上百倍，可以缓存数据来 减少对 global memory 访问次数，线程协作，以及避免不满足合并条件的访存。  shared memory 被分成许多 banks 区块，连续地址被分配到连续的 Banks 上，一个 bank 上存储了多个地址的数据（余数相同的地址存 储在同一个 bank 上）。每个 bank 每个周期可以响应一个地址请求，多个线程访问同一个地址请求有相应的广播机制，没有问题。但对 同一个 bank 进行多个并发访存（不同地址）将导致 bank 冲突 ，冲突的访存必须串行执行。[不明白一个 bank 存储的是一个 bit 还是一个 byte ，不过好像没有什么太大关系，总之是一个 bank 内存储了多个不同地址的数据，同一个时钟周期只能响应一个地址请求]  如果没有 bank 冲突， shared memory 和 registers 访问速度是同量级的。同时有几个线程在访问同一个同一个 bank 的不同地址，就 是几路 bank 冲突 8 way bank conflict 。几路 bank 冲突就会导致性能下降几倍。 避免 bank 冲突的方法 ： 数组宽度加一 tile[TILE_DIM][TILE_DIM] ，来填充 shared memory 数组。这样将使得读取和写入都不存在 bank 冲突。  矩阵转置需要同时访问行和列，这就导致访存不合并。 Texture memory   无法合并访存，同时又有多种寻址方式，以及小的数据区块读取。如坐标越界寻址（warp/clamp） 。  只需要将 global memory 绑定到 texture memory 就可以正常使用。 Occupancy   一个 SM 里面激活 warp/最多可容纳 warp 数目 ； 代表了 GPU 的繁忙程度。  SM 资源动态划分有多个约束，哪一个约束都得满足，那个约束先达到上限，其他约束将失效，影响 SM 同时启动线程的数量；可能导致 performance cliff Grid - Block 大小  Grid size 试探法     block 个数  SM 的个数，保证每个 SM 至少有一个 work-group    block 个数 / SM 个数  2 ，多个 block 可以在 SM 上并发执行，如果一个 block 在等待，启动另一个 block    block 个数 / SM 个数  100 ，对未来设备有良好的伸缩性   Block size     block 大小最好是 32(warp 的大小) 的倍数    尽量多的 warp 进行延迟掩藏    block 通常是 128 , 256 可以进行尝试   Latency Hiding   延迟掩藏：指令按顺序执行，一个线程的任一操作数没有准备好将阻塞等待；延迟可以通过足够多的线程切换来隐藏。  延迟掩藏和 warp 个数以及占用率的计算没有看懂。 Data Prefetching   数据预读取。让数据读取和计算并行 指令优化   较后考虑的事情 num / 2^n -- num  n num / 2^n -- num \u0026(2^n-1)  Unroll loop   循环展开，去掉不必要的判断语句，增加代码速度。但不利于扩展 #pragma unrool BLOCK_SIZE for (i = 0; i  BLOCK_SIZE; ++i) { ... }  总结     有效利用并行性    尽可能合并内存访问    利用 shared memory    利用 Texture / constant memory    减少 bank 冲突   ","wordCount":"1030","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Kyle Three Stones"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/"},"publisher":{"@type":"Organization","name":"Org Mode","logo":{"@type":"ImageObject","url":"https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io/hugo-blog accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/hugo-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://kylestones.github.io/hugo-blog>Home</a></div><h1 class=post-title>cuda 编程基础</h1><div class=post-meta>5 min&nbsp;·&nbsp;1030 words&nbsp;·&nbsp;Kyle Three Stones&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/blog/cuda/cuda-basic.org rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记</p><h3 id=headline-1>重看 CPU</h3><p>CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘
中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向
的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占
用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。</p><h4 id=headline-2>整体架构</h4><ol><li><p>CPU 可以分成 <strong>数据通道</strong> 和 <strong>控制逻辑</strong> 两个部分。</p></li><li><p>Fetch 取址 -> Decode 译码 -> Execute 执行 -> Memory 访存 -> Writeback 写回</p></li></ol><h4 id=headline-3>Pipeline</h4><p>流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法
很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。</p><h4 id=headline-4>Bypassing</h4><p>不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。</p><h4 id=headline-5>Branches</h4><ol><li><p>分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%</p></li><li><p>分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测</p></li></ol><h4 id=headline-6>IPC</h4><p>instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC</p><h4 id=headline-7>指令调度</h4><ol><li><p>Read-After-Write - RAW</p></li><li><p>Write-After-Read - WAR</p></li><li><p>Write-After-Write - WAW</p></li></ol><p>寄存器重命名来改善</p><h4 id=headline-8>Out-of-Order OoO</h4><p>乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器</p><h4 id=headline-9>存储器层次架构</h4><p>寄存器 - L1 - L2 - L3 - 主存 - 硬盘</p><dl><dt>硬件管理</dt><dd><p>L1 , L2, L3</p></dd><dt>软件管理</dt><dd><p>主存 , 硬盘</p></dd></dl><p>缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。</p><ol><li><p>分区 Banking ，避免多端口</p></li><li><p>一致性 coherency</p></li><li><p>控制器 Memory Controller ，多个通道，增加带宽</p></li></ol><p>编址方式</p><ul><li><p>Shared Memory</p></li><li><p>Distributed Memory</p></li><li><p>Hybrid Distributed-shared Memory</p></li></ul><h4 id=headline-10>CPU 内部并行性</h4><dl><dt>指令级并行 Instruction-Level extraction</dt><dd><p>超标量、乱序执行</p></dd><dt>数据并行 Data-Level Parallelism</dt><dd><p>矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽，
寄存器很宽</p></dd><dt>线程级并行 Thread-Level Parallelism</dt><dd><p>同步多线程 Simultaneous Multithreading SMT ，多核 Multicore</p></dd></dl><h4 id=headline-11>Multicore</h4><dl><dt>真多核</dt><dd><p>除最后一级缓存外，不共享其他资源；</p></dd><dt>假多核</dt><dd><p>可能只是多个 ALU</p></dd></dl><h4 id=headline-12>锁存</h4><ul><li><p>多个线程读写同一款数据：加锁；</p></li><li><p>谁的数据是正确的 ： 缓存一致性协议 Cohernce</p></li><li><p>什么样的数据是正确的 Consistency ： 存储器同一性模型</p></li></ul><h4 id=headline-13>Powerwall</h4><p>由于能量墙的限制，导致摩尔定律无法保持。</p><p>新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；</p><p>处理器的存储器带宽无法满足处理能力的提升。</p><h4 id=headline-14>Flynn 矩阵</h4><table><tbody><tr><td>SISD</td><td>SIMD</td></tr><tr><td>MISD</td><td>NIND</td></tr></tbody></table><h4 id=headline-15>名词不解释</h4><ul><li><p>Task 任务</p></li><li><p>Parallel Task 并行任务</p></li><li><p>Serial Execution 串行执行</p></li><li><p>Parallel Execution</p></li><li><p>Shared Memory 共享存储</p></li><li><p>Distributed Memory 分布式存储</p></li><li><p>Communication 通信</p></li><li><p>Synchronization 同步 –> 破坏了独立性、并行性</p></li><li><p>Granularity 粒度 –> 任务划分的粒度</p></li><li><p>Observed Speedup –> 加速比</p></li><li><p>Parallel Overhead 并行开销 –> 通信、同步</p></li><li><p>Scalability 可扩展性 –> GPU 从 4 核到 400 核时，性能上的提升</p></li></ul><h4 id=headline-16>并行编程模型</h4><ul><li><p>共享存储模型 shared memory model</p></li><li><p>线程模型 threads model</p></li><li><p>消息传递模型 message passing model</p></li><li><p>数据并行模型 Data Parallel Model –> 对数据切分</p></li></ul><p>OpenMP , MPI , SPMD , MPMD</p><h4 id=headline-17>Amdahl's Law</h4><p>程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)</p><p>设计并行处理程序和系统</p><h3 id=headline-18>GPU 设计思路</h3><ol><li><p>去掉复杂的分支预测、乱序执行、内存管理等单元</p></li><li><p>设计加入多个核（多个 SM）</p></li><li><p>在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）</p></li><li><p>提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数
的线程</p></li></ol><p>最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda
core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度
一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储
空间，动态分配给需要的单元。</p><p>具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。</p><ol><li><p>一个 Grid 内，每个 Block 的线程数是一样的</p></li><li><p>Block 内每个线程可以 synchronize 同步；</p></li><li><p>Block 内每个线程都可以访问 shared memory ；</p></li><li><p>每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；</p></li><li><p><strong>一个 Block 内的所有线程必须位于同一个 SM 中</strong> ；</p></li><li><p>Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展</p></li></ol><p>GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。</p><p>GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说，
带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。</p><pre class=example>
# CPU GPU 协同方式；好难对齐呀！！！
主存              显存
DRAM             GDRAM
 |                 |
CPU               GPU
 |                 |
I/O  <--------->  I/O
         PCIE
</pre><p>SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多
任务）</p><p>GPU 架构决定，编写 GPU 代码的时候需要注意</p><ol><li><p>尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归</p></li><li><p>不要使用静态变量</p></li><li><p>少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）</p></li><li><p>小心通过指针实现函数调用（注意区分设备侧和主机侧地址）</p></li></ol><h3 id=headline-19>GPU 内存模型</h3><dl><dt>寄存器</dt><dd><p>片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器</p></dd><dt>Local Memory</dt><dd><p>在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索
引访问；新的 GPU 有 cache</p></dd><dt>Shared Memory</dt><dd><p>片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个
SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict</p></dd><dt>Global Memory</dt><dd><p>片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写</p></dd><dt>Constant Memory</dt><dd><p>在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写</p></dd></dl><table><thead><tr><th>存储器</th><th>编程声明</th><th>作用域</th><th>生命期</th></tr></thead><tbody><tr><td>register</td><td>编译器管理，必须是单独的自动变量而不能是数组</td><td>thread</td><td>kernel</td></tr><tr><td>local</td><td>编译器管理，自动变量数组</td><td>thread</td><td>kernel</td></tr><tr><td>shared</td><td><span style=text-decoration:underline>_ shared_</span> int sharedVar</td><td>block</td><td>kernel</td></tr><tr><td>global</td><td><span style=text-decoration:underline>_ device_</span> int globalVar</td><td>grid</td><td>application</td></tr><tr><td>constant</td><td><span style=text-decoration:underline>_ constant_</span> int constantVar</td><td>grid</td><td>application</td></tr></tbody></table><h3 id=headline-20>线程调度</h3><p>cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。</p><p>warp 是 Block 内线程编号连续的 32 个线程， <strong>是线程调度的最小单元</strong> 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上
保证 warp 内的每个线程同步。</p><p>特征：</p><ol><li><p>在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）</p></li><li><p>同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？
TODO）</p></li><li><p>warp 内所有线程始终执行相同的指令</p></li></ol><dl><dt>divergent warp</dt><dd><p>由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必
须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时
候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割，
使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求
一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修
改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。</p></dd></dl><p>问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？</p><p>答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个
时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU
才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。</p><table><thead><tr><th>架构</th><th class=align-right>SM 中 SP 的数量</th></tr></thead><tbody><tr><td>开普勒</td><td class=align-right>192</td></tr><tr><td>mashival</td><td class=align-right>128</td></tr><tr><td>Fermi</td><td class=align-right>32</td></tr></tbody></table><p>问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少
warp 才能隐藏内存延迟？</p><p>答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%</p><p>线程同步可能导致 <strong>死锁</strong> ，需要注意逻辑正确性</p><div class="src src-c"><div class=highlight><pre class=chroma><code class=language-c data-lang=c><span class=cp># 下面代码将导致死锁
</span><span class=cp></span><span class=k>if</span> <span class=p>(</span><span class=n>condition</span><span class=p>)</span> <span class=p>{</span>
    <span class=p>....</span>
    <span class=n>__syncthread</span><span class=p>();</span>
<span class=p>}</span>
<span class=k>else</span> <span class=p>{</span>
    <span class=p>....</span>
    <span class=n>__syncthread</span><span class=p>();</span>
<span class=p>}</span></code></pre></div></div><h3 id=headline-21>CUDA 开发工具</h3><ol><li><p>nvcc 编译器</p></li><li><p>cuda-gdb 调试器，和 gdb 一样，额外增加了 cuda 相关命令，可通过 help cuda 查看</p></li><li><p>nvprof 性能分析工具，注意后面接的程序要加相对路径，否则会找不到进程</p></li></ol><p>Developer Zone 查看资料；两个主要的手册 &lt;CUDA C programming Guide> 和 &lt;CUDA C Best Practices Guide></p><p>cuda 和 OpenCL 是同一级别的。</p><h3 id=headline-22>cuda 算法框架</h3><ol><li><p>在设备侧分配空间和初始化</p></li><li><p>并行计算，kernel 函数</p></li><li><p>I/O 回主机，释放内存</p></li></ol><p><strong>cuda 算法最主要的性能问题是访存</strong> ，GPU 计算能力很强，应该努力 exploit exploied 其计算性能。</p><p>矩阵乘法，使用 CPU 代码需要三层循环，外面两次遍历结果矩阵的行和列，第三个循环用于一行和一列的乘和累加；而 cuda 代码已经
不需要外面的双层循环了，因为通过不同的线程索引，由 GPU 的并行线程处理了。同时不需要锁或者同步，因为各个计算单元相互独立，
互不依赖。</p><p>只有 block 内的同步，没有全局同步。block 内同步要求线程执行时间尽量接近，否则会浪费计算资源；而全局的同步需要很复杂的硬
件结构，会有很大的开销，所以并没有设计。</p><h3 id=headline-23>GPU 向量数据类型 - 函数库</h3><table><tbody><tr><td>char[1-4]</td><td>uchar[1-4]</td></tr><tr><td>int[1-4]</td><td>uint[1-4]</td></tr><tr><td>long[1-4]</td><td></td></tr><tr><td>float[1-4]</td><td></td></tr><tr><td>double1</td><td>double2</td></tr></tbody></table><div class="src src-c"><div class=highlight><pre class=chroma><code class=language-c data-lang=c><span class=n>int4</span> <span class=n>i4</span> <span class=o>=</span> <span class=n>make_int4</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>4</span><span class=p>);</span>

<span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=n>i4</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>y</span> <span class=o>=</span> <span class=n>i4</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>z</span> <span class=o>=</span> <span class=n>i4</span><span class=p>.</span><span class=n>z</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>w</span> <span class=o>=</span> <span class=n>i4</span><span class=p>.</span><span class=n>w</span><span class=p>;</span></code></pre></div></div><p>cuda 提供常见的函数库，如正余弦、指数等，同时提供低精度的版本，函数名前加两个下划线，这些函数执行更快，但精度较低</p><h3 id=headline-24>cuda 优化</h3><p>有效的数据并行算法 + 针对 GPU 架构特性的优化 = 最优性能</p><h4 id=headline-25>存储优化</h4><h5 id=headline-26>较少 CPU 和 GPU 侧数据的传输</h5><ol><li><p>Host - device 数据数据传输 PCIe 带宽远低于设备访问 global memory</p></li><li><p>减少传输：中间数据直接在 GPU 分配、操作、释放；如果没有减少数据传输，将 CPU 代码移植到 GPU 可能无法提升性能；GPU 有足
够的计算单元，不怕计算，怕数据传输</p></li><li><p>组团传输：大块传输好于小块传输；</p></li><li><p>内存传输与计算时间重叠：双缓存</p></li></ol><h5 id=headline-27>访存合并</h5><p><strong>coalescing 合并是最重要的性能影响因子</strong> 。Global memory 延迟通常在数百个 cycles 。</p><p>给定一个矩阵以行优先的方式存储于 global memory ，一个 warp 内相邻的 thread 分别读取相邻地址的数据（第 k 个线程读取第 k
段数据，如果有一些交叉或者错位也没有太大关系），就是合适的访存模式。忌讳随机凌乱的访问和一个线程访问很长的地址空间，会造
成多次数据读取，有很大带宽浪费。</p><p>设计程序时，优先考虑怎么样设计一个规则的访存访问模式。如果不规则，可以先读入 shared memory 中，然后在 shared memory 中不
规则访问，或者重排；</p><h5 id=headline-28>shared memory</h5><p><strong>shared memory 的设计很大程度上就是由于应对不规则的存储器访问</strong> 。shared memory 比 global memory 快上百倍，可以缓存数据来
减少对 global memory 访问次数，线程协作，以及避免不满足合并条件的访存。</p><p>shared memory 被分成许多 banks 区块，连续地址被分配到连续的 Banks 上，一个 bank 上存储了多个地址的数据（余数相同的地址存
储在同一个 bank 上）。每个 bank 每个周期可以响应一个地址请求，多个线程访问同一个地址请求有相应的广播机制，没有问题。但对
同一个 bank 进行多个并发访存（不同地址）将导致 <strong>bank 冲突</strong> ，冲突的访存必须串行执行。[不明白一个 bank 存储的是一个 bit
还是一个 byte ，不过好像没有什么太大关系，总之是一个 bank 内存储了多个不同地址的数据，同一个时钟周期只能响应一个地址请求]</p><p>如果没有 bank 冲突， shared memory 和 registers 访问速度是同量级的。同时有几个线程在访问同一个同一个 bank 的不同地址，就
是几路 bank 冲突 8 way bank conflict 。几路 bank 冲突就会导致性能下降几倍。 <strong>避免 bank 冲突的方法</strong> ： 数组宽度加一
tile[TILE_DIM][TILE_DIM] ，来填充 shared memory 数组。这样将使得读取和写入都不存在 bank 冲突。</p><p>矩阵转置需要同时访问行和列，这就导致访存不合并。</p><h5 id=headline-29>Texture memory</h5><p>无法合并访存，同时又有多种寻址方式，以及小的数据区块读取。如坐标越界寻址（warp/clamp） 。</p><p>只需要将 global memory 绑定到 texture memory 就可以正常使用。</p><h5 id=headline-30>Occupancy</h5><p>一个 SM 里面激活 warp/最多可容纳 warp 数目 ； 代表了 GPU 的繁忙程度。</p><p>SM 资源动态划分有多个约束，哪一个约束都得满足，那个约束先达到上限，其他约束将失效，影响 SM 同时启动线程的数量；可能导致
performance cliff</p><h5 id=headline-31>Grid - Block 大小</h5><h6 id=headline-32>Grid size 试探法</h6><ol><li><p>block 个数 > SM 的个数，保证每个 SM 至少有一个 work-group</p></li><li><p>block 个数 / SM 个数 > 2 ，多个 block 可以在 SM 上并发执行，如果一个 block 在等待，启动另一个 block</p></li><li><p>block 个数 / SM 个数 > 100 ，对未来设备有良好的伸缩性</p></li></ol><h6 id=headline-33>Block size</h6><ol><li><p>block 大小最好是 32(warp 的大小) 的倍数</p></li><li><p>尽量多的 warp 进行延迟掩藏</p></li><li><p>block 通常是 128 , 256 可以进行尝试</p></li></ol><h5 id=headline-34>Latency Hiding</h5><p>延迟掩藏：指令按顺序执行，一个线程的任一操作数没有准备好将阻塞等待；延迟可以通过足够多的线程切换来隐藏。</p><p>延迟掩藏和 warp 个数以及占用率的计算没有看懂。</p><h5 id=headline-35>Data Prefetching</h5><p>数据预读取。让数据读取和计算并行</p><h5 id=headline-36>指令优化</h5><p>较后考虑的事情</p><div class="src src-c"><div class=highlight><pre class=chroma><code class=language-c data-lang=c><span class=n>num</span> <span class=o>/</span> <span class=mi>2</span><span class=o>^</span><span class=n>n</span> <span class=o>--&gt;</span> <span class=n>num</span> <span class=o>&gt;&gt;</span> <span class=n>n</span>
<span class=n>num</span> <span class=o>/</span> <span class=mi>2</span><span class=o>^</span><span class=n>n</span> <span class=o>--&gt;</span> <span class=n>num</span> <span class=o>&amp;</span><span class=p>(</span><span class=mi>2</span><span class=o>^</span><span class=n>n</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span></code></pre></div></div><h5 id=headline-37>Unroll loop</h5><p>循环展开，去掉不必要的判断语句，增加代码速度。但不利于扩展</p><div class="src src-c"><div class=highlight><pre class=chroma><code class=language-c data-lang=c>
<span class=cp>#pragma unrool BLOCK_SIZE
</span><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
<span class=p>...</span>
<span class=p>}</span></code></pre></div></div><h5 id=headline-38>总结</h5><ol><li><p>有效利用并行性</p></li><li><p>尽可能合并内存访问</p></li><li><p>利用 shared memory</p></li><li><p>利用 Texture / constant memory</p></li><li><p>减少 bank 冲突</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://kylestones.github.io/hugo-blog/tags/cuda/>cuda,</a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://kylestones.github.io/hugo-blog/blog/machinelearning/convolution/><span class=title>« Prev</span><br><span>convolution</span></a>
<a class=next href=https://kylestones.github.io/hugo-blog/blog/machinelearning/darknet/><span class=title>Next »</span><br><span>Darknet</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share cuda 编程基础 on twitter" href="https://twitter.com/intent/tweet/?text=cuda%20%e7%bc%96%e7%a8%8b%e5%9f%ba%e7%a1%80&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f&hashtags=cuda%2c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share cuda 编程基础 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f&title=cuda%20%e7%bc%96%e7%a8%8b%e5%9f%ba%e7%a1%80&summary=cuda%20%e7%bc%96%e7%a8%8b%e5%9f%ba%e7%a1%80&source=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share cuda 编程基础 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f&title=cuda%20%e7%bc%96%e7%a8%8b%e5%9f%ba%e7%a1%80"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share cuda 编程基础 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share cuda 编程基础 on whatsapp" href="https://api.whatsapp.com/send?text=cuda%20%e7%bc%96%e7%a8%8b%e5%9f%ba%e7%a1%80%20-%20https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z" /></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share cuda 编程基础 on telegram" href="https://telegram.me/share/url?text=cuda%20%e7%bc%96%e7%a8%8b%e5%9f%ba%e7%a1%80&url=https%3a%2f%2fkylestones.github.io%2fhugo-blog%2fblog%2fcuda%2fcuda-basic%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z" /></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io/hugo-blog>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>