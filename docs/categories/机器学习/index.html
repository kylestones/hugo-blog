<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器学习 | Org Mode</title><meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://kylestones.github.io/hugo-blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/hugo-blog/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as=style><link rel=icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kylestones.github.io/hugo-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://kylestones.github.io/hugo-blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="机器学习"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://kylestones.github.io/hugo-blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><meta property="og:image" content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="机器学习"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=https://kylestones.github.io/hugo-blog accesskey=h title="Home (Alt + H)"><img src=https://kylestones.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kylestones.github.io/hugo-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://kylestones.github.io/hugo-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://kylestones.github.io/hugo-blog>Home</a></div><h1>机器学习</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>Machine Learning</h2></header><div class=entry-content><p>ML *机器学习* machine learning: Field of study that gives computers the ability to learn without being explicitly program. 机器学习并不是仅仅是若干算法的堆积，学会“十大算法”，熟练掌握具体算法算法的推导与编程实现，并不能让所有问题迎刃而解，因为 现实世界的问题千变万化。而应该像张无忌那样，忘记张三丰传授的太极剑法的具体招式，而只记住一些规则和套路，从而根据敌人的招 式去不断变化自己的招式，达到以不变应万变的效果。或者说用 Andrew Ng 的话，要成为一个 master carpenter （顶级木匠），可以 灵活使用工具来制造桌椅，只有手艺差的木匠才会抱怨工具不合适。因此必须把握算法背后的思想脉络，针对具体的任务特点，对现有套 路进行改造融通；要记住算法是 死 的，思想才是 活 的。 数据库提供数据管理技术，机器学习提供数据分析技术。 Supervised Learning 样本有标签的时候称为监督学习 Linear Regression 首先可以根据样本输入的维数来选择参数的个数（最终依据交叉验证的结果来选择模型和特征） \[ h_{\theta}(x) = \sum_{i=1}^{n} \theta_i x_i = \theta^{T}x \] 求取 \(\theta\) 的策略：在训练集上使得 \(h(x)\) 尽可能接近 y 。那么就涉及到距离的定义，距离通常定义为两者差的平方。因此 损失函数(cost function)定义为 \[ J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 \] 认为数据服从高斯分布 \(y|x;\theta \thicksim (\mu, \sigma^2)\) ，即其前置概率估计是高斯分布。 Last Mean Squares Algorithm 利用最小二乘法中的误差函数来描述损失函数；利用梯度下降法不断迭代来减小损失函数：先将参数初始化为某个值，然后不断迭代跟新 参数；跟新参数的规则：old value + 学习率 * 损失函数对该参数求偏导。 \begin{align} \theta_j & := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\ & := \theta_j + \alpha \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}, \quad for \ every \ j \\ & := \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}, \quad for \ every \ j; \ outer \ for \ every \ i \\ \end{align} 最小二乘法(ordinary least squares)仅仅是描述损失函数的模型，本身不是一个最优解工具，可以求解线性以及非线性问题。最优化方 法有：梯度下降法；矩阵解法；牛顿法；坐标上升法； 判断收敛： 两次迭代，发现 \(\theta\) 变化不大 \(J(\theta)\) 基本不变 【常用】 最优化方法 梯度下降法 梯度下降法：结果与初值有关，且最终一定会结束在某个局部最小值。若维度非常高时，可能不存在局部最小值，可能只是鞍点（深度学 习）。数学上，某一函数在该点的方向导数沿着梯度方向取得最大值。 梯度下降法具体包括： batch gradient descent，批量梯度下降法：每次都需要计算所有样本的误差来更新参数 stochastic gradient descent / incremental gradient descent，随机梯度下降法：每次只需要计算一个样本的误差来更新参数。 可以较快的收敛，但参数结果可能始终在最优解周围徘徊，但在实际应用中可以满足 mini-batch：每次使用同样个数的一小批的样本来更新权重参数。 Newton's method 牛顿法用于查找一个函数的零点。假如希望找到使得 \(f(\theta) = 0\) 的 \(\theta\) 值。在 \(\theta\) 处，用函数 f 的切线来近 似 f ，找到切线的零值点；然后在切线的零值点开始继续用该点处的切线来近似表示函数，不断迭代直至参数满足要求。计算零值点时 可以利用直角三角形一个角的对边的长度除以临边的长度来得到该角的正切值，而正切值就是函数在该点的导数。 \begin{equation} \theta := \theta - \frac{f(\theta)}{f'(\theta)} \end{equation} 而求解似然函数的极值点，就是找到似然函数导数的零点 \begin{equation} \theta := \theta - \frac{\ell ' (\theta)}{\ell ''(\theta)} \\ \theta := \theta - H^{-1} _{\theta} \nabla \ell(\theta), \quad Newton-Raphson \end{equation} 牛顿法： 优点：初值不影响结果，一般选择 0 为初值；算法一般都会收敛且收敛速度比梯度下降法快很多，是二次收敛(quadratic conversions)[在解距离最优解足够近时，下一次误差为上一次误差的平方] 缺点：每次迭代都需要计算 Hessian 矩阵（n*n维）。如果特征维数较多，代价会比较大。 Maximum Likelihood Estimation - MLE 似然：参数的函数 概率：数据的概率 极大似然估计：选择参数 \(\theta\) 使得数据出现的可能性尽可能大。 让所有样本出现的概率最大，需要将每个样本的概率都乘起来，而连乘操作易造成数据下溢，因此通常采用对数似然(log-likelihood)， 将连乘转换成相加操作。 贝叶斯学派认为参数是未被观测的随机变量，其本身也可有分布。频率学派认为虽然参数未知，但确实客观存在的固定值。极大似然估计 源自频率学派。 Locally Weighted Linear Regression 只利用待预测样本周围的样本来预测结果。 \begin{align} w^{(i)} = exp \left( - \frac{(x^{(i)}-x)^2}{2\tau^2} \right) \\ \sum_i w^{(i)} (y^{(i)} - \theta^T x^{(i)})^2 \end{align} x 是待预测的值，样本距离 x 的距离 \( |x^{(i)} - x| \) 越近，权重越大；反之则越小。 \(\tau\) 称为波长参数，是一个超参，控制着权重随距离下降的速度。\(\tau\) 越小，权重衰减的钟形越窄；\(\tau\) 越大，权重 衰减的钟形越宽。而且权值函数的积分可能是正无穷而不是像高斯密度函数那样积分值为 1 局部加权线性回归并没有依据训练样本学习得到一些参数，而是在每次预测的时候都需要使用训练样本来决策。是一个非参数学习算法 (non-parametric)，非正式的可以理解为其参数随着训练样本的增加而增加。 Andrew Moore 的 KD tree 讲述了在训练集较多时，高效计算的方法。 Logistic Regression 逻辑斯蒂回归用于处理二分类问题。由于标记只有两个：0 和 1。所以当计算的结果超过 1 或者小于 0 将变得没有意义。因此选取一个 函数值从 0 平滑过度到 1 的函数用于将计算求得的结果重映射到区间[0-1]。这样的函数存在很多，但是逻辑斯蒂回归选择了 sigmoid function，也称为 logistic function。利用广义线性模型也会推导出此处选择 sigmoid 函数。 \begin{align} g(z) = \frac{1}{1+e^{-z}} \\ h_{\theta}(x) = g({\theta}^T x) = \frac{1}{1 + e^{-{\theta}^T x}} \end{align} 假设 \begin{align*} P(y=1|x;θ) & = hθ (x) P(y=0|x;θ) & = 1 - hθ (x) P(y|x;θ) & = (hθ (x))^y (1 - hθ (x))1-y \end{align*} 似然函数：损失函数没有选择成算法输出与标记误差的平方，是因为这样的损失函数是非凸的，会有很多局部最优解而无法找到全局最优 解。 \begin{align*} ℓ(θ) & = ln L(θ) & = ln ∏i=1^m p(y|x;θ) & = ln ∏i=1^m (hθ (x(i)))^{y(i)} (1 - hθ (x(i)))^{1-y(i)} & = ∑i=1^m y(i) ln h(x(i)) + (1-y(i))ln(1-h(x(i))) \end{align*} 目标函数对 \(\theta\) 的每个分量求偏导，且求解过程直接使用 sigmoid 函数的导数表达式可简化计算。最终得到参数的迭代表达式 \begin{align} \frac{\partial}{\partial \theta_j} \ell(\theta) & = (y-h_{\theta}(x))x_j \\ \theta_j & := \theta_j + \alpha \sum_{i=1}^m(y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \\ \theta_j & := \theta_j + \alpha(y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \end{align} sigmoid 函数的导数： \begin{equation} g'(z) = g(z) (1-g(z)) \end{equation} 认为数据服从伯努利分布 \(y|x;\theta \thicksim Bernoulli(\phi)\) ，即数据的前置概率估计是伯努利分布。 TP – 将正类预测为正类的个数；true positive FN – 将正类预测为负类的个数；false negative TN – 将负类预测为负类的个数；true negative FP – 将负类预测为正类的个数；false positive 准确率、召回率、以及两者的调和均值（准确率和召回率都高的时候也会高）： \begin{gather} P = \frac{ TP }{ TP + FP } \\ R = \frac{ TP }{ TP + FN } \\ \frac{2}{F_1} = \frac{1}{P} + \frac{1}{R} \\ F_1 = \frac{2TP}{2TP + FP + FN} \end{gather} Perception Learning Algorithm sigmoid 函数并没有直接输出样本的类别标记，而是根据输出再与 0....</p></div><footer class=entry-footer>25 min&nbsp;·&nbsp;5289 words&nbsp;·&nbsp;Kyle Three Stones</footer><a class=entry-link aria-label="post link to Machine Learning" href=https://kylestones.github.io/hugo-blog/blog/machinelearning/machine-learning/></a></article></main><footer class=footer><span>&copy; 2022 <a href=https://kylestones.github.io/hugo-blog>Org Mode</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z" /></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>