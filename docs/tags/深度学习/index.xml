<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>深度学习 on Org Mode</title>
    <link>https://kylestones.github.io/hugo-blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on Org Mode</description>
    <image>
      <url>https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://kylestones.github.io/hugo-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://kylestones.github.io/hugo-blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AutoML</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/automl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/automl/</guid>
      <description>就像使用 CNN 提取特征来代替人工设计的特征，使用 AutoML 代替人设计网络的架构，应该是可以达到更好的效果。 前提   CNN 通常需要大量的时间来设计网络的架构。当然我们可以使用迁移学习，但是 只有针对数据集设计自己的网络才能达到最好的性能 。然而设计网络架构需要专业的技能，且具有很大的挑战（对于商业应用来说代价太大）。 NAS   Neural Architecture Search 就是用于搜索最优网络架构的算法。提供了深度学习的一个新的研究方向。    定义候选 building blocks    使用一个 RNN 作为控制器，用于选择拼装 building blocks    在交叉验证集上，训练拼装好的网络，使其收敛    根据网络的准确率来更新 RNN 控制器（update with policy gradient），希望其能选择出更好的网络架构    In simple terms: have an algorithm grab diierent blocks and put those blocks together to make a network. Train and test out that network.</description>
    </item>
    
    <item>
      <title>convolution</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/convolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/convolution/</guid>
      <description>cs231n   斯坦福大学的课程 CS231n: Convolutional Neural Networks for Visual Recognition 。发现写的很好，后悔没有早点看。可惜只看了 卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。 卷积   卷积神经网络明确假设输入是 images ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网 络采用全连接，用于图像中会产生太多的参数，导致 overfitting 。卷积神经网络的 layers 拥有神经元的维数为 (width,height,channels) 。  A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.  卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution operation essentially performs dot products between the filters and local regions of the input.</description>
    </item>
    
    <item>
      <title>cuda 编程基础</title>
      <link>https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/cuda/cuda-basic/</guid>
      <description>学习周斌老师的《NVIDIA CUDA 初级教程视频》笔记 重看 CPU   CPU 芯片中大量的晶体管用于 cache3 ，可以看到芯片中很大一部分面积都是 cache3 。主要由于 CPU 始终在大量的移动数据，将硬盘 中的数据移动到内存，将内存中的数据缓存到 cache ，将 cache 中的数据移动到寄存器，将某个寄存器移动到另一个寄存器，还有反向 的操作，将数据再写回硬盘，这些都是在大量的移动数据。CPU 主要是在为串行做优化，为了高速的数据移动，设计了内存管理单元、占 用很大芯片面积的 cache3 （或许是可以称 CPU 为吞吐机的原因），为了快速执行命令设计了各种分支预测、流水线、乱序执行等等。 整体架构     CPU 可以分成 数据通道 和 控制逻辑 两个部分。    Fetch 取址 -&amp;gt; Decode 译码 -&amp;gt; Execute 执行 -&amp;gt; Memory 访存 -&amp;gt; Writeback 写回   Pipeline   流水线是指令级并行 ILP 。可以极大的减小时钟周期，但同时也可能存在延迟（啥？），会增大芯片面积，指令流前后有关系时，无法 很好的利用流水线。流水线的长度称为级，并不是越大越好，通常在 14-20 级，不公开，涉及到芯片设计的商业秘密。 Bypassing   不等待前一条指令的所有流水线执行完成，只需要得到前一条指令结果的某一个数据，直接通过一个特殊的通道传递。 Branches     分支预测 Branch Prediction ，猜测下一条指令 ，基于过去分支记录，通常准确率大于 90%    分支断定 Branch Predication ，不使用分支预测，同时执行所有的分支。可减小面积、减小错误预测   IPC   instructions per cycle ；超标量 superscalar ，增加流水线的宽度， N 条流水线。从而提高 IPC 指令调度     Read-After-Write - RAW    Write-After-Read - WAR    Write-After-Write - WAW    寄存器重命名来改善 Out-of-Order OoO   乱序执行，重排指令，获得最大的吞吐率。可以重排缓冲区或发射队列/调度器 存储器层次架构   寄存器 - L1 - L2 - L3 - 主存 - 硬盘   硬件管理    L1 , L2, L3   软件管理    主存 , 硬盘    缓存： 将数据放在尽可能接近的位置；利用时间/空间的临近性。    分区 Banking ，避免多端口    一致性 coherency    控制器 Memory Controller ，多个通道，增加带宽    编址方式    Shared Memory    Distributed Memory    Hybrid Distributed-shared Memory   CPU 内部并行性    指令级并行 Instruction-Level extraction    超标量、乱序执行   数据并行 Data-Level Parallelism    矢量计算，单指令多数据 Single Instruction Multiple Data (SIMD) ; 执行单元 ALU 很宽， 寄存器很宽   线程级并行 Thread-Level Parallelism    同步多线程 Simultaneous Multithreading SMT ，多核 Multicore   Multicore    真多核    除最后一级缓存外，不共享其他资源；   假多核    可能只是多个 ALU   锁存     多个线程读写同一款数据：加锁；    谁的数据是正确的 ： 缓存一致性协议 Cohernce    什么样的数据是正确的 Consistency ： 存储器同一性模型   Powerwall   由于能量墙的限制，导致摩尔定律无法保持。  新摩尔定律：多核、单核性能并不会大幅度提升，频率也基本不会有大的提升；  处理器的存储器带宽无法满足处理能力的提升。 Flynn 矩阵     SISD SIMD   MISD NIND    名词不解释     Task 任务    Parallel Task 并行任务    Serial Execution 串行执行    Parallel Execution    Shared Memory 共享存储    Distributed Memory 分布式存储    Communication 通信    Synchronization 同步 –&amp;gt; 破坏了独立性、并行性    Granularity 粒度 –&amp;gt; 任务划分的粒度    Observed Speedup –&amp;gt; 加速比    Parallel Overhead 并行开销 –&amp;gt; 通信、同步    Scalability 可扩展性 –&amp;gt; GPU 从 4 核到 400 核时，性能上的提升   并行编程模型     共享存储模型 shared memory model    线程模型 threads model    消息传递模型 message passing model    数据并行模型 Data Parallel Model –&amp;gt; 对数据切分    OpenMP , MPI , SPMD , MPMD Amdahl&amp;#39;s Law   程序可能的加速比取决于可以被并行化的部分： speedup = 1/(1-p)  设计并行处理程序和系统 GPU 设计思路     去掉复杂的分支预测、乱序执行、内存管理等单元    设计加入多个核（多个 SM）    在一个核内增加 ALU 的宽度；即一个核内有多个 ALU ，ALU 被分成多个组，每个组内的 ALU 共享管理调度单元（指令流）    提供较大的上下文存储空间（pool of context storage），使得大量独立片元切换来掩藏延迟。每个 SM 上可以驻扎远多于 SP 个数 的线程    最终设计结果：一个 GPU 有多个 Streaming Multi-processor [SM] ，每个 SM 内有多个 Streaming Processor [SP] ，也就是 cuda core ，是最小的计算单元 ALU 。每个 SM 内的 cuda core 被分成多个组，有多个调用单元用于调度，一个调度单元同一时间可以调度 一个组，使得一个组内所有线程执行相同的指令，但读取不同的数据。同时一个 SM 内有一定数量的寄存器、共享存储空间、上下文存储 空间，动态分配给需要的单元。  具体编程时，并不需要关心具体的硬件结构， cuda 将线程设计了三级逻辑抽象： grid - block - thread 。并不与硬件结构一一对应。    一个 Grid 内，每个 Block 的线程数是一样的    Block 内每个线程可以 synchronize 同步；    Block 内每个线程都可以访问 shared memory ；    每个 Block 内最多的线程数有一定的限制（不同的芯片会不一样）；    一个 Block 内的所有线程必须位于同一个 SM 中 ；    Block 之间彼此相互独立执行，以任意顺序、任意调度；在运行期确定在哪个 SM 上调度，可扩展    GPU 适用于密集计算，高度并行的计算；其片上晶体管主要用于执行计算，而不是缓存数据或控制指令流。  GPU 带宽是非常宝贵的资源，应该尽量减少带宽请求次数，重复数据尽量只取一次。让 GPU 多做运算。GPU 显存很大，相对内存来说， 带宽也很大，但在片外（不再 GPU 芯片内，在显卡的板子上），但芯片内部局部存储较小，缓存较小。 # CPU GPU 协同方式；好难对齐呀！！！ 主存 显存 DRAM GDRAM | | CPU GPU | | I/O  I/O PCIE   SSE 显示向量运算指令；但 SIMD 处理并不总是需要显示 SIMD 指令，NVIDIA GPU 标量指令，但硬件进行矢量化，是 SIMT （单指令多 任务）  GPU 架构决定，编写 GPU 代码的时候需要注意    尽量少用递归，至少不鼓励使用递归，尤其是很深层次的递归    不要使用静态变量    少用 malloc 函数，因为成千上万个线程都执行 malloc ，可能导致显存很快用尽，同时也会影响性能（猜测）    小心通过指针实现函数调用（注意区分设备侧和主机侧地址）   GPU 内存模型    寄存器    片上，快速，可读写；线程专用。每个 SM 上有一定数量的寄存器，如 G80 ，每个 SM 有 8K 个寄存器   Local Memory    在片外的 Global Memory 中，每个线程私有。可存储稍微大一些的数据，一般用于存储自动变量数组，通过常量索 引访问；新的 GPU 有 cache   Shared Memory    片上，全速随机访问，可读写； block 内共享。和 cache 在同一个层次，可理解为用户可编程的 cache 。每个 SM 内有固定大小的共享存储器，如 G80 中，一个 SM 有 16 KB shared memory ；需要注意 bank conflict   Global Memory    片外，长延时（100 个时钟周期），可读写，随机访问性能差；带宽较大 300GB/s ，有 cache ；Host 可读写   Constant Memory    在 Global 中特定位置，即固定的地址，短延迟、高带宽、所有线程只读，容量较小，cache ；Host 可读写      存储器 编程声明 作用域 生命期     register 编译器管理，必须是单独的自动变量而不能是数组 thread kernel   local 编译器管理，自动变量数组 thread kernel   shared _ shared_ int sharedVar block kernel   global _ device_ int globalVar grid application   constant _ constant_ int constantVar grid application    线程调度   cuda 通过分级管理线程 Grid - Block - warp - Thread 。 逻辑 Block 可以想象对应硬件的 SM 。  warp 是 Block 内线程编号连续的 32 个线程， 是线程调度的最小单元 。warp 运行在一个 SM 中，threadIdx 值连续，硬件设计上 保证 warp 内的每个线程同步。  特征：    在硬件上，warp 的调度是 0 开销的（所有 warp 的上下文已经存储在硬件中）    同一时间，一个 SM 上只有一个 warp 在执行（不是一个 SM 上有多个调度器吗？多个调度器应该是可以同时执行多个 warp 的呀？ TODO）    warp 内所有线程始终执行相同的指令     divergent warp    由于没有为每个线程设计一个调度器（会占用额外的芯片面积），一个 warp 共享一个调度器，所以所有的线程必 须执行相同的命令。如果有条件分支，那么 warp 内的所有线程都会执行所有的分支，只是线程在不需要的分支时 候不操作任何寄存器。warp 内分支发散 GPU 做了一些优化，不太不要重点关注。但需要考虑好的算法数据分割， 使得warp 能尽早完工，释放资源，从而更好的利用资源，如并行规约 Parallel Reduction 的一个简单情况，求 一个数组的和，算法每次执行两个数的相加，如果始终让相邻的两个数相加，那么 warp 将无法很好的被利用，修 改成让两个相差数组长度一半索引的两个数相加，之后的 warp 就可以较早的被释放。    问： 每个 warp 有 32 个线程，但每个 SM 中只有 8 个 SP ，这时候如何调度呢？  答： 将 warp 分成四组，第一个时钟周期前 8 个线程执行一条命令，第二个时钟周期，随后的 8 个线程执行相同的命令，直到第四个 时钟周期，所有 32 个线程都执行完某一条命令，然后再执行下一条命令。即一条指令分成四次调度才能执行完成。当然这是很老的 GPU 才有的现象，现代 GPU 一个 SM 内 SP 的个数很多，不再需要分多次才能执行完一个 warp 。不过这种逻辑思想是一样的。    架构 SM 中 SP 的数量     开普勒 192   mashival 128   Fermi 32     问： 假设一个 kernel 包含 1 次 global memory 的读操作，需要 200 个 cycles ，和 4 次独立的 multiples/add 操作，需要多少 warp 才能隐藏内存延迟？  答： 4 次乘/加操作，每次乘/加需要 4 个 cycles ，共需要 16 个 cycles ； 200/16 并向上取整得到 13 个 warp 。 TODO！@#￥%  线程同步可能导致 死锁 ，需要注意逻辑正确性 # 下面代码将导致死锁 if (condition) { .</description>
    </item>
    
    <item>
      <title>Darknet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/darknet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/darknet/</guid>
      <description>   darknet 以 layer 为中心，通过构建不同的 layer 构成一个 net ；    所有的层保存在 net.layers 中 net-&amp;gt;layers = calloc(net-&amp;gt;n, sizeof(layer));    darknet 需要一个“.txt”文件，每行表示一张图像的信息： &amp;lt;object-class&amp;gt; &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;width&amp;gt; &amp;lt;height&amp;gt;    每个单元格会预测B个边界框（bounding box）以及边界框的置信度（confidence score）。    所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 Pr(object)，后者记为 预测框与实际框（ground truth）的 IOU 。很多人可能将 Yolo 的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的 乘积，预测框的准确度也反映在里面。    中心坐标的预测值 (x,y) 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的。而边界框的 w 和 h 预测值 是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 [0,1] 范围。通过 sigmoid 函数保证 (x,y) 在 0-1 之间， 这样，每个边界框的预测值实际上包含5个元素：(x,y,w,h,c)，其中前4个表征边界框的大小与位置，而最后一个值是置信度。    加速库： NNPACK </description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/deeplearning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/deeplearning/</guid>
      <description>神经网络和深度学习  神经网络概论     结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等    非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字    人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣 钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也 可以处理非结构化的数据。  每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会人为决定每个神经元 的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神 经网络在监督学习中效果很好很强大。  神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以 及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的 算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法]  scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神 经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好 的 SVM 算法结果要优于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在达到一定的性能之后效果变无 法继续提升，而神经网络此时的效果将明显领先于传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算法 都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速度， 因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，从 而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高（ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值）。速度的提升使得我们可以训练大型的网络或者在一定的时间内完成网络 的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断循环，迭代的更快使得验证自己的想法更加快速得到 验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。  1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任 意m 维到 n 维的映射）。虽然如此，但是若要模拟复杂的函数可能需要特别特别多的隐层神经元，因此现代网络总是加大网络的深度， 以让每一层的函数尽量简单，而整个网络完成复杂的映射。 神经网络基础   一张彩色图像像素点由 RGB 三个通道组成，作为神经网络的输入时，将三个矩阵都转换成向量并拼接起来组成一个列向量 \(x^{(i)} \in \mathbb{R}^{n_x}\)，列向量中先是红色通道的所有像素点，然后是绿色通道的所有像素点，最后是蓝色通道的所有像素点。m 个训 练样本 \(\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(i)},y^{(i)}) \}\) ；同时使用 \(X \in \mathbb{R}^{n_x \times m} \) 表示所有的训练样本\[X= \left[ \begin{array}{cccc} | &amp;amp; | &amp;amp; &amp;amp; | \\ x^{(1)} &amp;amp; x^{(2)} &amp;amp; \cdots &amp;amp; x^{(m)} \\ | &amp;amp; | &amp;amp; &amp;amp; | \end{array} \right] \] 相比于让每个样本按行向量堆叠，在神经网络中构建过程会简单很多。\(y^{(i)} \in \{0,1\}\)同 时将所有的标签组成一个行向量 \(Y \in \mathbb{R}^{1 \times m}\) \[ Y = [ y^{(1)}, y^{(2)}, \cdots, y^{(m)} ]\] 在Python 中使用 (n,m) = X.</description>
    </item>
    
    <item>
      <title>fast.ai</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/fast-ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/fast-ai/</guid>
      <description>调参  分阶段使用多个学习速率   越接近输入的层，在 fine-tune 的时候，参数需要调节的越小，越靠近输出层，参数需要调节的越大。因此在 fine-tune 的时候，不同 的层最好使用不同的学习速率。例如 1-2 层使用 0.001 ，3-4 层使用 0.01 ，5-6 层使用 0.1 等等，越远离输出层，逐渐让学习速率 缩小 10 倍。[ Once the last layers are producing good results, we implement differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last.</description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/r-cnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/r-cnn/</guid>
      <description>预处理     减去 RGB 像素的均值（无论训练还是测试统一使用训练样本集的均值）    Rescale 在图像的长边不超过阈值的情况下，将短边 resize 到指定值   def img_rescale(img, targetSize=600, maxSize=1000): w = img.width h = img.height minDim = min(w,h) maxDim = max(w,h) scale = targetSize / minDim if scale * maxDim &amp;gt; maxSize: scale = maxSize / maxDim img = rescale(img, scale) return img   为什么要这样 rescale 呢？这样仅仅只能让多数的图像的短边统一长度，长边的长度可能各不相同；另外一些图像长边都是最大值，但 是短边各不相同。这样做有什么意义呢？保持图像的横纵比？但是图像大小不一样，要怎样去训练呢？ RoI Pooling   RPN 生成的 RoI 由 (r,c,h,w) 表示，其中 (r,c) 是左上角的坐标，(h,w) 是高和宽。  RoI max pooling works by dividing the h*w RoI window into an H*W grid of sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window into the corresponding output grid cell.</description>
    </item>
    
    <item>
      <title>mAP</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/map/</guid>
      <description>词语解释   mAP 是 Mean Average Precision 的缩写，是检测算法的评价指标。这个名词中包含两个平均，其中 Mean 取的是不同类别的平均值， Average 取的是不同的召回率的平均值，当然计算的是正确率 Precision 的平均值。另外 coco 数据集还取了不同 IoU 阈值的平均。  另外想说一下，recall 这个单词被国内广泛翻译称召回率，大多数人根本无法从字面理解这个召回率到底是个什么鬼。而周志华老师在 其西瓜书中，将 recall 翻译成查全率，将 precision 翻译成查准率。一下子直观了很多。查准率表示模型查找到结果的准确率，就是 你说这些都是好瓜，但其中真正是好瓜的比例；查全率表示模型找到所有正样本的比例，就是说在所有好瓜中，你判别出来了多少。 计算方法   不同的数据集有不同的 mAP 计算方法。主要包括 PASCAL 07 、PASCAL 10、COCO 数据集的计算方法较常用。  PASCAL 数据集使用的都是固定的 IoU 阈值（默认为 0.5），就是只要预测的 box 和真实的 box 的 IoU 大于等于 0.5 ，就认为检测正 确（当然类别也必须正确）。所不同的是 PASCAL 07 只计算 11 个查准率 precision 的平均值，而 PASCAL 10 则要求所有的检测结果 都用于计算 AP 。  PASCAL 07 只计算查全率 recall 在 0.</description>
    </item>
    
    <item>
      <title>Mask R-CNN</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/maskrcnn/</guid>
      <description>训练 train  输入  mini-batch   论文指出每个 GPU 上面跑 2 张图片，共 8 个 GPU ，所以 batch-size = 16 TODO ；每张图片上 sample 256 个 anchor 用于训练。 image spatial   图片大小  到底是用多大尺寸的图片训练网络的？ paper 中将图像的短边 resize 为 800 （测试的时候也会 resize 吗？）; 800*1024 ? 800*800 ? 1024*1024 ? 任意 size ? TODO anchor   anchor 用于生产目标的最初候选区域  生成 anchor 的方法    anchor scale [32, 64, 128, 256, 512]    anchor ratio [0.</description>
    </item>
    
    <item>
      <title>mxnet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/gluon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/gluon/</guid>
      <description>mxnet 常用包  from mxnet import gluon # 提供简单易用的 mxnet 接口 from mxnet import nd from mxnet import init # 用于权重参数初始化 from mxnet import autograd # 自动求导 from mxnet.gluon import nn # 用于构建网络结构 from mxnet.gluon import data as gdata from mxnet.gluon.data.vision import transforms # 用于变换数据 import sys import time  常用函数    transfroms.Compose    实现数据格式的转换，转换成 (height, width, channel) 格式，以及变成浮点数；这是 mxnet 要求的格式； 同时可以实现 argument   gluon.</description>
    </item>
    
    <item>
      <title>something</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/summarize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/summarize/</guid>
      <description>https://pan.baidu.com/s/1hs3Z2ao https://www.aliyun.com/zixun/wenji/1283158.html https://www.cnblogs.com/objectDetect/p/5947169.html https://www.zhihu.com/question/43609045/answer/132235276 https://blog.csdn.net/shentanyue/article/details/82109580?utm_source=blogxgwz1 https://blog.csdn.net/qq_33783896/article/details/80675398 网络架构     解释resnet、优缺点以及适用范围    解释inception net、优缺点以及适用范围    densenet结构优缺点以及应用场景    dilated conv优缺点以及应用场景    moblenet、shufflenet的结构   卷积核参数   一个标准卷积层参数的个数是 input*ksize*ksize*output ，不知道为什么自己会两次犯了相同错却仍然不自知。  如果卷积分组 group ，那么每个卷积核的大小将变小，变为 imput/group*ksize*ksize ，一个 group 的参数为 input/group*ksize*ksize*output/group ；所有 group 组成整个卷积层的参数，input/group*ksize*ksize*output/group*group = input/group*ksize*ksize*output 。即参数会变为原来的 1/group 倍。  deepwith conv 对输入的每一个 channel 分别进行独立的卷积操作，此时输入的 channel 个数必然等于输出 channel 的个数，此卷积 层参数的个数为 ksize*ksize*input。 感受野   vgg 论文上说明两次 3x3 卷积可以达到 5x5 卷积核的效果、三层 3x3 卷积可以达到 7x7 卷积核的效果； 现在终于理解作者的意思， 使用两次 3x3 卷积，第二层卷积核的感受野就是 5x5 ，而三层 3x3 卷积，第三层卷积的感受野为 7x7 。  感受野计算公式： Dilated conv vs Deconvolution     Dilated convolution 在卷积核的每两个值中间插入 d-1 个空洞    Deconvolution 在 feature map 上插入像素值为 0 的点    Unlike dilated convolutions, which have same output size as input size (if input borders are properly padded) &amp;#34;deconvolution&amp;#34; layers actually produce upsampling (larger input then output)    数学公式：  \begin{align*} Z &amp;amp;= W ⋅ A dW &amp;amp;= dZ ⋅ A dA &amp;amp;= W^T ⋅ dZ \end{align*} Dilated convolution   The goal of this layer is to increase the size of the receptive field (input activations that are used to compute a given output) without using downsampling (in order to preserve local information).</description>
    </item>
    
    <item>
      <title>TensorFlow</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/tensorflow/</guid>
      <description>架构   阅读大神的 《TensorFlow 内核剖析》 对 TensorFlow 的整个代码框架有了一些了解，以下是读书笔记。  Graph (计算图)是 TensorFlow 领域模型的核心。计算图就是节点与边的集合，是一个 DAG (有向无环图)图。  Node(节点)持有零条或多条输入/输出的边，分别使用 in_edges， out_edges 表示。  Edge(边) 持有前驱节点与后驱节点，从而实现了计算图的连接，也是计算图前向遍历，后向遍历的衔接点。边上的数据以 Tensor 的形 式传递。计算图中存在两类边：    普通边：用于承载 Tensor，常用实线表示；    控制依赖：控制节点的执行顺序，常用虚线表示。    TensorFlow 计算的单位是 OP，它表示了某种抽象计算。通过定义 OP 来构建 DAG 图。OP 拥有 0 个或多个「输入/输出」，及其 0 个 或多个「属性」。其中，输入/输出以Tensor 的形式存在。在系统实现中，OP 的元数据使用 Protobuf 格式的 OpDef 描述，实现前端与 后端的数据交换，及其领域模型的统一。OpDef 定义包括 OP 的名字，输入输出列表，属性列表，优化选项等。其中，属性常常用于描述 输入/输出的类型，大小，默认值，约束，及OP 的其他特性。  计算图的执行过程将按照 DAG 的拓扑排序，依次启动 OP 的运算。其中，如果存在多个入度为 0 的节点，TensorFlow 运行时可以实现 并发，同时执行多个 OP 的运算，提高执行效率。 架构设计   TensorFlow 遵循良好的分层架构：    front end ： 用户接口，负责构造计算图    runtime ： 实现计算图的拆分。提供本地运行模式和分布式运行模式，两者共享大部分设计和实现    计算层 ： 基于 Eigen 实现计算的逻辑实现；同时支持各种硬件的并行加速    通信层 ： 基于 gRPC 实现组件间的数据交换。同时支持 RDMA    设备层 ： 支持多种异构计算设备。实际执行计算的载体 前端系统     Client 是前端系统的主要组成部分，它是一个支持多语言的编程环境，且对 Python 和 C++ 的支持比较完善。实现时通过 Swig 完成对 后端 C++ 的调用。基于这些编程接口来构造计算图。  此时，TensorFlow 并未执行任何的图计算，直至与后台计算引擎建立 Session，并以 Session 为桥梁，建立 Client 与 Master 之间的 通道，并将 Protobuf 格式的 GraphDef 序列化后传递给 Master，启动计算图的执行过程。 runtime  master   在分布式的运行时环境中，Client 执行 Session.</description>
    </item>
    
    <item>
      <title>ubuntu18.04 install gpu mxnet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/gpu-mxnet-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/gpu-mxnet-install/</guid>
      <description>安装 Ubuntu18.04     从 中科大源 下载系统镜像，并制作 u 盘启动盘   # 使用 wget 下载 Ubuntu18.04 $ wget http://mirrors.aliyun.com/ubuntu-releases/bionic/ubuntu-18.04.1-desktop-amd64.iso # fdisk 查看 u 盘分区 $ sudo fdisk -l # 使用 dd 命令制作 u 盘启动镜像 $ dd if=ubuntu18.04.1-desktop-amd64.iso of=/dev/sdb     启动电脑，修改为 U 盘启动    关闭 uefi 安全检查，有秘钥的通通删掉    使用 U 盘启动后，选择安装 Ubuntu    手动分区，建议只有两个分区，一个根分区，一个 home 分区；这样在需要重装系统的时候可以格式化根分区，同时保留 home 分区 的内容    安装完成后，修改 root 的密码，从而可以使用 root 登录系统   安装 NVIDIA 驱动   硬件配置    GPU GTX2070       由于 GPU 较新，需要添加 PPA；如非必要可以直接使用 Ubuntu repository    查看驱动列表    安装驱动    重启使驱动生效   # Ubuntu repository 的驱动虽然比较旧，但是更加稳定，bug 较少； # 如果想要安装最先的版本，可以添加由 Ubuntu 团队维护的 PPA # PPA 仍然在测试，可能存在某些依赖问题 # 不需要手动更新，Ubuntu18.</description>
    </item>
    
    <item>
      <title>VGG GoogLeNet ResNet</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/vgg-googlenet-resnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/vgg-googlenet-resnet/</guid>
      <description>VGG   VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION    论文表明增加网络的深度可能提高网络的性能，论文成功将网络的深度推到 16-19 层    使用 small size (3x3) filter ：两层 3x3 的卷积与 5x5 的卷积等效，三层 3x3 的卷积与 7x7 的卷积等效；选用小尺寸的 filter 可以减小参数    第一层卷积的滤波器的个数是 64 ，之后每经过一个 max-pooling 层都将滤波器的个数乘 2，直到最大为 512 之后不再继续翻倍    Not all the conv layer are followed by max-pooling.    论文为了训练 19 层深层网络，先构建了一些浅层的网络，用于预训练，然后逐渐使用预训练好的浅层网络参数对深层的网络进行初始化。 不过文章中作者也指出，写完 paper 后发现，使用随机初始化是不需要预训练的。看来大神们发 paper 也是历经坎坷呀。 GoogLeNet   强调算法的重要性，比硬件、大的数据量更加重要。Most of the progress is not just the result of more powerful hardware, larger dataset and bigger models, but mainly a consequence of new ideas, algorithms and improved network architechtures.</description>
    </item>
    
    <item>
      <title>YOLO 实现细节</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/yolo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/yolo/</guid>
      <description>之前写的乱七八糟，现在开始重构，主要针对 YOLOv3。真的是得亲自动手实现，才能真正了解一个算法。The best way to go about learning object detection is to implement the algorithms by yourself, from scratch. – Ayoosh Kathuria 主网络   主网络采用 Darknet53 ，网络为全卷积网络 FCN(Fully Convolutional Networks)，之前一直以为全卷积网络就是不包含全连接的网络， 现在才知道，全卷积网络连 Pooling 层都没有，Pooling 层使用步长为 2 的卷积代替，防止由于 Pooling 导致 low-level features 的丢失；分类之前使用 Global Average Pooling 。[Darknet53 中有全连接层呀，上面写错了] 卷积层   Darknet 的每个卷积层都是由 Conv-BN-LReLU 组成，是网络的最小重复单元。 def cbl_gen(channels, kernel_size, strides, padding): &amp;#39;&amp;#39;&amp;#39;conv-BN-LeakyReLU cell&amp;#39;&amp;#39;&amp;#39; cbl_unit = nn.HybridSequential() # 所有卷积后面都有 BN ，所以 bias 始终为 False cbl_unit.</description>
    </item>
    
    <item>
      <title>乱想</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/guess/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/guess/</guid>
      <description> 目标检测     loss 改进。类似人脸识别，通过改进 loss 来提高 map ； FocalLoss 也算是一种改进    微软的 Deformable-Convolutional 应该大有用处，现在功力没有很好的发挥出来    神经网络对细节很敏感，通过在图片上粘贴一些胶带就可以成功的误导网络。是不是说明网络没有很好的区分背景？或者说背景没有 达到随机的效果？将目标完全分割出来（没有背景）用于训练会怎样？感觉人类在观察事物的时候，第一步就是先对目标进行了分割 呀？   网络参数剪枝     先训练一个大型的网络，然后裁剪成一个小一点的网络，其性能比直接训练的同等大小的网络效果要好很多。说明现在的网络训练方 法还是有很大的问题。 人类做梦是在进行无监督学习吗？ 神经网络的训练能否设计成监督训练和无监督训练相结合的方法？   AutoML   人工设计的特征远远没有网络自己提取的特征好，人工设计的网络结构也很有可能没有自己学习的结构效果好。每个人的大脑内的神经元 也应该是各不相同，我们只是被告知什么是猫、什么是鸟，而我们的神经元会自己学习怎样链接。 婚礼现场生成     一个人的一系列帧，可以插入视频某起始帧的某个地方    可以换脸，让自己喜欢的明星来参加自己的婚礼   边缘计算   软件硬化：高通 CMDA 、思科路由器、地平线    专业话，继续推动摩尔定律。由计算变化成 AI    低功耗    软件和硬件联合优化    特殊场景的特殊问题，一定要有场景。Google X 成立了 6-7 年，但只是一地鸡毛。  做东西一定要解决实际问题 。  对话系统：对话一定是要有目的，而不是仅仅为了对话。对话是要解决实际问题的。  亚马逊老总左贝斯：将理想和现实分开，  把握住十年中不变的东西。  最终由 big data 转变到 big computer 的 发展历程   余凯：历史都是先是 toC ，然后再是 toB 。  英伟达股票大幅增长，英伟达是 toC 。 创业   创业就是赌，如果是实实在在的放在桌子上东西，那根本不是创业，那是上班。  看长线，踏踏实实做下去，更可能钓大鱼。  看短期（最近 2-3 个月）和远期（目标），不用看半年或者 1-2 年，因为到时候，你的这段时间的思考都会废掉。 </description>
    </item>
    
    <item>
      <title>人脸识别</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/facerecognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/facerecognition/</guid>
      <description>FaceNet   A Unified Embedding for Face Recognition and Clustering  原来使用卷积神经网络来提取人脸的特征通常都是使用 softmax-loss 来训练网络，以期望网络的到的 embedding 足够好。本文作者直 接使用 embedding 的误差来训练网络，然后通过计算 embedding 的欧式距离来实现人脸验证。 triplet loss   每次使用三张图像，一个是 anchor ，另外两张中一张图像与 anchor 是同一个人，另一张是不同的人。  \begin{align*} L = ∑_i^N ≤ft [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + α \right ]_+ \end{align*} Triplet Selection   为了较好的训练效果，挑选hard-positive 和 hard-negative 的人脸对，就是同一个人时选择两张差别最大的图像，不同人脸的时候， 挑选差别最小的两张图像。其中 \(\alpha\) 是 margin 。当然所有这些选择都是在一个 mini-batch 中，而不是整个训练样本中。  另外为了防止网络进入局部最优解或者训练崩溃（如 f(x) = 0），选择 semi-hard negative 样本，即满足 \[ ||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 \]  论文中有 online 和 offline 两种方式来选择。选择方法较麻烦 center loss   A Discriminative Feature Learning Approach for Deep Face Recognition  作者认为 Softmax 仅仅增加了类间离散度，并没有很好的减小类内离散度，所以作者对 softmaax-loss 进行了改造，增加了每一个样本 到中心点距离的惩罚项，强制让学习到的同一个人的 embedding 都簇拥到一起。Penalizes the distances between the deep features and their corresponding class centers.</description>
    </item>
    
    <item>
      <title>卷积神经网络进化</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/revolution/</guid>
      <description>总体趋势：选取的函数越来越简单，手工设计的部分越来越少 CNN   Yann Lecun 在 1998 的 LeNet 奠定了神经网络的基本架构 : CONV - POLING - FC 。 激活函数   在经典的神经网络以及 LeNet 中使用的激活函数都是 sigmoid 函数。sigmoid 函数是非线性函数，且在输入较大或者较小的时候斜率会 变得很小，不利于参数的学习。  从 AlexNet 开始，激活函数变成了 ReLU ，为分段线性，且 non-saturating，大大加快了网络的训练速度。同时为防止过拟合，提出了 Dropout 方法， Dropout 随机的使网络中的一些节点失活，使得节点不能过度依赖某一个输入，从而权重得以分散开来，另外使用随机 失活的网络，有预训练的效果，类似于先训练一个简单的网络，然后在没有失活的大型网络上 fine-tune 。虽然 AlexNet 网络与 LeNet 的架构基本相同，但由于其 ReLU 和 Dropout 等方法的使用，网络使用了 120 万张训练图片，从数据中学到了更本质的特征，将 cumulative match character (CMC) top5的正确率一下子提升了 10% ，成功掀起了深度学习的研究热潮。  何凯明大神在一次报告中使用 RevoLUtion 来表示 ReLU 对深度学习的贡献，同时使用红色字体高亮了单词中的 ReLU ，非常形象。 Network in NetWork   除了 mini-batch size 外，网络的一层的输入维度为 height * width * channels ，可以通过 polling 操作来减小 height 和 width ，但是怎样减少 channel 的个数呢？ 1 * 1 卷积可以大显身手。当然，如果你愿意也可以用来增加 channel 的个数。 DepthWise convolution   卷积时，并不再是同时对所有的通道进行卷积，而是分别对每个通道使用不同的滤波器进行卷积，得到同等数量的新的 feature maps， 然后使用 1x1 的卷积宽通道进行卷积操作。  不同的通道得到的是不同的特征，需要分别对每个通道进行单独的处理，所以使用 DW ，各个通道之间也可能有关联，所以最后进行 1x1 卷积综合各个通道的特征。  这样不仅大大减少了参数的个数，也加速了网络的计算速度。 网络架构    AlexNet    有大量的超参需要手工调节。 需要仔细设计了什么时候使用卷积层、什么时候使用池化层以及卷积核的大小   VGG    没有太多的超参。虽然有 16 个权重层，但总体结构并不复杂。固定卷积核大小为 3 * 3、步长为 1、same padding，池化层 2 * 2、步长为 2 ；网络的结构很规整：总是几个卷积层后接一个池化层、滤波器的个数不断更新为原来的 2 倍， 从而图像 的宽和高每次池化后都缩减到一半、每组卷积的通道数都增加一倍。   GoogLeNet    采用模块化结构。将 1 * 1 卷积、3 * 3 卷积、5 * 5 卷积、max pooling （需要 same padding ，且步长改为 1 使 得图像的高和宽保持不变）全部在一个网络层中使用，将每一种操作得到的结果堆叠起来得到网络的输出，让网络自己 决定这一层网络到底需要什么操作，而不人工指定该层就是卷积层或者池化层或者全连接层。   网络深度   最初使用 BP 算法的神经网络只有两层；LeNet 进化成了 7 层；AlexNet 8 层；VGG 飞升到 19 层；GoogLeNet 22 层；而 ResNet 使用 skip connection 直接进化到了 152 层，后来成功训练了 1000 层的网络。 Batch Normalization   为了让模型更加容易训练，通常会先将样本进行预处理，其中一个关键的预处理方法就是将样本进行归一化处理。归一化之后样本在不同 的维度分布更加合理，有利于加速模型训练。  为了让后一层网络更容易训练，Batch Normalization 让网络的每一层输出都进行归一化，显著减小了多层之间协调更新的问题，输入的 稳定使得网络的每一层可以单独训练。  很好的正则化方法，何凯明大神说可以替代其他所有的正则化方法。  Batch normalization 是优化深度神经网络中最激动人心的创新之一。另外并不希望所有网络层的输出都是0 均值、方差为 1 ，所以 batch norm 为每个节点增加了均值和方差两个参数来调节归一化结果的分布，这两个参数由网络学习得到。又由于增加了均值这个参数 使得节点原来的偏移参数 b 不再有意义，可以去掉。  可以有两种不同的使用方法：在求取激活函数之前进行归一化，然后再利用激活函数得到该层网络的输出；也可以先计算激活函数的输出， 然后再进行归一化。第一种方法较为常用。  \begin{align*} μ = \frac{1}{m} ∑_i Z[l](i) σ^2 = \frac{1}{m} ∑_i (Z[l](i) - μ)^2 Znorm[l](i) = \frac{Z[l](i) - μ}{\sqrt{σ^2+ε}} {\widetilde{Z}}[l](i) = γ Znorm[l](i) + β \end{align*}  使用 mini-batch 前向传播的时候在计算激活函数之前先使用 batch norm ，然后计算激活函数，继续传播；反向传播时使用和求取权重 参数 W 一样的方法来求取均值和方差参数 \(d\gamma, \ d\beta\) 。在卷积层之后使用需要计算所有 channel 的平均值。  batch norm 使得网络每一层的输出值都得到归一化，归一化到某个分布。这将减小前面层网络参数的变化对后面层权重的影响，因为不 论前面层如何变化，都始终服从某个固定的分布，当前面层的输入变化时，其输出变化不会很大，所以后面的网络层的输入不会变化很大， 从而前面输入的变化对后面层网络权重参数的训练的影响减小，类似 达到了让每层网络参数独立训练的效果 。另外 batch norm 还有 正则化的效果，由于使用 mini-batch 只是所有训练样本的一小部分，所以其均值和方法都含有一定的噪声，每次使用 mini-batch的样 本去训练网络，并用含有噪声的均值和方法去归一化每一层的输出，就类似于 Dropout 随机丢弃网络中神经元节点一样，达到了的正则 化的效果。  测试时一般一次只输入一个样本，而不是像训练时那样，每次使用 mini-batch size 数量的样本。需要使用训练样本来估计网络每一层 输出的均值和方差，并用于测试时使用。一般使用不同的 mini-batch 的各个层输出值的指数加权平均来估计  \begin{align*} {μmean}[l] &amp;amp; = β {μmean}[l] + (1-β) {μ}^{\{i\}[l]} {σmean}2[l] &amp;amp; = β {σmean}2[l] + (1-β) {σ}^{2\{i\}[l]} \end{align*}  疑问：这里求取平均值只是穿越了不同的 mini-batch ，那么不用关系 epoch 吗？是不是取最后一个 epoch 的所有 mini-batch 的平均 效果更好？感觉这个好像就是训练好网络之后，又重新将所有训练样本训练一般一样。吴恩达说两者的效果都不错。这里的平均值次数是 不是应该选的比较大一点？0.</description>
    </item>
    
    <item>
      <title>损失函数</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/loss-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/loss-function/</guid>
      <description>为了度量算法关于某个数据集的性能，我们需要损失函数。当算法希望生成比真实值一个较小的数字，那么损失函数中应该体现出现，较 大的输出比较小的输出有更大的惩罚。    Loss: Used to evaluate and diagnose model optimization only.    Metric: Used to evaluate and choose models in the context of the project. Mean Squared Error     MSE 是经常被使用的损失函数，易于理解，且表现很好。    take the difference between your predictions and the ground truth    square it    average it out across the whole dataset   def MSE(y_predicted, y): squared_error = (y_predicted, y) ** 2 sum_squared_error = np.</description>
    </item>
    
    <item>
      <title>目标检测</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/objectdetection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/objectdetection/</guid>
      <description>YOLO   非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且 license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子    Darknet is public domain.    Do whatever you want with it.    Stop emailing me about it!    YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is enough. – Mae West ，个人感觉作者可能有点故意让两者混淆。  区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目 标检测，所以其最主要的优点就是 速度 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。  而且此时是 R-CNN two-stage 方法盛行的时候，先找到一些候选区域（代替窗口扫描），然后只在这些候选区域上进行检测。多么好的 主意。一般人肯定都会在思考怎样更加快速有效的提取这些候选区域（Faster R-CNN），或者加快提取特征的速度（Fast R-CNN）。作者 居然可以完全另辟蹊径，直接进行检测输出 one-stage 完成，根本无需候选区域，的确大牛。由此也可以看到有些时候一些非常好的思 想同样会束缚住我们。  先放三张 YOLOv3 的检测结果：    算法   首先将图像分成 s*s grid cells ，在每个 grid cell 上都输出 B 个 bouding box ，每个 bouding box 都包含 5 个值 (tx,ty,tw,th,to) ，其中 (tx,ty) 表示目标的中心点，(tw,th) 表示宽度和高度，(to) 表示 bouding box 中有 object 的概率。另外 每个 grid cell 会输出 C 个不同类别的使用 Softmax 求解到的概率。这样每个 grid cell 输出向量的维数是 (B*5 + C) ，网络总的 输出向量维数是 (s*s * (B*5 +c)) 。目标的中心点落在哪个 grid cell 中，则由这个 grid cell 进行检测。  其中 (tx,ty) 是相对于 grid cell 左上角 (cx,cy) 的偏移 offset ，通过 sigmoid 函数使得 offset 在 0~1 之间（认为每个 grid cell 的宽度和高度都是 1 ）。而 (tw,th) 利用与 Anchor Boxes 的宽和高的相对关系来求取。当然 YOLOv1 中并没有使用 Anchor box， 这是在 YOLOv2 中的改进。相比于直接使用坐标，使用 offset 或者相对关系可以是模型更加稳定。因为通过随机初始化，网络需要很长 的训练时间才能找到目标的位置，而找到相对偏移量则会容易很多。  由于 sum-squared error 比较容易优化，作者使用均方误差计算检测误差。即计算 grid cell 的四个坐标值与 ground truth 值差值的 平方和。  另外由于大多数的 grid cell 中没有 object ，只有少数的 grid cell 中会有目标，所以作者为这两种不同的误差赋予了不同的权重， 以阻止预测值趋向于 0 。具体 \(\lambda_{coord}=5, \ \lambda_{noobj}=0.</description>
    </item>
    
    <item>
      <title>见招拆招</title>
      <link>https://kylestones.github.io/hugo-blog/blog/machinelearning/seethemove/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kylestones.github.io/hugo-blog/blog/machinelearning/seethemove/</guid>
      <description>机器学习并不是若干算法的堆积，熟练掌握了“十大算法”或“二十大算法”并不能让一切问题迎刃而解。所以不能将目光仅聚焦在具体算法 的推导和编程实现上。基本算法仅能呈现典型“套路”，而现实世界任务千变万化，以有限的套路应对无限的变化，焉有不败！所以务必掌 握算法背后的思想脉络，面对现实问题时，根据任务特点对现有套路进行改造融通。无论科研创新还是应用实践，皆以此为登堂入室之始。 — 周志华 &amp;lt;机器学习&amp;gt;  还有张三丰传授太极剑法给张无忌时，张无忌忘记了剑法具体的招式，仅记住了太极剑法的指导思想，从而根据敌人的招式使用相应的制 敌之策，达到见招拆招的目的。  Easy to learn. Hard to master. YOLO   下面举 YOLO 算法中的几个的例子    作者通过均方无法来计算预测的 binding boxes 和 ground truth 之间的误差，由于 loss function 中还包含分类错误的误差。而 由于大多数的 grid cell 都没有目标，所以不应该让有目标和没有目标的 grid cell 产生相同权重的误差，所以作者让目标位置误 差的权重 \(\lambda_{coord} = 5\) ，让没有目标的分类误差权重 \(\lambda_{noobj} = 0.5\) ，从而来平衡由于数量悬殊导致的 问题。    使用均方误差计算，size 比较大的目标相比于 size 比较小的目标产生更大的误差。所以作者使用开方之后的宽度和高度值相减然后 求平方。    作者使用 224x224 的图像在 ImageNet 上使用分类网络对检测网络进行预训练，同时作者想让检测网络输入的分辨率为 448x448 。 由于需要同时改变输入的尺寸以及网络的任务，作者先使用 448x448 的ImageNet 对网络进行 fine-tune，然后在使用检测误差进行 调优，以达到更好的效果。    作者想要使用 Anchor Boxes ，但是 R-CNN 一般都是人为设定其大小，这个是目标的先验，如果能有更好的先验，那么应该会有更好 的检测结果，所以作者使用 k-means 聚类方法来求取 Anchor Boxes 先验的大小。    k-means 算法一般使用欧式距离度量误差，而此处，作者真实关心的是两者的 IOU ，所以作者用 1 - IOU 作为损失函数。    可以发现作者始终在依据自己的实际需求，对算法进行了一些改进，而不是直接生搬硬套 。 Debug - Diagnostic   吴恩达老师也非常强调运用自己的聪明智慧去设计诊断方法。自己思考，自己想测试方法，去发现问题到底出现在哪里，而不要盲目的去 修改测试，白白浪费更多的时间。 比如一定要找到是算法的收敛性问题还是目标函数选择有问题？很多人在 \(J(\theta)\) 有问题的 时候，却始终在不断增加迭代次数。  花费在诊断上的时间通常在 1/3 - 1/2 之间，但这些时间是值得的。Error analysis and diagnostic also give insight into the problem.</description>
    </item>
    
  </channel>
</rss>
