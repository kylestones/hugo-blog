#+TITLE:          目标检测
#+AUTHOR:         Kyle Three Stones
#+DATE:           <2018-08-24 Fri 07:19>
#+EMAIL:          kyleemail@163.com
#+ATTR_HTML:      :width 100%
#+OPTIONS:        H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t f:t tex:t
#+TAGS:           目标检测, 深度学习
#+CATEGORIES:     深度学习


** YOLO

非常敬佩作者 Joseph Redmon ，没有使用开源框架，而是自己使用 C 和 cuda 另外写了一套框架 DarkNet ，并且将其开源。而且
license 写的非常有意思，有点狂放不羁，可能这就是大牛该有的样子

0. Darknet is public domain.
1. Do whatever you want with it.
2. Stop emailing me about it!

YOLO 是 You Only Look Once 的简写。当然 YOLO 很可能让人想起另一句话 You only live once, but if you do it right, once is
enough. -- Mae West ，个人感觉作者可能有点故意让两者混淆。

区别于 RNN 系列的文章，需要先查找 region proposals ，然后在之上进行目标检测。YOLO 只需要运行一遍卷积神经网络就可以完成目
标检测，所以其最主要的优点就是 *速度* 。而且 mAP (mean Average Precision) 随着算法的改进也表现的相当不错。

而且此时是 R-CNN two-stage 方法盛行的时候，先找到一些候选区域（代替窗口扫描），然后只在这些候选区域上进行检测。多么好的
主意。一般人肯定都会在思考怎样更加快速有效的提取这些候选区域（Faster R-CNN），或者加快提取特征的速度（Fast R-CNN）。作者
居然可以完全另辟蹊径，直接进行检测输出 one-stage 完成，根本无需候选区域，的确大牛。由此也可以看到有些时候一些非常好的思
想同样会束缚住我们。

先放三张 YOLOv3 的检测结果： 

[[./bottle.jpg]]

[[./person.jpg]]

[[./monkeyking.jpg]]


*** 算法

首先将图像分成 s*s grid cells ，在每个 grid cell 上都输出 B 个 bouding box ，每个 bouding box 都包含 5 个值
(tx,ty,tw,th,to) ，其中 (tx,ty) 表示目标的中心点，(tw,th) 表示宽度和高度，(to) 表示 bouding box 中有 object 的概率。另外
每个 grid cell 会输出 C 个不同类别的使用 Softmax 求解到的概率。这样每个 grid cell 输出向量的维数是 (B*5 + C) ，网络总的
输出向量维数是 (s*s * (B*5 +c)) 。目标的中心点落在哪个 grid cell 中，则由这个 grid cell 进行检测。

其中 (tx,ty) 是相对于 grid cell 左上角 (cx,cy) 的偏移 offset ，通过 sigmoid 函数使得 offset 在 0~1 之间（认为每个 grid
cell 的宽度和高度都是 1 ）。而 (tw,th) 利用与 Anchor Boxes 的宽和高的相对关系来求取。当然 YOLOv1 中并没有使用 Anchor box，
这是在 YOLOv2 中的改进。相比于直接使用坐标，使用 offset 或者相对关系可以是模型更加稳定。因为通过随机初始化，网络需要很长
的训练时间才能找到目标的位置，而找到相对偏移量则会容易很多。

由于 sum-squared error 比较容易优化，作者使用均方误差计算检测误差。即计算 grid cell 的四个坐标值与 ground truth 值差值的
平方和。

另外由于大多数的 grid cell 中没有 object ，只有少数的 grid cell 中会有目标，所以作者为这两种不同的误差赋予了不同的权重，
以阻止预测值趋向于 0 。具体 \(\lambda_{coord}=5, \ \lambda_{noobj}=0.5\) 。

同时由于均方误差会让较大的 bouding box 相对于较小的 bouding box 产生更多的误差，作者将宽和高开方之后再计算误差。

\begin{align*}
\lambda_{coord} \sum_{i=0}^{s^2} \sum_{j=0}^{B} {\mathit{1}}_{ij}^{obj} 
\left[ ( x_i - \hat{x}_i )^2 + ( y_i - \hat{y}_i )^2 + ( \sqrt{w_i} - \sqrt{ \hat{w}_i } )^2 
+ ( \sqrt{h_i} - \sqrt{ \hat{h}_i } )^2  \right]
\end{align*}

此时每个 grid cell 只能检测一个目标，如果多个目标则中心点在同一个 grid cell 则无法同时检测，且对较小的目标检测效果不太好。

将最后一个激活函数修改成线性激活函数，应该是去掉激活函数一样，使得 feature 有更大的可学习空间。


**** YOLOv2

作者借鉴吸收了许多他人的方法，结合自己的思考来提升算法。YOLO 有明显的定位错误和较低的 recall

+ Batch Normalization :: 大大加速了算法的收敛速度，且是一个很好的正则化的方法。此时作者已经放弃使用 Dropout ，只使用 BN
     来达到正则化的效果。
+ Anchor Boxes :: 为 bouding Boxes 引入先验。在训练样本上使用 k-means 聚类来找到 Bouding Boxes 的 good prior ，而不像
                  R-CNN中那样人为凭经验设定 Anchor Boxes 的大小。假如使用欧氏距离来计算 k-means 的误差，larger boxes 会
                  比 smaller boxes 产生更大的误差，而此时真正关心的是 prior 和 ground truth Boxes 的 IOU (intersection
                  over union) ，所以作者将距离度量的方法由欧式距离变为 \(d = 1 - IOU(box,centroid)\) ，作者最终选择了 5
                  个 anchor boxes ，是 tradeoff model complexity 和 high recall 的结果。使用 Anchor Boxes 后，不再是每个
                  grid cell 只检测一个目标，而是每个 Anchor Boxes 检测一个目标，也就需要为每一个 Anchor Boxes 输出所有类
                  别的概率。tx,ty 使用相对于 grid cell 左上角的 offset 而非坐标，tw,th 使用 box prior 的相对大小，使得模
                  型更加容易训练；另外作者移除了全连接层，使用 global average pooling 代替；去掉了一个 max-pooling 层使
                  得输出的 feature map 有更大的分辨率；grid cell 的个数也由 7*7 变成 13*13，作者将图像的输入调整为
                  416x416 ，经过网络或缩小 32 倍，变成 13x13 。奇数可以保证只有一个中心点，而较大的物体中心点一般在图像
                  的中心，有偶数个中心点则不太好归类。
+ Fine-Grained Feature :: YOLOv2 最终得到的 feature map 的大小是 13x13 ，为了在多个分辨率上进行检测，作者从 26x26
     feature map 串接到 13x13 feature map 。The passthrough layer concatenates the higher resolution features with the
     low resolution features by stacking adja-cent features into different channels instead of spatial lo-cations,
     similar to the identity mappings in ResNet. This turns the 26×26×512 feature map into a 13×13×2048 feature map,
     which can be concatenated with the original features. Our detector runs on top of this expanded feature map so that
     it has access to fine grained features. ResNet 中的 skip connect 是逐 channel 逐 element 想加的，这里是什么意思？
+ Multi-Scale Training :: 由于 YOLOv2 中只有卷积层和池化层（去掉了全连接层），所以网络可以接受任何维数的输入。作者使用间
     隔为 32 的从 320 到 608 {320,352,...,608} 这些不同分辨率的图像来训练网络。每 10 patches 随机选择输入图像的大小，强
     制网络在不同的分辨率上表现都不错。
+ High Resolution Classifier :: 当网络需要同时在输入图像的尺寸和目标（由分类变成检测）都改变的时候，逐一进行 fine tune
     。所有 state-of-art 的检测方法都会先使用 ImageNet 进行预训练，此时输入的大小为 224x224 ，先使用 448x448 的输入在
     ImageNet 上进行 fine tune ，运行 10 epochs 。然后在使用检测的代价函数去 fine tune 。
+ Darknet-19 :: 效仿 VGG 只使用 3x3 卷积，并且在 polling 之后将 channel 加倍；学习 NIN 在 3x3 卷积之间使用 1x1 conv 来压
                缩特征（减小 channel 的个数），Global average pooling 代替全连接；BN 加速训练与正则化。


**** YOLO9000

作者提出了一种检测和分类的联合训练方法。有检测 label 的样本用于训练检测的 Bouding Boxes ，而用于分类的样本可以扩充检测类
别的个数。

ImageNet 依据 WordNet 来标记，而 WordNet 是一个有向图，而不是树，因为同一个节点可能有两个父节点。作者将其改造成树
WordTree。首先将所有只有一条 path 的添加到树中，剩余的节点按照增加最少边数来添加。从根节点到某节点的 path 所有节点条件概
率的乘积即为该节点的分类概率，是一个 multi-label model。而且可以利用 WordTree 结合不同的数据集

另外使用相同级别的同义词为一个单位来计算 Softmax ，而不是所有的类别统一来计算 Softmax 。因为使用 Softmax 要求不同的类别
相互独立，而这里显示并不符合。

作者利用 WordTree 结合 COCO 和 ImageNet 组成训练样本来训练网络，由于 ImageNet 比 COCO 大很多，通过 oversampling 来使两者
的比例为 4:1 。构造了一个包含 9000 种类别的样本，此时每个 Anchor Boxes 都需要输出 9000 中类别的概率？？？

使用 detection image 样本训练时，使用 YOLOv2 损失函数来计算并反向传播，使用分类样本则值修正分类错误，修改的范围是这个类
别集其上层类别。


**** YOLOv3

+ objectness score :: 使用 logistic regression 来求取 objectness score 。先找到概率最大的 box prior ，然后抑制那些与该
     box IOU 大于一定值的其他 box 。
+ class prediction :: multilablel classification 使用独立的 logistic classifier 来分类，各个类别并不相互独立，而是相互有
     重叠。
+ prediction across scales :: 由于 high level feature maps 有更强的语义信息，而 low level feature maps 有更强的空间位置
     信息，结合两者可以更好的预测目标的位置。参考 feature pyramid network (FPN) ，使用网络多层的 feature map 来组成不同
     scale 的特征金字塔来检测。FPN代替原来的图像金字塔，使用卷积后的不同尺寸的 feature maps 组成特征金字塔。论文中用
     stage 表明 feature map size 相同的层，每个 stage 的最后一层 feature maps 用于生成特征金字塔。因为并不是直接使用这些
     特征 feature maps 组成特征金字塔，而是让顶层（靠近输出层）的 feature maps 进行上采样 upsample ，得到和下层 feature
     maps 相同的 size，下层的 feature maps 进行 1x1 卷积以减少 channel 的个数，然后逐元素相加来组成该层的 feature maps
     。由于要执行 element-wise 相加，作者让特征金字塔的每一层的 feature maps channel 固定为 256 个，首先将最顶层的
     feature maps 使用 1x1 卷积将 channel 的个数降低为 256 个，组成特征金字塔的顶层。下面层也都会先使用 1x1 卷积将
     channel 个数减小为 256 ，然后再与上层的特征逐元素相加得到新的一层。最终得到特征金字塔。在每一层上都独立进行目标检测。
+ Darknet-53 :: 借鉴 ResNet 和 vgg ，使用 shortcut connecttions 、3x3 CONV 、1x1 CONV 。
+ other standard stuff :: multi-scale training、lots of data augmentation、batch normalization 


** R-CNN

+ R-CNN :: 使用选择搜索 selective search 的方法得到很多可能含有目标的矩形框（region proposal）；然后将得到的不同大小的
           region proposal 统一 resize 到某个固定的大小，并送入卷积神经网络提取固定长度的语义特征；之后使用每个类别的
           SVM 分类器来识别目标的种类；并且利用 bounding-Box Regression 对 region proposal 进行调节，只是简单的学习四个
           参数来调节 x,y,w,h，以更好的匹配目标。
+ Fast R-CNN :: 仍然使用 selective search 来得到 region proposal ；利用一个卷积神经网络同时进行分类和 bounding box 回归，
                即使用卷积神经网络得到 feature maps，将 region proposal 对应到该 feature maps 上，然后将 feature maps 上
                的每一个 region proposal 输入两个全连接层，一个用来分类，另一个用来进行 bbox 回归。而且使用 RoI max
                pooling 的方法，将 feature maps 上的每个 region proposal 分割成 WxH 固定数量的 bins （每个 channel 独立
                进行，不改变 channel 的个数），每个 bins 内执行 max pooling 得到最大值，这样无论 feature maps 上的
                region proposal 尺寸的大小，都统一变成 WxH 后送入之后的全连接层。
+ Faster R-CNN :: 使用卷积神经网络来提取 region proposal 。作者设计 Region Proposal Network (RPN) ，先将任意尺寸的图片经
                  过一些卷积操作，然后在某层 feature maps 上使用固定大小的滑动窗口（文中使用 3x3 ）扫描 feature maps ，
                  每个窗口位置上提取固定长度的 feature （论文中提取 256d 的特征），然后将得到的所有特征经过两个全连接分
                  支，一个用于分类是前景还是背景，另一个用于输出 region proposal 的位置和大小。这就是 RPN 网络。当然由于
                  作者只使用固定的窗口而且只扫描一遍，为了得到较好的效果，作者提出了 anchor boxes 的概念，就是要让一个窗
                  口对应多个不同大小的矩形框。具体作者使用了 128^2, 256^2, 512^2 和 1:1, 1:2, 2:1 组合成的 k=9 种不同大
                  小的 anchor boxes （作者将输入图像都 rescale 到 1000*600，这些 anchor boxes 对应的都是输入图像上的矩形
                  框）。The design of multi-scale anchors is a key component for sharing features without extra cost for
                  addressing scales. 这样 RPN 中的每个窗口 ： 都对应了 k 个anchor boxes ，分类分支生成 2k 个输出，回归分
                  支生成 4k 个输出。根据卷积层尺寸的不同最终得到输出的维数也不同，假如卷积层大小为 WxH ，那么分类层最终
                  将有 WxHx2k个输出，回归层有 WxHx4k 个输出。并且滑动窗口可以使用 3x3 的卷积实现，后面的两个分类和回归分
                  支可以使用1x1 卷积实现。训练 RPN 网络时使用分类和回归两者的共同误差来训练网络。检测网络使用 Fast R-CNN
                  的方式实现，每个 RoI 输出 C+1 个类别概率以及 4C 个物体边框（C 为物体的种类）。注意，每一个 feature
                  maps 上的 region proposal 都需要独立经过 RoI max pooling （各个 channel 独立进行 max pooling ，即保持
                  channel 个数不变，只是空间上分成了许多 bins） 然后输入之后的全连接，这里并没有实现共享。另外作者让 RPN
                  网络和 Fast R-CNN 网络共享大部分卷积操作，具体作者采用 4 步训练法来训练网络
                  1. 使用 ImageNet 进行预训练，然后使用 RPN 网络进行 fine-tune
                  2. 同样使用 ImageNet 上预训练的网络和上一步训练得到的 region proposal 来训练 Fast R-CNN 网络
                  3. 利用第 2 步中得到的参数来初始化 RPN 网络中共享的卷积层参数，并固定这些卷积层的参数，只 fine-tune
                     RPN 独有的参数
                  4. 固定共享层的参数，值训练检测网络的参数


** MASK R-CNN

Mask R-CNN 最主要的共享在于图像分割，放在这里作为目标检测似乎有点不妥。

效果图：

[[./mask.jpg]]

在 Faster R-CNN 的基础上再增加一个用于分割的 branch ，采用 FCN 全卷积神经网络来对目标进行分割。采用逐元素分类的方法，对
每个 RoI 输出 kxmxm 个输出，而不是将其整合称一个向量（会损失空间位置信息），其中 k 表示目标的类别的个数，m 是经过
RoIAlign 之后得到的固定大小。使用二分类来判定每个像素点是否属于某个类别的目标。

由于采用 pixel-to-pixel 的形式，故需要让 RoI 和原图精准对应，作者在 RoI pooling 的基础上进行上进行了改进，不再对 RoI 的
边界坐标和 bins 的大小进行量化（当求取的是浮点数时进行取整），而是保留这些浮点值；并在每一个 bins 内使用双线性差值得到采
样个数个（论文中使用的是 4 ，即在每一个 bins 内使用双线性差值求得 4 个点的值，4 个点将一个 bin 分成大小相等的 9 份； We
note that the results are not sensitive to the exact sampling locations, or how many points are sampled, as long as no
quantization is performed.）点的值，然后使用 max 或者 average pooling （作者表明两者影响不大，并且论文中采用了 average
pooling）得到该 bin 的输出。No quantization is performed on any coordinates involved in the RoI, its bins, or the
sampling points.

损失采用分类边框回归和分割误差三者的和表示。 \(L = L_{cls} + L_{box} + L_{mask}\)

The mask branch has a Km 2 - dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for
each of the K classes. To this we apply a per-pixel sigmoid, and define L mask as the average binary cross-entropy loss.
For an RoI associated with ground-truth class k, L mask is only defined on the k-th mask (other mask outputs do not
contribute to the loss).

MASK R-CNN 使用了特征金字塔 FPN


** SSD



** FPN

feature pyramid network : 由于 high level feature maps 有更强的语义信息，而 low level feature maps 有更强的空间位置信息，
结合两者可以更好的预测目标的位置。使用网络多层的 feature map 来组成不同 scale 的特征金字塔来检测。FPN 代替原来的图像金字
塔，使用卷积后的不同尺寸的 feature maps 组成特征金字塔。论文中用 stage 表明 feature map size 相同的层，每个 stage 的最后
一层 feature maps 用于生成特征金字塔。因为并不是直接使用这些特征 feature maps 组成特征金字塔，而是让顶层（靠近输出层）的
feature maps 进行上采样 upsample ，得到和下层 feature maps 相同的 size，下层的 feature maps 进行 1x1 卷积以减少 channel
的个数，然后逐元素相加来组成该层的 feature maps 。由于要执行 element-wise 相加，作者让特征金字塔的每一层的 feature maps
channel 固定为 256 个，首先将最顶层的 feature maps 使用 1x1 卷积将 channel 的个数降低为 256 个，组成特征金字塔的顶层。下
面层也都会先使用 1x1 卷积将 channel 个数减小为 256 ，然后再与上层的特征逐元素相加得到新的一层。最终得到特征金字塔。在每
一层上都独立进行目标检测。

RPN 采用 FPN ： 在一个特征层使用一个固定的大小的 anchor （不过仍然有三种比例）
