#+TITLE:          something
#+AUTHOR:         Kyle Three Stones
#+DATE:           <2018-10-31 Wed>
#+EMAIL:          kyleemail@163.com
#+OPTIONS:        H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t f:t tex:t
#+TAGS:           深度学习
#+CATEGORIES:     深度学习


https://pan.baidu.com/s/1hs3Z2ao  
https://www.aliyun.com/zixun/wenji/1283158.html
https://www.cnblogs.com/objectDetect/p/5947169.html
https://www.zhihu.com/question/43609045/answer/132235276
https://blog.csdn.net/shentanyue/article/details/82109580?utm_source=blogxgwz1
https://blog.csdn.net/qq_33783896/article/details/80675398

** 网络架构

+ 解释resnet、优缺点以及适用范围
+ 解释inception net、优缺点以及适用范围
+ densenet结构优缺点以及应用场景
+ dilated conv优缺点以及应用场景
+ moblenet、shufflenet的结构


** 卷积核参数

一个标准卷积层参数的个数是 input*ksize*ksize*output ，不知道为什么自己会两次犯了相同错却仍然不自知。

如果卷积分组 group ，那么每个卷积核的大小将变小，变为 imput/group*ksize*ksize ，一个 group 的参数为
input/group*ksize*ksize*output/group ；所有 group 组成整个卷积层的参数，input/group*ksize*ksize*output/group*group =
input/group*ksize*ksize*output 。即参数会变为原来的 1/group 倍。

deepwith conv 对输入的每一个 channel 分别进行独立的卷积操作，此时输入的 channel 个数必然等于输出 channel 的个数，此卷积
层参数的个数为 ksize*ksize*input。


** 感受野

vgg 论文上说明两次 3x3 卷积可以达到 5x5 卷积核的效果、三层 3x3 卷积可以达到 7x7 卷积核的效果； *现在终于理解作者的意思，
使用两次 3x3 卷积，第二层卷积核的感受野就是 5x5 ，而三层 3x3 卷积，第三层卷积的感受野为 7x7* 。

感受野计算公式：



** Dilated conv vs Deconvolution

1. Dilated convolution 在卷积核的每两个值中间插入 d-1 个空洞
2. Deconvolution 在 feature map 上插入像素值为 0 的点
3. Unlike dilated convolutions, which have same output size as input size (if input borders are properly padded)
   "deconvolution" layers actually produce upsampling (larger input then output)

数学公式：

\begin{align*}
Z &= W \cdot A \\
dW &= dZ \cdot A \\
dA &= W^T \cdot dZ
\end{align*}


*** Dilated convolution

The goal of this layer is to increase the size of the receptive field (input activations that are used to compute a
given output) without using downsampling (in order to preserve local information). Increasing the size of the receptive
field allows to use more context (information spatially further away).

Dilated convolutions “inflate” the kernel by inserting spaces between the kernel elements. The dilation “rate” is
controlled by an additional hyperparameter d. Implementations may vary, but there are usually d−1 spaces inserted
between kernel elements such that d = 1 corresponds to a regular convolution.

Dilated convolutions are used to cheaply increase the receptive field of output units without increasing the kernel
size, which is especially effective when multiple dilated convolutions are stacked one after another.

A kernel of size k dilated by a factor d has an effective size \[k' = k + (k − 1)(d − 1)\]

To understand the relationship tying the dilation rate d and the output size o, it is useful to think of the impact of d
on the effective kernel size. 

\[ o = \frac{ \lfloor i + 2p - k - (k-1)(d-1) \rfloor }{s} + 1 \]

Dilated Convolution 的最大价值是可以不改变 feature map 的大小而增大感受野。

直接在 kernel 上插入空洞，两个节点之间插入 dilate-1 个空洞；这样 filter size 不变，但是感受野已经增大。

而之前的 FCN 使用 pooling 下采样来增大感受野，但随后又不得不通过 Deconvolution 或者 upsampling 来增大 feature map 大小，
这样的一小一大总会损失很多信息。


**** 为什么使用空洞卷积(dilated convolution)

1. 基于FCN的语义分割问题中，需保持输入图像与输出特征图的size相同。
1. 若使用池化层，则降低了特征图size,需在高层阶段使用上采样，由于池化会损失信息，所以此方法会影响导致精度降低；
1. 若使用较小的卷积核尺寸，虽可以实现输入输出特征图的size相同，但输出特征图的各个节点感受野小；
1. 若使用较大的卷积核尺寸，由于需增加特征图通道数，此方法会导致计算量较大；
1. 所以，引入空洞卷积(dilatedconvolution),在卷积后的特征图上进行0填充扩大特征图size，这样既因为有卷积核增大感受野，也因为0填充保持计算点不变。


*** Deconvolution

Here the idea is to upsample an initial layer, and therefore we spread the input by padding them with zeros and then
apply a standard convolution (with strides of 1) on the resulting input representation.

1. Dilated Conv 在卷积核中间插入空洞，仍然使用步长 1 进行卷积，唯一变化的只是与 kernel 进行运算的像素点变化了。并不改变
   feature map 的大小。
2. 而 Deconv 则是在 feature map 中间添加 0 像素点，然后在变大的 feature map 上进行卷积操作，用于增大 feature map 的大小

deconvolution:

Deconvolution layer is a very unfortunate name and should rather be called a transposed convolutional layer. A
deconvolution layer performs also convolution! That is why transposed convolution fits so much better as name and the
term deconvolution is actually misleading. You can see it as a convolution with fractional stride.

In a sense, upsampling with factor f is convolution with a fractional input stride of 1/f. So long as f is integral, a
natural way to upsample is therefore backwards convolution (sometimes called deconvolution) with an output stride of f.
Such an operation is trivial to implement, since it simply reverses the forward and backward passes of convolution.


convolution 卷积操作通过矩阵 W 进行前向传播，通过矩阵 \(W^T\) 进行反向误差的传播；transposed convolution 通过矩阵
\(W^T\) 进行前向传播，通过 \( (W^T)^T = W \) 进行反向传播；而 deconvolution 在数学上定义为卷积的逆。


** RoIAlign

RoIAlign 改进于 RoI max Pooling，而 RoI max Pooling 主要用于解决将不同大小的 RoI (region proposal) 统一分割成固定的 HxW
个数的 bins，然后在每一个 bins 中使用 max pooling 得到相应 grid cell 的值，这样就将任意大小的 RoI 转换成了固定大小的 HxW
的区域。然后送入之后的全连接层，或者卷积层。

RoI Pooling 对于目标的 BindingBox 检测来说精度足够，但是 Mask RCNN 需要 pixel-to-pixel 对齐，需要较高的精度，无法容忍
RoI Pooling 由于量化而导致的误差。RoI Pooling 存在两次量化舍入，第一次是得到 RoI 边界时的舍入，即从源图片变化到 RoI 层相
应的 feature maps，feature maps 的尺寸会有相应倍数的缩小，那么目标的 Binding Boxes 同样会有相应倍数的缩小，如果 Binding
Boxes 的尺寸无法整除缩小的倍数，RoI Pooling 便对其进行量化，得到整数像素点的 RoI ；另外将 RoI 分割成固定个数的 bins 时，
如果 RoI 的尺寸无法整除所要分割的 bins 的个数时，同样需要进行量化。

RoIAlign 去掉了所有量化取整操作，直接保存为浮点类型的值，无论是在得到 RoI 的大小还是将 RoI 分割成 bins 时。这样在每个
RoI bins 之内，根据 feature maps 临近的点双线性插值来得到 bins 内个一个或者多个采样点的，然后使用 max/average pooling 计
算求得每一个 bin 的输出值。同时 paper 中指出只要去除所有的量化操作，结果与每个 bin 之内采样个数关系不大。


** compute IOU

#+BEGIN_SRC python
def compute_iou(box, boxes):
    x1 = np.maxinum(box[0], boxes[:, 0])
    y1 = np.maxinum(box[1], boxes[:, 1])
    x2 = np.mininum(box[2], boxes[:, 2])
    y2 = np.mininum(box[3], boxes[:, 3])

    intersection = np.maxinum(x2 - x1, 0) * np.maxinum(y2 - y1, 0)

    boxesarea = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    union = (box[2] - box[0]) * (box[3] - box[1]) + boxesarea - intersection

    return intersection / union
#+END_SRC


** Data argument

[[https://github.com/aleju/imgaug][code]]

A standard machine learning situation. Train on batches of images and augment each batch via crop, horizontal flip
("Fliplr") and gaussian blur:

affine transformations 仿射变换, perspective transformations 透视变换, contrast changes, gaussian noise, dropout of
regions, hue/saturation changes 色相/饱和度, cropping/padding, blurring, ...

crop/pad 都使用参数 (top, right, bottom, left) 的顺序来表示裁剪或填充的大小。即图像的上层、右侧、下侧、左侧分别要裁剪或
者填充的像素个数。

flipud up-down 翻转；fliplr left-right 翻转；

invert 像素点的值取 255 - self 的值 ？

add 像素点上增加值；

multi 像素点乘以某个值

blur 模糊

noise 噪声

Dropout 随机丢弃一些像素点的值；也可以成块丢弃


** 计算机视觉的任务分类

像素级别： 图像复原（去模糊）、超分

感知： 语义层面的理解：分类、检测、分割

认知： 各个物体之间的关系 Visual Genome


** 什么时候用local-conv？什么时候用全卷积

两个不是一个等级的概念。local-conv 就是普通的卷积操作，filter 有一定的大小，且同一个 channel 内共享权重；

全卷积指整个网络中不再包含全连接层，全部由卷积层和 pooling 层组成。


** 如何优化模型 : 加速收敛， 避免overfit, 提升精度 ..？

*** 加速收敛

+ 激活函数 ReLU 代替 sigmoid / tanh ；
+ 新的优化方法 Momentum SGD 、RMSProp 、Adam ；
+ BN ：传统的神经网络，每一层输入的分布都在变化，让网络每一层的输出进行归一化，使得网络层每一层都可以单独优化；此时可以
  增大学习速率来加速训练；
+ 使用预训练网络
+ ResNet 能加速收敛？？？ 1x1 CONV？？？


*** 避免过拟合

+ 增加训练样本，使得样本更好的涵盖该分布
+ Data argumentation
+ 正则化：利用损失函数，强迫让权重变小，模型变得简单；奥卡姆剃刀原理
+ Dropout : 每次训练时，随机丢弃一些神经元，让网络变得简单
+ Batch Normalization 
+ early stop : 权重随着不断迭代会不断增大


*** 提升精度

+ 更好的损失函数
+ 好的网络结构 ResNet 
+ 更大的网络
+ 更多的训练样本
+ 更长的训练时长
+ BN


** CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？
为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

局部感受野、权值共享


** Batch Normalization 如何实现？作用？

实现过程：计算训练阶段 mini_batch 数量激活函数前结果的均值和方差，然后对其进行归一化，最后对其进行缩放和平移。

作用：a.限制参数对隐层数据分布的影响，使其始终保持均值为0，方差为1的分布；

b.削弱了前层参数和后层参数之间的联系，使得当前层稍稍独立于其他层，加快收敛速度；

c.有轻微的正则化效果。


** Momentum 优化算法原理？作用？

原理：在梯度下降算法中引入指数加权平均数，在更新梯度方向的过程中，在一定程度上保留了之前梯度更新的方向，同时利用当前
mini_batch的梯度方向微调最终的更新方向。

作用：在一定程度上增加梯度更新方向的稳定性，从而使得收敛速度更快。


** 什么造成梯度消失？推导？

+ 爆炸：出现在激活函数处在激活区，而且权重W过大的情况下、大量的连乘操作；问题：使得学习不稳定
+ 消失：sigmoid 函数为例，在函数的两端梯度求导结果非常小（饱和区），后向传播过程中由于连续多次用到激活函数的导数值，使得
  整体的乘积梯度结果变得越来越小，也就出现了梯度消失的现象。 sigmoid / tanh 在远离中心点的时候，斜率非常小；问题：难以知
  道朝哪个方向移动来优化代价函数；


+ 如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸，
+ 如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失。


+ 梯度爆炸一般出现在深层网络和权值初始化值太大的情况下
+ 两种情况下梯度消失经常出现，一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。

遇到斜率极大的悬崖时，梯度更新会很大程度上改变参数值，使参数弹射的很远，通常会完全跳过这类悬崖结构，使大量的优化工作成为
无用功。（在 RNN 中很常见，因为长时间序列会产生大量的相乘）

+ clip gradients 
+ 预训练-finetune
+ ReLU
+ BN 
+ ResNet


** 激活函数

为什么不再继续使用 sigmoid 函数作为激活函数：容易出现梯度消散和梯度爆炸；当值较大时（即远离 0 点）函数的斜率趋向于 0 ，
结合深度网络易导致导数消散；在 0 点附近的时候，斜率趋向正无穷，易导致梯度爆炸；

Relu: 思想也很简单，如果激活函数的导数为 1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu
就这样应运而生。

优点
-- 解决了梯度消失、爆炸的问题
-- 计算方便，计算速度快
-- 加速了网络的训练

缺点
-- 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
-- 输出不是以0为中心的

Sigmoid激活函数 缺点：

    a. 不是关于原点对称；

    b. 需要计算exp

Tanh 激活函数 优点：

    a. 关于原点对称

    b. 比sigmoid梯度更新更快

ReLU激活函数 优点：

    a.神经元输出为正时，没有饱和区

    b.计算复杂度低，效率高

    c.在实际应用中，比sigmoid、tanh更新更快

    d.相比于sigmoid更加符合生物特性

ReLU激活函数 缺点：

    a.神经元输出为负时，进入了饱和区

    b.神经元的输出在非0中心

    c.使得数据存在Active ReLU、Dead ReLU(当wx+b<0时，将永远无法进行权值更新，此时的神经元将死掉)的问题

Leaky ReLU激活函数 优点：

    a. 解决了ReLU激活函数Dead ReLU问题；

Maxout激活函数max(w1*x+b1,w2*x+b2)  缺点：

    a. 参数较多；


** 交叉熵
为什么交叉熵损失可以提高具有sigmoid和softmax输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替
sigmoid的利弊


** 规则化项有什么，各有什么样的效果，为什么起作用

L0范数：计算向量中非0元素的个数。

L1范数：计算向量中各元素绝对值之和。

L2范数：计算向量中各元素平方和的开方。

L0范数和L1范数目的是使参数稀疏化。L1范数比L0范数容易优化求解。

L2范数是防止过拟合，提高模型的泛化性能。

L∞，

Frobenius范数


** PCA传统计算流程：

    去除均值
    计算协方差矩阵
    计算特征值和特征向量
    特征值从大到小排序
    保留前N个特征向量
    投影重构（记得吧去除的均值还回去）


** 监督学习的生成模型和判别模型 
这可以说是一个最基础的问题，但是深挖起来又很复杂，面试的时候应该说出几个有亮点的部分。 
（1）基本说法 
生成模型是由数据学习联合概率分布P(X,Y)，然后再求出条件概率分布P(Y|X)，典型的生成模型有朴素贝叶斯和马尔科夫模型。 
判别模型就是直接学习判别函数或者是条件概率分布，应该是更直接一些。两者各有优缺点。 
（2）进阶区分 
 应该说生成模型的假设性更强一些，因为通常是从后验分布的角度思考问题，通常对x的分布进行了一些假设。 
 训练过程中，对于判别模型通常是最大化对数似然，对生成模型则是最大化联合对数似然函数 
 因为生成模型对于特征的分布都做出了一定的假设（如高斯判别模型假设特征分布满足多元高斯分布），所以如果对于特征的分布估计比较正确的情况下，生成模型的速度更好准确性也更高。 
 生成模型在训练数据的时候对于每一类数据的都是独立估计的（也就是每一类的参数不同），这也就说明如果有新类别加入的情况下，是不需要对原有类别进行重新训练的 
 对于半监督学习，生成模型往往更有用 
 生成模型有一个大的缺点就是不能对特征进行某些预处理（如特征映射），因为预处理后的数据分布往往有了很大的变化。


** 频率学派的一些基本理论 
（1）期望损失（风险函数）、经验损失（经验风险）、结构风险 
期望损失：理论上知道模型后得到的平均损失较期望损失（依赖于真实分布），但是模型正是我们要求的 
经验损失：经验损失指针对模型的抽样值（训练集）进行平均的损失估计，根据大数定律当训练数据足够的时候经验损失和期望损失是等价的 
结构风险：经验损失是假设经验分布和自然分布相同时得到的，但是这样会造成过拟合，所以引入了正则化，惩罚模型复杂度。 
（2）极大似然MLE、极大后验MAP 
因为我们有的时候利用经验损失求解的时候会遇到不好求解的问题（如不连续0-1）这是可以用对数极大似然估计等价的对参数进行分析。 
同理最大后验利用先验概率达到惩罚模型的作用。如l2-norm岭回归对应高斯先验、L1对应拉普拉斯先验。


** 什么是凸集、凸函数、凸学习问题？

    凸集：若对集合C中任意两点u和v，连接他们的线段仍在集合C中，那么集合C是凸集。

    公式表示为：αu+(1-α)v∈C α∈[0, 1]

    凸函数：凸集上的函数是凸函数。凸函数的每一个局部极小值也是全局极小值( f(x) = 0.5x^2 )。

    公式表示为：f(αu + (1-α)v) ≤ αf(u)+ (1-α)f(v)



** tf.layer 和 tf.nn 

tf.layer 高级 API ；tf.nn 低层次的 API ; 

tf.nn是最基础的层，如tf.nn.conv2d, tf.nn.max_pool等，需要编程者自己定义权重。包括卷积操作（conv）、池化操作（pooling）、
归一化、loss、分类操作、embedding、RNN、Evaluation。

tf.layers 是基于 tf.nn 封装的高级函数，如果自己定义 Conv2d，只需要一个函数即可

tf.contrib.layers.conv2d 也是封装完好的高级函数，最初开发的功能放置在 tf.contrib，等到稳定后变放入到 tf.layer 中；

conv1 = tf.nn.conv2d(input_tensor,conv1_weights,strides=[1,2,2,1],padding='SAME')

strides=[1,2,2,1] ，strides 在官方定义中是一个一维具有四个元素的张量，其规定前后必须为1，中间两个数分别代表了水平滑动和
垂直滑动步长值。

tf.nn.conv2d 中的 filter 参数，是 [filter_height, filter_width, in_channels, out_channels] 的形式，而
tf.nn.conv2d_transpose 中的 filter 参数，是 [filter_height, filter_width, out_channels，in_channels] 的形式，注意
in_channels 和 out_channels 反过来了！因为两者互为反向，所以输入输出要调换位置


** pooling

pooling 目的是为了保持某种不变性（旋转、平移、伸缩等）；减小 feature map 的大小；


** anchor

虽然 anchor 是在原始图像上，但是 anchor 的个数由 feature_map 的大小决定。
