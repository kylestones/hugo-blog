#+TITLE:          convolution
#+AUTHOR:         Kyle Three Stones
#+DATE:           <2018-12-16 Sun>
#+EMAIL:          kyleemail@163.com
#+OPTIONS:        H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t f:t tex:t
#+TAGS:           卷积, 深度学习
#+CATEGORIES:     深度学习


** [[http://cs231n.github.io/convolutional-networks/][cs231n]]


斯坦福大学的课程 [[http://cs231n.stanford.edu/][CS231n: Convolutional Neural Networks for Visual Recognition]] 。发现写的很好，后悔没有早点看。可惜只看了
卷积这一部分。有中文翻译的部分可以只看中文总结，后面的英文原文更方便理解。


** 卷积

卷积神经网络明确假设输入是 *images* ，根据这个假设可以大量的减少参数的个数。同时有利于更 efficient 的实现。普通的神经网
络采用全连接，用于图像中会产生太多的参数，导致 *overfitting* 。卷积神经网络的 layers 拥有神经元的维数为
(width,height,channels) 。

A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume
with some differentiable function that may or may not have parameters.

卷积操作的本质就是滤波器和输入的部分区域做点积。卷积的反向传播也是卷积，只是做了转置。Note that the convolution
operation essentially performs dot products between the filters and local regions of the input. The backward pass for a
convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters).

卷积输出的每个元素都是输入的一个区域和滤波器逐元素相乘并求和，然后再加上一个 bias 得到的。The visualization below
iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the
highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.


*** 卷积层特性：

1. 输入维度 \(W_1 * H_1 * D_1\)
1. 四个超参： filters number K , fileter spatial size F , stride S , padding P
1. 输出维度 \(W_2 * H_2 * D_2\) , 其中 \(W_2=(W_1-F+2P)/S+1, \ D_2=K\)
1. 由于参数共享，每个 filter 有 \(F \cdot F \cdot D_1\) 个权重，整个卷积层有 \((F \cdot F \cdot D_1) \cdot K\) 个权重和
   \(K\) 个 biases
1. 由 d-th 个 filter 使用 stride S 和 d-th 个 bias 生成输出的 d-th 通道（ size \(W_2*H_2\) ）


*** Converting FC layers to CONV layers

卷积层和全连接层可以相互转化。It is worth noting that the only difference between FC and CONV layers is that the neurons
in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share
parameters. However, the neurons in both layers still compute dot products, so their functional form is identical.
Therefore, it turns out that it’s possible to convert between FC and CONV layers:

1. For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large
   matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the
   blocks are equal (due to parameter sharing).
1. Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with K=4096 that is looking at
   some input volume of size 7×7×512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other
   words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be
   1×1×4096 since only a single depth column “fits” across the input volume, giving identical result as the initial FC
   layer.

Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of
32 pixels gives an identical result to forwarding the converted ConvNet one time.

将图片放大，然后输入网络，相当于 crop 原图像大小的输入，然后在相应的位置求平均，来得到更好的结果，这样比 crop 多次，然后
在卷积多次要快的多，因为这样会有很多重复的计算：Naturally, forwarding the converted ConvNet a single time is much more
efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation.
This trick is often used in practice to get better performance, where for example, it is common to resize an image to
make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the
class scores.

没懂下面的内容：通过执行两次卷积，以原图像和移位 16 个像素的图像分别作为输入，可以将本来缩小比例为 32 的网络，求解出缩小
比例为 16 的网络？ Lastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride
smaller than 32 pixels? We could achieve this with multiple forward passes. For example, note that if we wanted to use a
stride of 16 pixels we could do so by combining the volumes received by forwarding the converted ConvNet twice: First
over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and
height.


**** Padding

使用 padding 保持卷积之后，图像的尺寸不变，网络的表现会更好。valid 卷积无法充分获取图像边界的信息。 Why use padding? In
addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves
performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of
the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away”
too quickly.


*** 使用矩阵乘法实现卷积

caffe 中的 im2col 实现图像卷积展开成矩阵。由于同一个值需要存储在矩阵的不同位置，会消耗大量的内存。但可以利用现有的矩阵乘
法库（BLAS）来加速计算。

Implementation as Matrix Multiplication. Note that the convolution operation essentially performs dot products between
the filters and local regions of the input. A common implementation pattern of the CONV layer is to take advantage of
this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows:

1. The local regions in the input image are stretched out into columns in an operation commonly called im2col. For
   example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take
   [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating
   this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an
   output matrix X_col of im2col of size [363 x 3025], where every column is a stretched out receptive field and there
   are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may
   be duplicated in multiple distinct columns.
1. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size
   [11x11x3] this would give a matrix W_row of size [96 x 363].
1. The result of a convolution is now equivalent to performing one large matrix multiply np.dot(W_row, X_col), which
   evaluates the dot product between every filter and every receptive field location. In our example, the output of this
   operation would be [96 x 3025], giving the output of the dot product of each filter at each location.
1. The result must finally be reshaped back to its proper output dimension [55x55x96].

This approach has the downside that it can use a lot of memory, since some values in the input volume are replicated
multiple times in X_col. However, the benefit is that there are many very efficient implementations of Matrix
Multiplication that we can take advantage of (for example, in the commonly used BLAS API). Moreover, the same im2col
idea can be reused to perform the pooling operation, which we discuss next.


** Pooling Layer

Pooling 可以大大较少参数的个数，防止过拟合。 It is common to periodically insert a Pooling layer in-between successive
Conv layers in a ConvNet architecture. Its function is to progressively *reduce the spatial size of the representation
to reduce the amount of parameters and computation in the network, and hence to also control overfitting.*

The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX
operation. 

特性：

1. 两个超参： spatial extent F, stride S
1. zero padding

+ 反向传播 :: 前向传播的时候记录 max value 的索引，方便反向传播计算。 max pooling 反向传播时，先将 feature maps 上采样，
          将梯度值传入 max value 索引的地方，其他位置的值均赋值为 0 。avera pooling 则将梯度值除以扩大的倍数，并将平均值
          分别填入每一个位置。总之确保 loss 值反向传播中大小和保持不变。
+ 摒弃 pooling :: 在 variational autoencoders (VAEs) or generative adversarial networks (GANs) 中不使用 pooling 对于网络
                的训练很重要。可以使用 stride 较大的卷积替代。


Pooling sizes with larger receptive fields are too destructive.

Pooling 可以使用任意函数，只是 max 使用比较多。 In addition to max pooling, the pooling units can also perform other
functions, such as average pooling or even L2-norm pooling.

Average pooling 表现不如 max pooling 而逐渐被抛弃。 Average pooling was often used historically but has recently fallen
out of favor compared to the max pooling operation, which has been shown to work better in practice.


** Layer Patterns

INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC

Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper
networks, because multiple stacked CONV layers can develop more complex features of the input volume before the
destructive pooling operation.

堆叠小尺寸的卷积核，相比于直接只用一个大尺寸的卷积核，可以得到更好的特征，且只需要更好的参数。但是需要更多的内存来存储中
间的卷积层。 Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters
allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we
might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.

从输入到输出只有一单条线路的卷积神经网络，正在被更复杂的网络结构所替代 GoogLeNet, ResNet, DenseNet 等。It should be
noted that the conventional paradigm of a linear list of layers has recently been challenged, in Google’s Inception
architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see
details below in case studies section) feature more intricate and different connectivity structures.

尽量使用已有的网络模型，并在自己的数据集上 finetune ，而不是从头开始设计。 In practice: use whatever works best on
ImageNet. If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you’ll be pleased to know
that in 90% or more of applications you should not have to worry about these. I like to summarize this point as “don’t
be a hero”: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently
works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a
ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning school.


** Layer Sizing Patterns

输入图像的大小最好可以多次被 2 整除。 The input layer (that contains the image) should be divisible by 2 many times.
Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.


使用 3*3 的卷积可以有效减小参数的个数，需要增大感受野可以堆叠多个 3*3 卷积。 The conv layers should be using small
filters (e.g. 3x3 or at most 5x5), using a stride of S=1, and crucially, padding the input volume with zeros in such way
that the conv layer does not alter the spatial dimensions of the input. That is, when F=3, then using P=1 will retain
the original size of the input. When F=5, P=2. For a general F, it can be seen that P=(F−1)/2 preserves the input size.
If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that
is looking at the input image.


Pooling 层有较大的破坏性，一般不会使用较大的尺寸。The pool layers are in charge of downsampling the spatial dimensions
of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2
(i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both
width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this
makes. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is
then too lossy and aggressive. This usually leads to worse performance.


卷积层只改变通道数，不修改图像的大小。POOL 负责减小图像的大小。 Reducing sizing headaches. The scheme presented above is
pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge
of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad
the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN
architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and
symmetrically wired.


卷积步长选择为 1 效果较好。 Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as
already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only
transforming the input volume depth-wise.

由于内存的限制，会首先大幅度减小图像的尺寸。Compromising based on memory constraints. In some cases (especially early in
the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For
example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three
activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per
image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to
compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example,
one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As
another example, an AlexNet uses filter sizes of 11x11 and stride of 4.





** [[https://arxiv.org/abs/1603.07285][<A guide to convolution arithmetic for deep learning>]] 

论文的部分翻译，[[https://github.com/vdumoulin/conv_arithmetic][github 地址]] ，由于不是很理解转置卷积和空洞卷积，所以看了看这篇文章，翻译一下以备忘。相比于斯坦福大学的课
程，感觉这篇论文有点故弄玄虚，不符合大道至简呀【微吐槽】。


** Introduction

CNNs 在深度学习中处于核心角色，但通常第一次使用 CNNs 却是一场噩梦。因为一个卷积层输出的大小会由输入的大小、卷积核的尺寸、
padding 的大小、stride 共同决定，而且相互之间的关心有点复杂。相对而言，全连接层的输出的大小和输入的大小并没有直接关系。
同时 CNNs 经常会引入 pooling层，进一步增加了复杂性。另外转置卷积也在被更频繁的使用。

这个手册有两个目的：

1. 解释卷积和转置卷积的相互关系
1. 对卷积层、pooling 层、转置卷积层中 input shape、kernel shape、zero padding、strides、output shape 的相互关系有一个直
   观的理解


*** Discrete convolutions

神经网络的必备元素是仿射变换 ： 输入是一个向量，乘以一个矩阵，在加上一个偏移得到输出。这适用于任何形式的输入（图片、音频
片段、无序特征；不管维数是多少都可以在转换前变成一个向量）

图片、音频片段等数据都有内在结构，一般来说，他们有以下重要的属性：

1. 使用多维数组存储。 [They are stored as multi-dimensional arrays.]
1. 特征在一个维度或者多个维度的存储顺序有影响。 [They feature one or more axes for which ordering matters (e.g., width
   and height axes for an image, time axis for a sound clip).]
1. channel 维度表示的是数据的不同 view 。[One axis, called the channel axis, is used to access different views of the
   data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio
   track).]

当使用仿射变换的时候，这些结构信息将无法被有效利用。因为所有轴上的数据都被统一对待，根本没有考虑数据的拓扑信息（直接将数
据转换成了一个向量）。而在计算机视觉和语音识别中最好保留这些数据的结构，因此有了离散卷积 (discrete convolution)。

离散卷积是一个线性变换，保留了数据的顺序，采用稀疏连接和权值共享。

卷积核扫描输入，在每一个位置上，卷积核和其覆盖的输入区域逐元素相乘并求和得到该位置的输出。可以重复执行这个过程来得到多个
输出，这些生成的输出称为 feature maps 。如果输入是多个 feature maps ，卷积核需要是三维的，卷积核 channel 的个数应该等于
输入 feature maps channel 的个数，输入的每一个 feature map 都需要一个特定的卷积核来扫描，得到的多个结果（不同的 channel
）逐元素相加得到输出的一个 feature map 。

strides 表示的是 kernel 移动的步长，也可以看做是保留步长为 1 卷积输出的某些部分。比如步长为 2 的卷积输出可以看做，步长为
1 的卷积得到的输出中只保留奇数位元素（从 1 开始计数）得到的结果。


*** pooling

pooling 是 CNNs 的另一个关键部件。用于减小 feature maps 的尺寸，通常有 average pooling 和 max pooling 两种。 Pooling 和
卷积很像（我感觉就是一种特殊的卷积呀！），同样会扫描输入，但是将 kernel 换成了 pooling function 。


** Convolution arithmetic

下面的分析依赖于卷积不会跨越不同轴的特性；[i.e., the choice of kernel size, stride and zero padding along axis j only
affects the output size of axis j.]


*** Half (same) padding

保持输出和输入的尺寸不变时使用，需要填充的个数 \( p = \lfloor k / 2 \rfloor \) 。


*** Full padding

卷积通常会较小输出的尺寸，但有时候需要增大输出的尺寸（这个不是转置卷积的任务吗），此时可以用 full padding 。此时填充的个
数 \( p = k - 1 \) ，这里 full padding 的意思是让输入的每个像素点对 kernel 都有相同的地位。


*** Zero padding, non-unit strides

卷积输出大小的通用公式 \[ o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1 \]

这里向下取整，是因为如果剩余的区域（含 padding ）不足一个 kernel 的长度时，不再继续执行卷积。


** Pooling arithmetic

pooling 层使得网络对输入的微小偏移不敏感。max pooling 使用较频繁： 将输入分割成 patches （通常不会重叠） ，并输出每个
patch 内的最大值；另外还有 average pooling ，输出每个 patch 内的平均值。

计算卷积输出大小的公式同样适用于 pooling ，但是 Pooling 并不会 padding ，所以 Pooling 的通用输出公式为

\[ o = \lfloor \frac{i - k}{s} \rfloor + 1 \]


** Transposed convolution arithmetic

转置卷积一般用于常规卷积的反方向操作，将常规卷积的输出转换到输入，同时保持和常规卷积的兼容性。例如，可以用于 autoencoder
中的 decoding layer 或者将 feature map 变换到更高维的空间中。全连接层比较简单，只需要利用权重矩阵的转置就可以实现反向操
作。卷积层会比全连接层复杂很多，不过可以将卷积层归结为高效实现的矩阵操作，洞悉全连接层将有助于解决卷积。如同卷积操作不会
跨越不同的坐标轴（不会跨越 channel），转置卷积也有相同的属性，这将简化转置卷积算法。

下文重点关注;

1. 2-D 转置卷积 (N=2)
1. square inputs \((i_1 = i_2 = i)\)
1. square kernel size \((k_1 = k_2 = k)\)
1. 所有方向使用相同的 strides \((s_1 = s_2 = s)\)
1. 所有 axes 使用相同大小的 padding \((p_1 = p_2 = p)\)

这些结果都可以推广到 N-D 和 non-square 情形

转置卷积等效的卷积输出大小同样可以用上面卷积中的通用公式  \[ o = \lfloor \frac{i + 2p - k}{s} \rfloor + 1 \] 只是这里的
参数需要正确对应。


*** 卷积视为矩阵操作

当 \(i = 4, k = 3, s = 1, p = 0\) 时，如果将 input 和 output 从左到右，由上到下展开成向量，卷积可以表示成一个稀疏的矩阵
C ，其中非 0 的元素是 kernel 中相应的元素，其中 \(i\) 和 \(j\) 分别表示 kernel 的行和列

\begin{equation*}
\left(
\begin{array}{cccccccccccccccc}
w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0 \\
0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 \\
0 & 0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} 
\end{array}
\right)
\end{equation*}

这种线性操作将输入矩阵展开成 16 维的向量，同时生成一个 4 维的向量，然后 reshape 成 2x2 的输出矩阵。

使用这种方法表示时，反向传播可以很容易通过矩阵 C 的转置矩阵实现，即 loss 乘以 \(C^T\) 来反向传播误差。将一个 4 维的向量
作为输入，得到一个 16 维的输出，并且其连接模式（感觉应该是输入输出矩阵展开成向量的方式）和矩阵 C 兼容。

值得注意的是， kernel W 同时定义了用于前向传播矩阵 C ，也定义了反向传播矩阵 \(C^T\) 。



*** 转置卷积

转置卷积 (transposed convolution) 也叫做小数步长卷积 (fractionally strided convolutions) 或 deconvolutions [学术上可能使
用这个名称，但是在数学上 deconvolution 表示逆矩阵，和矩阵的转置并不一样] 。kernel 定义了矩阵 W ，可以认为是常规卷积，在
前向传播时乘以矩阵 C ，反向传播时乘以矩阵 \(C^T\) ；也可以看做转置卷积，在前向传播时乘以矩阵 \(C^T\) ，反向传播时乘以
\((C^T)^T = C\) ，所以 *一个转置卷积总是可以对应到一个常规卷积，但需要对输入增加许多无效的 0 值（额外的值全为 0 的行和
列），导致效率低下* 。

实际代码实现时将转置卷积看做，将梯度作为输入的常规卷积？？？ TODO [The transposed convolution operation can be thought of as
the gradient of some convolution with respect to its input, which is usually how transposed convolutions are implemented
in practice.]


*** No zero padding, unit strides, transposed

思考转置卷积最简单的方法是，将输入看做常规卷积的某一层输出的 feature map ，通过转置卷积来恢复该层的输入（由于转置卷积并
不是卷积的逆，无法原值恢复输入，仅仅可以恢复相同宽和高的输入）。

\(i=4, k=3, s=1, p=0\) 的常规卷积得到一个 2x2 的输出，转置卷积将用 2x2 的输入得到 4x4 的输出。

另一种得到转置卷积输出的方法（第一种方法在哪里？）：利用常规卷积来实现： \(i'=2, k'=k, s'=1, p'=2\) 。可以看到卷积核和步
长保持不变，但是增加了 padding 。这是一个低效的方法，这里只是为了阐述方便，实际实现中不会计算那些无效的 0 值。

通过卷积的连接方式（connectivity pattern : 输入、输出的大小），理解转置卷积的等效常规卷积需要增加 padding 的背后逻辑，并
用其来指导设计等效的卷积。如，输入左上角的 pixel 只会影响输出的左上角，输入的右上角的像素只会作用于输出的右上角，其他的
位置类似。

为了保持相同的连接方式，需要 zero pad ，使得第一个卷积（最左上角的卷积）只与输入的最左上角的像素相乘（kernel 其他位置与
padding 的 0 值相乘），也就是让 \(p = k -1 \) 。

\(s=1, p=0, k\) 卷积的转置，等效于卷积 \(k'=k, s'=s, p'=k-1\) ，且其输出为 \[ o' = i' + (k - 1) \] 

有趣的是，这里等效的常规卷积是 full padding 、单位步长的卷积。

[[https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides_transposed.gif][i'=2, k'=3, s'=1, p'=2 动图]]


*** Zero padding, unit strides, transposed

既然知道无 padding 的卷积的转置等效于让 input 增加 padding 的常规卷积，当 input 有 padding 的时候，等效的转置卷积输入会有较
少个数的 padding 。因为常规卷积输入 padding 的值并非真正有效的值（但是参与了运算），而当前面的输入需要作为转置卷积的输出
的时候，输出的大小的等效值变小了（padding 并不需要输出），从而转置卷积的输入也就需要较小的 padding 。

\(i=5, k=4, p=2\) 卷积的转置，等效于 \(k'=k, s'=s, p'=k-p-1\) 的卷积，且输出大小为 \[o' = i' + (k-1) -2p\] 


**** Half(same) padding, transposed

由于 same padding 的常规卷积，输出的大小等于输入的大小；因此 same padding 常规卷积的转置的等效常规卷积是其本身，也是
一个 same padding 的卷积。即 \(k=2n+1, s=1, p= \lfloor k/2 \rfloor = n\) 卷积的转置等效于 \(k'=k, s'=s, p'=p\) ，且其输
出大小为

\begin{align*}
o' &= i' + (k-1) -2p \\
&= i' + 2n -2n \\
&= i'
\end{align*}

[[https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides_transposed.gif][动图]]


**** Full padding, transposed

non-padded 卷积的转置等效于一个 full padding 的卷积，理所当然，一个 full padding 卷积的转置等效于一个 non-padded 卷积。
两者互为输入输出关系。

\(s=1, k, p=k-1\) 卷积的转置等效于 \(k'=k, s'=s, p'=0\) 的卷积，且输出大小为 

\begin{align*}
o' &= i' + (k-1) -2p \\
&= i' - (k-1)
\end{align*}

[[https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/full_padding_no_strides_transposed.gif][动图]]


*** No zero padding, non-unit strides, transposed

步长 \(s > 1\) 卷积的转置，等效的卷积步长 \(s < 1\) ，这也是转置卷积被称为小数步长卷积 fractionally strided convolutions
的原因。当卷积的步长 \(s>1\) 时，其转置等效的卷积需要在 input 的每两个 pixel 之间插入 \(s-1\) 的 0 值，从而使得 kernel
的移动步伐慢于 unit strides 的卷积。当然这里只是为了便于理解，实际代码实现肯定不会去让那些无效的 0 值去做乘法。

\(p=0, k, s\) 且 \(i-k\) 可以整除 \(s\) 卷积的转置的等效卷积为 \(\tilde{i}', k'=k, s'=1, p'=k-1\) ，其中 \(\tilde{i}'\)
表示在 input 的两个 pixel 之间插入 \(s-1\) 个 0 值，且其输出为 \[o' = s(i'-1) + k\]

[[https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides_transposed.gif][动图]]


*** Zero padding, non-unit strides, transposed

\(k, s, p\) 且 \(i+2p-k\) 可以整除 \(s\) 卷积的转置，等效的卷积为 \(\tilde{i}', k'=k, s'=1, p'=k-p-1\) ，其中
\(\tilde{i}'\) 表明在 input 的每两个 pixel 之间插入 \(s-1\) 个 0 值，且其输出为 \[o'=s(i'-1)+k-2p\]

[[https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_transposed.gif][动图]]


\(k,s,p\) 卷积的转置，等效的卷积为 \(a, \tilde{i}', k'=k, s'=1, p'=k-p-1\) 其中 \(\tilde{i}'\) 表示在 input 的每两个
pixel 之间插入 \(s-1\) 个 0 值，\(a=(i+2p-k) \  mod \ s \) 表示在 input 的底部和右侧需要添加的 0 值的个数，且其输出为
\[o'=s(i'-1)+a+k-2p\] 

[[https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_odd_transposed.gif][动图]]


*** 转置卷积等效卷积参数的通用规则

\begin{align*}
p' &= k -p -1 \\
s' &= 1 \\
k' &= k \\
& \text{adding s-1 zeros between each input unit}
\end{align*}



** Miscellaneous convolutions

*** Dilated convolutions

Dilated convolutions 通过在 kernel 相邻的两个元素之间插入空隙来扩充 kernel 。扩大比例由超参 d 来决定，即在 kernel 的相邻
元素之间插入 d-1 个 0 值后，执行常规卷积操作。

Dilated convolutions 是不增大卷积核的尺寸，同时可以扩大感受野的有效方法，特别是堆叠使用多层的 dilated convolutions 的时
候。

扩充后的卷积核的尺寸为 \[ \hat{k} = k + (k-1)(d-1) \]

参考常规卷积输出的通用公式，利用扩充后卷积核的大小，可以得到 Dilated convolutions 输出的大小 

\[ o = \lfloor \frac{i + 2p - k - (k-1)(d-1) }{s} \rfloor + 1 \]

这个将导致输出的 size 减小


** 读后感

1. 论文中解释了一些自己原来不清楚的知识点，如 full padding 的含义、
1. 感觉论文没有很清楚的解释，只是单纯的列举出了各种情况下卷积和转置卷积，以及相互对应关系
1. 我还是不清楚实际中是怎样实现转置卷积以及空洞卷积的，当时就是看 DarkNet 的代码没看懂才来阅读的！！！转置卷积是通过将输
   入输出展开成矩阵，然后使用矩阵乘法来实现？还是转换成等效的卷积实现？
1. 读完之后没有那种豁然开朗的感觉


** Deformable Convolutions NetWorks

正常的 3x3 卷积，首先使用一个规则的区域 R 采集输入的 feature maps X ，然后乘以权重 w 。

\[ \mathit{R} = \{(-1,-1),(-1,0), \ldots, (0,1), (1,1) \]

\[ y(P_0) = \sum_{P_n \in R} W \cdot X(P_0 + P_n) \]

而可变形卷积就是将输入进行添加 offset \(\{\triangle P_n | n = 1,\ldots, N \} \ where \ N = | R |\) ，相应的卷积公式变成

\[ y(P_0) = \sum_{P_n \ in R} W \cdot X(P_0 + P_n + \triangle P_n) \]

因为每个输入的位移都是二维的，所以可以在二维的 feature map 上任意方向上移动。

又由于增加 offset 的点可能不对应整数坐标，此时通过双线性差值得到偏移后的输入值。

MxNet 有可变形卷积的接口，不过记得可变形卷积官方代码说 MxNet 实现的有问题？

#+BEGIN_SRC python
mxnet.ndarray.contrib.DeformableConvolution
#+END_SRC
